{
  "version": 4,
  "terraform_version": "1.3.7",
  "serial": 10,
  "lineage": "17122e45-039e-2f7e-c0f9-6f096d83d445",
  "outputs": {},
  "resources": [
    {
      "module": "module.test-module",
      "mode": "data",
      "type": "kubectl_path_documents",
      "name": "syntetic",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "disable_template": false,
            "documents": [
              "---\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: hotrod\n  labels:\n    name: hotrod",
              "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hotrod\n  name: hotrod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hotrod\n  strategy: {}\n  template:\n    metadata:\n      labels:\n        app: hotrod\n    spec:\n      containers:\n      - image: jaegertracing/example-hotrod:latest\n        name: hotrod\n        args: [\"all\"]\n        env:\n          - name: JAEGER_AGENT_HOST\n            value: tempo-tempo-distributed-distributor.tempo.svc\n          - name: JAEGER_AGENT_PORT\n            value: \"6831\"\n        ports:\n          - containerPort: 8080\n            name: frontend\n          - containerPort: 8081\n            name: customer\n          - containerPort: 8083\n            name: route\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100M\n          requests:\n            cpu: 100m\n            memory: 100M",
              "apiVersion: v1\nkind: Service\nmetadata:\n  name: hotrod\nspec:\n  selector:\n    app: hotrod\n  ports:\n    - name: frontend\n      protocol: TCP\n      port: 8080\n      targetPort: frontend"
            ],
            "id": "ea9f8352e9695623d42bafa1f3a422aef4c86bf80f4799e9fb8caf1b4bd7f2d5",
            "manifests": {
              "/api/v1/namespaces/hotrod": "apiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    name: hotrod\n  name: hotrod\n",
              "/api/v1/services/hotrod": "apiVersion: v1\nkind: Service\nmetadata:\n  name: hotrod\nspec:\n  ports:\n  - name: frontend\n    port: 8080\n    protocol: TCP\n    targetPort: frontend\n  selector:\n    app: hotrod\n",
              "/apis/apps/v1/deployments/hotrod": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hotrod\n  name: hotrod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hotrod\n  strategy: {}\n  template:\n    metadata:\n      labels:\n        app: hotrod\n    spec:\n      containers:\n      - args:\n        - all\n        env:\n        - name: JAEGER_AGENT_HOST\n          value: tempo-tempo-distributed-distributor.tempo.svc\n        - name: JAEGER_AGENT_PORT\n          value: \"6831\"\n        image: jaegertracing/example-hotrod:latest\n        name: hotrod\n        ports:\n        - containerPort: 8080\n          name: frontend\n        - containerPort: 8081\n          name: customer\n        - containerPort: 8083\n          name: route\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100M\n          requests:\n            cpu: 100m\n            memory: 100M\n"
            },
            "pattern": ".terraform/modules/test-module/manifests/syntetic-monitor.yaml",
            "sensitive_vars": null,
            "vars": null
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.test-module",
      "mode": "data",
      "type": "template_file",
      "name": "loki",
      "provider": "provider[\"registry.terraform.io/hashicorp/template\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "5e63d0fb91fd9f28d5e0b762950f88bd12b84519ed635abd567052a4127f33b0",
            "rendered": "---\nglobal:\n  image:\n    # -- Overrides the Docker registry globally for all images\n    registry: null\n  # -- Overrides the priorityClassName for all pods\n  priorityClassName: null\n  # -- configures cluster domain (\"cluster.local\" by default)\n  clusterDomain: \"cluster.local\"\n  # -- configures DNS service name\n  dnsService: \"kube-dns\"\n  # -- configures DNS service namespace\n  dnsNamespace: \"kube-system\"\n\n# -- Overrides the chart's name\nnameOverride: null\n\n# -- Overrides the chart's computed fullname\nfullnameOverride: null\n\n# -- Image pull secrets for Docker images\nimagePullSecrets: []\n\nloki:\n  # Configures the readiness probe for all of the Loki pods\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 30\n    timeoutSeconds: 1\n  image:\n    # -- The Docker registry\n    registry: docker.io\n    # -- Docker image repository\n    repository: grafana/loki\n    # -- Overrides the image tag whose default is the chart's appVersion\n    tag: null\n    # -- Docker image pull policy\n    pullPolicy: IfNotPresent\n  # -- Common annotations for all pods\n  podAnnotations: {}\n  # -- The number of old ReplicaSets to retain to allow rollback\n  revisionHistoryLimit: 10\n  # -- The SecurityContext for Loki pods\n  podSecurityContext:\n    fsGroup: 10001\n    runAsGroup: 10001\n    runAsNonRoot: true\n    runAsUser: 10001\n  # -- The SecurityContext for Loki containers\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop:\n        - ALL\n    allowPrivilegeEscalation: false\n  # -- Specify an existing secret containing loki configuration. If non-empty, overrides `loki.config`\n  existingSecretForConfig: \"\"\n  # -- Config file contents for Loki\n  # @default -- See values.yaml\n  config: |\n    {{- if .Values.enterprise.enabled}}\n    {{- tpl .Values.enterprise.config . }}\n    {{- else }}\n    auth_enabled: {{ .Values.loki.auth_enabled }}\n    {{- end }}\n\n    server:\n      http_listen_port: 3100\n      grpc_listen_port: 9095\n\n    memberlist:\n      join_members:\n        - {{ include \"loki.name\" . }}-memberlist\n\n    {{- if .Values.loki.commonConfig}}\n    common:\n    {{- toYaml .Values.loki.commonConfig | nindent 2}}\n      storage:\n      {{- include \"loki.commonStorageConfig\" . | nindent 4}}\n    {{- end}}\n\n    limits_config:\n      enforce_metric_name: false\n      reject_old_samples: true\n      reject_old_samples_max_age: 168h\n      max_cache_freshness_per_query: 10m\n      split_queries_by_interval: 15m\n\n    {{- with .Values.loki.memcached.chunk_cache }}\n    {{- if and .enabled .host }}\n    chunk_store_config:\n      chunk_cache_config:\n        memcached:\n          batch_size: {{ .batch_size }}\n          parallelism: {{ .parallelism }}\n        memcached_client:\n          host: {{ .host }}\n          service: {{ .service }}\n    {{- end }}\n    {{- end }}\n\n    {{- if .Values.loki.schemaConfig}}\n    schema_config:\n    {{- toYaml .Values.loki.schemaConfig | nindent 2}}\n    {{- else }}\n    schema_config:\n      configs:\n        - from: 2022-01-11\n          store: boltdb-shipper\n          {{- if eq .Values.loki.storage.type \"s3\" }}\n          object_store: s3\n          {{- else if eq .Values.loki.storage.type \"gcs\" }}\n          object_store: gcs\n          {{- else }}\n          object_store: filesystem\n          {{- end }}\n          schema: v12\n          index:\n            prefix: loki_index_\n            period: 24h\n    {{- end }}\n\n    {{- if or .Values.minio.enabled (eq .Values.loki.storage.type \"s3\") (eq .Values.loki.storage.type \"gcs\") }}\n    ruler:\n      storage:\n      {{- include \"loki.rulerStorageConfig\" . | nindent 4}}\n    {{- end -}}\n\n    {{- with .Values.loki.memcached.results_cache }}\n    query_range:\n      align_queries_with_step: true\n      {{- if and .enabled .host }}\n      cache_results: {{ .enabled }}\n      results_cache:\n        cache:\n          default_validity: {{ .default_validity }}\n          memcached_client:\n            host: {{ .host }}\n            service: {{ .service }}\n            timeout: {{ .timeout }}\n      {{- end }}\n    {{- end }}\n\n    {{- with .Values.loki.storage_config }}\n    storage_config:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    {{- with .Values.loki.query_scheduler }}\n    query_scheduler:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n  # Should authentication be enabled\n  auth_enabled: false\n\n  # -- Check https://grafana.com/docs/loki/latest/configuration/#common_config for more info on how to provide a common configuration\n  commonConfig:\n    path_prefix: /var/loki\n    replication_factor: 3\n\n  storage:\n    bucketNames:\n      chunks: chunks\n      ruler: ruler\n      admin: admin\n    type: s3\n    s3:\n      s3: null\n      endpoint: null\n      region: null\n      secretAccessKey: null\n      accessKeyId: null\n      s3ForcePathStyle: false\n      insecure: false\n    gcs:\n      chunkBufferSize: 0\n      requestTimeout: \"0s\"\n      enableHttp2: true\n    local:\n      chunks_directory: /var/loki/chunks\n      rules_directory: /var/loki/rules\n\n  # -- Configure memcached as an external cache for chunk and results cache. Disabled by default\n  # must enable and specify a host for each cache you would like to use.\n  memcached:\n    chunk_cache:\n      enabled: false\n      host: \"\"\n      service: \"memcached-client\"\n      batch_size: 256\n      parallelism: 10\n    results_cache:\n      enabled: false\n      host: \"\"\n      service: \"memcached-client\"\n      timeout: \"500ms\"\n      default_validity: \"12h\"\n\n  # -- Check https://grafana.com/docs/loki/latest/configuration/#schema_config for more info on how to configure schemas\n  schemaConfig: {}\n\n  # -- Structured loki configuration, takes precedence over `loki.config`, `loki.schemaConfig`, `loki.storageConfig`\n  structuredConfig: {}\n\n  # -- Additional query scheduler config\n  query_scheduler: {}\n\n  # -- Additional storage config\n  storage_config:\n    hedging:\n      at: \"250ms\"\n      max_per_second: 20\n      up_to: 3\n\nenterprise:\n  # Enable enterprise features, license must be provided\n  enabled: false\n\n  # Default verion of GEL to deploy\n  version: v1.5.0\n\n  # -- Grafana Enterprise Logs license\n  # In order to use Grafana Enterprise Logs features, you will need to provide\n  # the contents of your Grafana Enterprise Logs license, either by providing the\n  # contents of the license.jwt, or the name Kubernetes Secret that contains your\n  # license.jwt.\n  # To set the license contents, use the flag `--set-file 'license.contents=./license.jwt'`\n  license:\n    contents: \"NOTAVALIDLICENSE\"\n\n  # -- Set to true when providing an external license\n  useExternalLicense: false\n\n  # -- Name of external licesne secret to use\n  externalLicenseName: null\n\n  # -- If enabled, the correct admin_client storage will be configured. If disabled while running enterprise,\n  # make sure auth is set to `type: trust`, or that `auth_enabled` is set to `false`.\n  adminApi:\n    enabled: true\n\n  # enterprise specific sections of the config.yaml file\n  config: |\n    {{- if .Values.enterprise.adminApi.enabled }}\n    {{- if or .Values.minio.enabled (eq .Values.loki.storage.type \"s3\") (eq .Values.loki.storage.type \"gcs\") }}\n    admin_client:\n      storage:\n        s3:\n          bucket_name: {{ .Values.loki.storage.bucketNames.admin }}\n    {{- end }}\n    {{- end }}\n    auth:\n      type: {{ .Values.enterprise.adminApi.enabled | ternary \"enterprise\" \"trust\" }}\n    auth_enabled: {{ .Values.loki.auth_enabled }}\n    cluster_name: {{ .Release.Name }}\n    license:\n      path: /etc/loki/license/license.jwt\n\n  image:\n    # -- The Docker registry\n    registry: docker.io\n    # -- Docker image repository\n    repository: grafana/enterprise-logs\n    # -- Overrides the image tag whose default is the chart's appVersion\n    tag: v1.4.0\n    # -- Docker image pull policy\n    pullPolicy: IfNotPresent\n\n  # -- Configuration for `tokengen` target\n  tokengen:\n    # -- Whether the job should be part of the deployment\n    enabled: true\n    # -- Name of the secret to store the admin token in\n    adminTokenSecret: \"gel-admin-token\"\n    # -- Additional CLI arguments for the `tokengen` target\n    extraArgs: []\n    # -- Additional Kubernetes environment\n    env: []\n    # -- Additional labels for the `tokengen` Job\n    labels: {}\n    # -- Additional annotations for the `tokengen` Job\n    annotations: {}\n    # -- Additional volumes for Pods\n    extraVolumes: []\n    # -- Additional volume mounts for Pods\n    extraVolumeMounts: []\n    # -- Run containers as user `enterprise-logs(uid=10001)`\n    securityContext:\n      runAsNonRoot: true\n      runAsGroup: 10001\n      runAsUser: 10001\n      fsGroup: 10001\n\n  nginxConfig:\n    file: |\n      worker_processes  5;  ## Default: 1\n      error_log  /dev/stderr;\n      pid        /tmp/nginx.pid;\n      worker_rlimit_nofile 8192;\n\n      events {\n        worker_connections  4096;  ## Default: 1024\n      }\n\n      http {\n        client_body_temp_path /tmp/client_temp;\n        proxy_temp_path       /tmp/proxy_temp_path;\n        fastcgi_temp_path     /tmp/fastcgi_temp;\n        uwsgi_temp_path       /tmp/uwsgi_temp;\n        scgi_temp_path        /tmp/scgi_temp;\n\n        proxy_http_version    1.1;\n\n        default_type application/octet-stream;\n        log_format   {{ .Values.gateway.nginxConfig.logFormat }}\n\n        {{- if .Values.gateway.verboseLogging }}\n        access_log   /dev/stderr  main;\n        {{- else }}\n\n        map $status $loggable {\n          ~^[23]  0;\n          default 1;\n        }\n        access_log   /dev/stderr  main  if=$loggable;\n        {{- end }}\n\n        sendfile     on;\n        tcp_nopush   on;\n        resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};\n\n        {{- with .Values.gateway.nginxConfig.httpSnippet }}\n        {{ . | nindent 2 }}\n        {{- end }}\n\n        server {\n          listen             8080;\n\n          {{- if .Values.gateway.basicAuth.enabled }}\n          auth_basic           \"Loki\";\n          auth_basic_user_file /etc/nginx/secrets/.htpasswd;\n          {{- end }}\n\n          location = / {\n            return 200 'OK';\n            auth_basic off;\n          }\n\n          location = /api/prom/push {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /api/prom/tail {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection \"upgrade\";\n          }\n\n          location ~ /api/prom/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /prometheus/api/v1/alerts.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /prometheus/api/v1/rules.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /loki/api/v1/push {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /loki/api/v1/tail {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection \"upgrade\";\n          }\n\n          location ~ /loki/api/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /admin/api/.* {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /compactor/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /distributor/.* {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /ring {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /ingester/.* {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /ruler/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /scheduler/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          {{- with .Values.gateway.nginxConfig.serverSnippet }}\n          {{ . | nindent 4 }}\n          {{- end }}\n        }\n      }\n\nserviceAccount:\n  # -- Specifies whether a ServiceAccount should be created\n  create: true\n  # -- The name of the ServiceAccount to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: null\n  # -- Image pull secrets for the service account\n  imagePullSecrets: []\n  # -- Annotations for the service account\n  annotations: {}\n  # -- Set this toggle to false to opt out of automounting API credentials for the service account\n  automountServiceAccountToken: true\n\n# RBAC configuration\nrbac:\n  # -- If pspEnabled true, a PodSecurityPolicy is created for K8s that use psp.\n  pspEnabled: false\n  # -- For OpenShift set pspEnabled to 'false' and sccEnabled to 'true' to use the SecurityContextConstraints.\n  sccEnabled: false\n\n# Monitoring section determines which monitoring features to enable\nmonitoring:\n  # Dashboards for monitoring Loki\n  dashboards:\n    # -- If enabled, create configmap with dashboards for monitoring Loki\n    enabled: true\n    # -- Alternative namespace to create dashboards ConfigMap in\n    namespace: null\n    # -- Additional annotations for the dashboards ConfigMap\n    annotations: {}\n    # -- Additional labels for the dashboards ConfigMap\n    labels: {}\n\n  # Recording rules for monitoring Loki, required for some dashboards\n  rules:\n    # -- If enabled, create PrometheusRule resource with Loki recording rules\n    enabled: true\n    # -- Include alerting rules\n    alerting: true\n    # -- Alternative namespace to create recording rules PrometheusRule resource in\n    namespace: null\n    # -- Additional annotations for the rules PrometheusRule resource\n    annotations: {}\n    # -- Additional labels for the rules PrometheusRule resource\n    labels: {}\n    # -- Additional groups to add to the rules file\n    additionalGroups: []\n    # - name: additional-loki-rules\n    #   rules:\n    #     - record: job:loki_request_duration_seconds_bucket:sum_rate\n    #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job)\n    #     - record: job_route:loki_request_duration_seconds_bucket:sum_rate\n    #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job, route)\n    #     - record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate\n    #       expr: sum(rate(container_cpu_usage_seconds_total[1m])) by (node, namespace, pod, container)\n\n  # Alerting rules for monitoring Loki\n  alerts:\n    # -- If enabled, create PrometheusRule resource with Loki alerting rules\n    enabled: true\n    # -- Alternative namespace to create alerting rules PrometheusRule resource in\n    namespace: null\n    # -- Additional annotations for the alerts PrometheusRule resource\n    annotations: {}\n    # -- Additional labels for the alerts PrometheusRule resource\n    labels: {}\n\n  # ServiceMonitor configuration\n  serviceMonitor:\n    # -- If enabled, ServiceMonitor resources for Prometheus Operator are created\n    enabled: true\n    # -- Alternative namespace for ServiceMonitor resources\n    namespace: null\n    # -- Namespace selector for ServiceMonitor resources\n    namespaceSelector: {}\n    # -- ServiceMonitor annotations\n    annotations: {}\n    # -- Additional ServiceMonitor labels\n    labels: {}\n    # -- ServiceMonitor scrape interval\n    interval: null\n    # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)\n    scrapeTimeout: null\n    # -- ServiceMonitor relabel configs to apply to samples before scraping\n    # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n    relabelings: []\n    # -- ServiceMonitor will use http by default, but you can pick https as well\n    scheme: http\n    # -- ServiceMonitor will use these tlsConfig settings to make the health check requests\n    tlsConfig: null\n\n  # Self monitoring determines whether Loki should scrape it's own logs.\n  # This feature currently relies on the Grafana Agent Operator being installed,\n  # which is installed by default using the grafana-agent-operator sub-chart.\n  # It will create custom resources for GrafanaAgent, LogsInstance, and PodLogs to configure\n  # scrape configs to scrape it's own logs with the labels expected by the included dashboards.\n  selfMonitoring:\n    enabled: false\n\n    # Grafana Agent configuration\n    grafanaAgent:\n      # -- Controls whether to install the Grafana Agent Operator and its CRDs.\n      # Note that helm will not install CRDs if this flag is enabled during an upgrade.\n      # In that case install the CRDs manually from https://github.com/grafana/agent/tree/main/production/operator/crds\n      installOperator: false\n      # -- Alternative namespace for Grafana Agent resources\n      namespace: null\n      # -- Grafana Agent annotations\n      annotations: {}\n      # -- Additional Grafana Agent labels\n      labels: {}\n      # -- Enable the config read api on port 8080 of the agent\n      enableConfigReadAPI: false\n\n    # PodLogs configuration\n    podLogs:\n      # -- Alternative namespace for PodLogs resources\n      namespace: null\n      # -- PodLogs annotations\n      annotations: {}\n      # -- Additional PodLogs labels\n      labels: {}\n      # -- PodLogs relabel configs to apply to samples before scraping\n      # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n      relabelings: []\n\n    # LogsInstance configuration\n    logsInstance:\n      # -- Alternative namespace for LogsInstance resources\n      namespace: null\n      # -- LogsInstance annotations\n      annotations: {}\n      # -- Additional LogsInstance labels\n      labels: {}\n\n# Configuration for the write\nwrite:\n  # -- Number of replicas for the write\n  replicas: 3\n  image:\n    # -- The Docker registry for the write image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the write image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the write image. Overrides `loki.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for write pods\n  priorityClassName: null\n  # -- Annotations for write pods\n  podAnnotations: {}\n  # -- Additional selector labels for each `write` pod\n  selectorLabels: {}\n  # -- Labels for ingestor service\n  serviceLabels: {}\n  # -- Additional CLI args for the write\n  extraArgs: []\n  # -- Environment variables to add to the write pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the write pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the write pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the write pods\n  extraVolumes: []\n  # -- Resource requests and limits for the write\n  resources: {}\n  # -- Grace period to allow the write to shutdown before it is killed. Especially for the ingestor,\n  # this must be increased. It must be long enough so writes can be gracefully shutdown flushing/transferring\n  # all data and to successfully leave the member ring on shutdown.\n  terminationGracePeriodSeconds: 300\n  # -- Affinity for write pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"loki.writeSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n  # -- Node selector for write pods\n  nodeSelector: {}\n  # -- Tolerations for write pods\n  tolerations: []\n  persistence:\n    # -- Size of persistent disk\n    size: 10Gi\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n\n# Configuration for the read node(s)\nread:\n  # -- Number of replicas for the read\n  replicas: 3\n  autoscaling:\n    # -- Enable autoscaling for the read, this is only used if `queryIndex.enabled: true`\n    enabled: false\n    # -- Minimum autoscaling replicas for the read\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the read\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the read\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the read\n    targetMemoryUtilizationPercentage:\n  image:\n    # -- The Docker registry for the read image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the read image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the read image. Overrides `loki.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for read pods\n  priorityClassName: null\n  # -- Annotations for read pods\n  podAnnotations: {}\n  # -- Additional selecto labels for each `read` pod\n  selectorLabels: {}\n  # -- Labels for read service\n  serviceLabels: {}\n  # -- Additional CLI args for the read\n  extraArgs: []\n  # -- Environment variables to add to the read pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the read pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the read pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the read pods\n  extraVolumes: []\n  # -- Resource requests and limits for the read\n  resources: {}\n  # -- Grace period to allow the read to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for read pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"loki.readSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n  # -- Node selector for read pods\n  nodeSelector: {}\n  # -- Tolerations for read pods\n  tolerations: []\n  persistence:\n    # -- Size of persistent disk\n    size: 10Gi\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n\n# Configuration for the gateway\ngateway:\n  # -- Specifies whether the gateway should be enabled\n  enabled: true\n  # -- Number of replicas for the gateway\n  replicas: 1\n  # -- Enable logging of 2xx and 3xx HTTP requests\n  verboseLogging: true\n  autoscaling:\n    # -- Enable autoscaling for the gateway\n    enabled: false\n    # -- Minimum autoscaling replicas for the gateway\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the gateway\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the gateway\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the gateway\n    targetMemoryUtilizationPercentage:\n  # -- See `kubectl explain deployment.spec.strategy` for more\n  # -- ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy\n  deploymentStrategy:\n    type: RollingUpdate\n  image:\n    # -- The Docker registry for the gateway image\n    registry: docker.io\n    # -- The gateway image repository\n    repository: nginxinc/nginx-unprivileged\n    # -- The gateway image tag\n    tag: 1.19-alpine\n    # -- The gateway image pull policy\n    pullPolicy: IfNotPresent\n  # -- The name of the PriorityClass for gateway pods\n  priorityClassName: null\n  # -- Annotations for gateway pods\n  podAnnotations: {}\n  # -- Additional CLI args for the gateway\n  extraArgs: []\n  # -- Environment variables to add to the gateway pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the gateway pods\n  extraEnvFrom: []\n  # -- Volumes to add to the gateway pods\n  extraVolumes: []\n  # -- Volume mounts to add to the gateway pods\n  extraVolumeMounts: []\n  # -- The SecurityContext for gateway containers\n  podSecurityContext:\n    fsGroup: 101\n    runAsGroup: 101\n    runAsNonRoot: true\n    runAsUser: 101\n  # -- The SecurityContext for gateway containers\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop:\n        - ALL\n    allowPrivilegeEscalation: false\n  # -- Resource requests and limits for the gateway\n  resources: {}\n  # -- Grace period to allow the gateway to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for gateway pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"loki.gatewaySelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n  # -- Node selector for gateway pods\n  nodeSelector: {}\n  # -- Tolerations for gateway pods\n  tolerations: []\n  # Gateway service configuration\n  service:\n    # -- Port of the gateway service\n    port: 80\n    # -- Type of the gateway service\n    type: ClusterIP\n    # -- ClusterIP of the gateway service\n    clusterIP: null\n    # -- (int) Node port if service type is NodePort\n    nodePort: null\n    # -- Load balancer IPO address if service type is LoadBalancer\n    loadBalancerIP: null\n    # -- Annotations for the gateway service\n    annotations: {}\n    # -- Labels for gateway service\n    labels: {}\n  # Gateway ingress configuration\n  ingress:\n    # -- Specifies whether an ingress for the gateway should be created\n    enabled: false\n    # -- Ingress Class Name. MAY be required for Kubernetes versions \u003e= 1.18\n    # ingressClassName: nginx\n    # -- Annotations for the gateway ingress\n    annotations: {}\n    # -- Hosts configuration for the gateway ingress\n    hosts:\n      - host: gateway.loki.example.com\n        paths:\n          - path: /\n            # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers\n            # pathType: Prefix\n    # -- TLS configuration for the gateway ingress\n    tls:\n      - secretName: loki-gateway-tls\n        hosts:\n          - gateway.loki.example.com\n  # Basic auth configuration\n  basicAuth:\n    # -- Enables basic authentication for the gateway\n    enabled: false\n    # -- The basic auth username for the gateway\n    username: null\n    # -- The basic auth password for the gateway\n    password: null\n    # -- Uses the specified username and password to compute a htpasswd using Sprig's `htpasswd` function.\n    # The value is templated using `tpl`. Override this to use a custom htpasswd, e.g. in case the default causes\n    # high CPU load.\n    htpasswd: \u003e-\n      {{ htpasswd (required \"'gateway.basicAuth.username' is required\" .Values.gateway.basicAuth.username) (required \"'gateway.basicAuth.password' is required\" .Values.gateway.basicAuth.password) }}\n\n    # -- Existing basic auth secret to use. Must contain '.htpasswd'\n    existingSecret: null\n  # Configures the readiness probe for the gateway\n  readinessProbe:\n    httpGet:\n      path: /\n      port: http\n    initialDelaySeconds: 15\n    timeoutSeconds: 1\n  nginxConfig:\n    # -- NGINX log format\n    logFormat: |-\n      main '$remote_addr - $remote_user [$time_local]  $status '\n              '\"$request\" $body_bytes_sent \"$http_referer\" '\n              '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n    # -- Allows appending custom configuration to the server block\n    serverSnippet: \"\"\n    # -- Allows appending custom configuration to the http block\n    httpSnippet: \"\"\n    # -- Config file contents for Nginx. Passed through the `tpl` function to allow templating\n    # @default -- See values.yaml\n    file: |\n      worker_processes  5;  ## Default: 1\n      error_log  /dev/stderr;\n      pid        /tmp/nginx.pid;\n      worker_rlimit_nofile 8192;\n\n      events {\n        worker_connections  4096;  ## Default: 1024\n      }\n\n      http {\n        client_body_temp_path /tmp/client_temp;\n        proxy_temp_path       /tmp/proxy_temp_path;\n        fastcgi_temp_path     /tmp/fastcgi_temp;\n        uwsgi_temp_path       /tmp/uwsgi_temp;\n        scgi_temp_path        /tmp/scgi_temp;\n\n        proxy_http_version    1.1;\n\n        default_type application/octet-stream;\n        log_format   {{ .Values.gateway.nginxConfig.logFormat }}\n\n        {{- if .Values.gateway.verboseLogging }}\n        access_log   /dev/stderr  main;\n        {{- else }}\n\n        map $status $loggable {\n          ~^[23]  0;\n          default 1;\n        }\n        access_log   /dev/stderr  main  if=$loggable;\n        {{- end }}\n\n        sendfile     on;\n        tcp_nopush   on;\n        resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};\n\n        {{- with .Values.gateway.nginxConfig.httpSnippet }}\n        {{ . | nindent 2 }}\n        {{- end }}\n\n        server {\n          listen             8080;\n\n          {{- if .Values.gateway.basicAuth.enabled }}\n          auth_basic           \"Loki\";\n          auth_basic_user_file /etc/nginx/secrets/.htpasswd;\n          {{- end }}\n\n          location = / {\n            return 200 'OK';\n            auth_basic off;\n          }\n\n          location = /api/prom/push {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /api/prom/tail {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection \"upgrade\";\n          }\n\n          location ~ /api/prom/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /loki/api/v1/push {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /loki/api/v1/tail {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection \"upgrade\";\n          }\n\n          location ~ /loki/api/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          {{- with .Values.gateway.nginxConfig.serverSnippet }}\n          {{ . | nindent 4 }}\n          {{- end }}\n        }\n      }\n\nnetworkPolicy:\n  # -- Specifies whether Network Policies should be created\n  enabled: false\n  metrics:\n    # -- Specifies the Pods which are allowed to access the metrics port.\n    # As this is cross-namespace communication, you also need the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespaces which are allowed to access the metrics port\n    namespaceSelector: {}\n    # -- Specifies specific network CIDRs which are allowed to access the metrics port.\n    # In case you use namespaceSelector, you also have to specify your kubelet networks here.\n    # The metrics ports are also used for probes.\n    cidrs: []\n  ingress:\n    # -- Specifies the Pods which are allowed to access the http port.\n    # As this is cross-namespace communication, you also need the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespaces which are allowed to access the http port\n    namespaceSelector: {}\n  alertmanager:\n    # -- Specify the alertmanager port used for alerting\n    port: 9093\n    # -- Specifies the alertmanager Pods.\n    # As this is cross-namespace communication, you also need the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespace the alertmanager is running in\n    namespaceSelector: {}\n  externalStorage:\n    # -- Specify the port used for external storage, e.g. AWS S3\n    ports: []\n    # -- Specifies specific network CIDRs you want to limit access to\n    cidrs: []\n  discovery:\n    # -- (int) Specify the port used for discovery\n    port: null\n    # -- Specifies the Pods labels used for discovery.\n    # As this is cross-namespace communication, you also need the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespace the discovery Pods are running in\n    namespaceSelector: {}\n\n# -------------------------------------\n# Configuration for `minio` child chart\n# -------------------------------------\nminio:\n  enabled: true\n  # accessKey: enterprise-logs\n  # secretKey: supersecret\n  buckets:\n    - name: chunks\n      policy: none\n      purge: false\n    - name: ruler\n      policy: none\n      purge: false\n    - name: admin\n      policy: none\n      purge: false\n  persistence:\n    size: 5Gi\n  resources:\n    requests:\n      cpu: 100m\n      memory: 128Mi\n",
            "template": "---\nglobal:\n  image:\n    # -- Overrides the Docker registry globally for all images\n    registry: null\n  # -- Overrides the priorityClassName for all pods\n  priorityClassName: null\n  # -- configures cluster domain (\"cluster.local\" by default)\n  clusterDomain: \"cluster.local\"\n  # -- configures DNS service name\n  dnsService: \"kube-dns\"\n  # -- configures DNS service namespace\n  dnsNamespace: \"kube-system\"\n\n# -- Overrides the chart's name\nnameOverride: null\n\n# -- Overrides the chart's computed fullname\nfullnameOverride: null\n\n# -- Image pull secrets for Docker images\nimagePullSecrets: []\n\nloki:\n  # Configures the readiness probe for all of the Loki pods\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 30\n    timeoutSeconds: 1\n  image:\n    # -- The Docker registry\n    registry: docker.io\n    # -- Docker image repository\n    repository: grafana/loki\n    # -- Overrides the image tag whose default is the chart's appVersion\n    tag: null\n    # -- Docker image pull policy\n    pullPolicy: IfNotPresent\n  # -- Common annotations for all pods\n  podAnnotations: {}\n  # -- The number of old ReplicaSets to retain to allow rollback\n  revisionHistoryLimit: 10\n  # -- The SecurityContext for Loki pods\n  podSecurityContext:\n    fsGroup: 10001\n    runAsGroup: 10001\n    runAsNonRoot: true\n    runAsUser: 10001\n  # -- The SecurityContext for Loki containers\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop:\n        - ALL\n    allowPrivilegeEscalation: false\n  # -- Specify an existing secret containing loki configuration. If non-empty, overrides `loki.config`\n  existingSecretForConfig: \"\"\n  # -- Config file contents for Loki\n  # @default -- See values.yaml\n  config: |\n    {{- if .Values.enterprise.enabled}}\n    {{- tpl .Values.enterprise.config . }}\n    {{- else }}\n    auth_enabled: {{ .Values.loki.auth_enabled }}\n    {{- end }}\n\n    server:\n      http_listen_port: 3100\n      grpc_listen_port: 9095\n\n    memberlist:\n      join_members:\n        - {{ include \"loki.name\" . }}-memberlist\n\n    {{- if .Values.loki.commonConfig}}\n    common:\n    {{- toYaml .Values.loki.commonConfig | nindent 2}}\n      storage:\n      {{- include \"loki.commonStorageConfig\" . | nindent 4}}\n    {{- end}}\n\n    limits_config:\n      enforce_metric_name: false\n      reject_old_samples: true\n      reject_old_samples_max_age: 168h\n      max_cache_freshness_per_query: 10m\n      split_queries_by_interval: 15m\n\n    {{- with .Values.loki.memcached.chunk_cache }}\n    {{- if and .enabled .host }}\n    chunk_store_config:\n      chunk_cache_config:\n        memcached:\n          batch_size: {{ .batch_size }}\n          parallelism: {{ .parallelism }}\n        memcached_client:\n          host: {{ .host }}\n          service: {{ .service }}\n    {{- end }}\n    {{- end }}\n\n    {{- if .Values.loki.schemaConfig}}\n    schema_config:\n    {{- toYaml .Values.loki.schemaConfig | nindent 2}}\n    {{- else }}\n    schema_config:\n      configs:\n        - from: 2022-01-11\n          store: boltdb-shipper\n          {{- if eq .Values.loki.storage.type \"s3\" }}\n          object_store: s3\n          {{- else if eq .Values.loki.storage.type \"gcs\" }}\n          object_store: gcs\n          {{- else }}\n          object_store: filesystem\n          {{- end }}\n          schema: v12\n          index:\n            prefix: loki_index_\n            period: 24h\n    {{- end }}\n\n    {{- if or .Values.minio.enabled (eq .Values.loki.storage.type \"s3\") (eq .Values.loki.storage.type \"gcs\") }}\n    ruler:\n      storage:\n      {{- include \"loki.rulerStorageConfig\" . | nindent 4}}\n    {{- end -}}\n\n    {{- with .Values.loki.memcached.results_cache }}\n    query_range:\n      align_queries_with_step: true\n      {{- if and .enabled .host }}\n      cache_results: {{ .enabled }}\n      results_cache:\n        cache:\n          default_validity: {{ .default_validity }}\n          memcached_client:\n            host: {{ .host }}\n            service: {{ .service }}\n            timeout: {{ .timeout }}\n      {{- end }}\n    {{- end }}\n\n    {{- with .Values.loki.storage_config }}\n    storage_config:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    {{- with .Values.loki.query_scheduler }}\n    query_scheduler:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n  # Should authentication be enabled\n  auth_enabled: false\n\n  # -- Check https://grafana.com/docs/loki/latest/configuration/#common_config for more info on how to provide a common configuration\n  commonConfig:\n    path_prefix: /var/loki\n    replication_factor: 3\n\n  storage:\n    bucketNames:\n      chunks: chunks\n      ruler: ruler\n      admin: admin\n    type: s3\n    s3:\n      s3: null\n      endpoint: null\n      region: null\n      secretAccessKey: null\n      accessKeyId: null\n      s3ForcePathStyle: false\n      insecure: false\n    gcs:\n      chunkBufferSize: 0\n      requestTimeout: \"0s\"\n      enableHttp2: true\n    local:\n      chunks_directory: /var/loki/chunks\n      rules_directory: /var/loki/rules\n\n  # -- Configure memcached as an external cache for chunk and results cache. Disabled by default\n  # must enable and specify a host for each cache you would like to use.\n  memcached:\n    chunk_cache:\n      enabled: false\n      host: \"\"\n      service: \"memcached-client\"\n      batch_size: 256\n      parallelism: 10\n    results_cache:\n      enabled: false\n      host: \"\"\n      service: \"memcached-client\"\n      timeout: \"500ms\"\n      default_validity: \"12h\"\n\n  # -- Check https://grafana.com/docs/loki/latest/configuration/#schema_config for more info on how to configure schemas\n  schemaConfig: {}\n\n  # -- Structured loki configuration, takes precedence over `loki.config`, `loki.schemaConfig`, `loki.storageConfig`\n  structuredConfig: {}\n\n  # -- Additional query scheduler config\n  query_scheduler: {}\n\n  # -- Additional storage config\n  storage_config:\n    hedging:\n      at: \"250ms\"\n      max_per_second: 20\n      up_to: 3\n\nenterprise:\n  # Enable enterprise features, license must be provided\n  enabled: false\n\n  # Default verion of GEL to deploy\n  version: v1.5.0\n\n  # -- Grafana Enterprise Logs license\n  # In order to use Grafana Enterprise Logs features, you will need to provide\n  # the contents of your Grafana Enterprise Logs license, either by providing the\n  # contents of the license.jwt, or the name Kubernetes Secret that contains your\n  # license.jwt.\n  # To set the license contents, use the flag `--set-file 'license.contents=./license.jwt'`\n  license:\n    contents: \"NOTAVALIDLICENSE\"\n\n  # -- Set to true when providing an external license\n  useExternalLicense: false\n\n  # -- Name of external licesne secret to use\n  externalLicenseName: null\n\n  # -- If enabled, the correct admin_client storage will be configured. If disabled while running enterprise,\n  # make sure auth is set to `type: trust`, or that `auth_enabled` is set to `false`.\n  adminApi:\n    enabled: true\n\n  # enterprise specific sections of the config.yaml file\n  config: |\n    {{- if .Values.enterprise.adminApi.enabled }}\n    {{- if or .Values.minio.enabled (eq .Values.loki.storage.type \"s3\") (eq .Values.loki.storage.type \"gcs\") }}\n    admin_client:\n      storage:\n        s3:\n          bucket_name: {{ .Values.loki.storage.bucketNames.admin }}\n    {{- end }}\n    {{- end }}\n    auth:\n      type: {{ .Values.enterprise.adminApi.enabled | ternary \"enterprise\" \"trust\" }}\n    auth_enabled: {{ .Values.loki.auth_enabled }}\n    cluster_name: {{ .Release.Name }}\n    license:\n      path: /etc/loki/license/license.jwt\n\n  image:\n    # -- The Docker registry\n    registry: docker.io\n    # -- Docker image repository\n    repository: grafana/enterprise-logs\n    # -- Overrides the image tag whose default is the chart's appVersion\n    tag: v1.4.0\n    # -- Docker image pull policy\n    pullPolicy: IfNotPresent\n\n  # -- Configuration for `tokengen` target\n  tokengen:\n    # -- Whether the job should be part of the deployment\n    enabled: true\n    # -- Name of the secret to store the admin token in\n    adminTokenSecret: \"gel-admin-token\"\n    # -- Additional CLI arguments for the `tokengen` target\n    extraArgs: []\n    # -- Additional Kubernetes environment\n    env: []\n    # -- Additional labels for the `tokengen` Job\n    labels: {}\n    # -- Additional annotations for the `tokengen` Job\n    annotations: {}\n    # -- Additional volumes for Pods\n    extraVolumes: []\n    # -- Additional volume mounts for Pods\n    extraVolumeMounts: []\n    # -- Run containers as user `enterprise-logs(uid=10001)`\n    securityContext:\n      runAsNonRoot: true\n      runAsGroup: 10001\n      runAsUser: 10001\n      fsGroup: 10001\n\n  nginxConfig:\n    file: |\n      worker_processes  5;  ## Default: 1\n      error_log  /dev/stderr;\n      pid        /tmp/nginx.pid;\n      worker_rlimit_nofile 8192;\n\n      events {\n        worker_connections  4096;  ## Default: 1024\n      }\n\n      http {\n        client_body_temp_path /tmp/client_temp;\n        proxy_temp_path       /tmp/proxy_temp_path;\n        fastcgi_temp_path     /tmp/fastcgi_temp;\n        uwsgi_temp_path       /tmp/uwsgi_temp;\n        scgi_temp_path        /tmp/scgi_temp;\n\n        proxy_http_version    1.1;\n\n        default_type application/octet-stream;\n        log_format   {{ .Values.gateway.nginxConfig.logFormat }}\n\n        {{- if .Values.gateway.verboseLogging }}\n        access_log   /dev/stderr  main;\n        {{- else }}\n\n        map $status $loggable {\n          ~^[23]  0;\n          default 1;\n        }\n        access_log   /dev/stderr  main  if=$loggable;\n        {{- end }}\n\n        sendfile     on;\n        tcp_nopush   on;\n        resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};\n\n        {{- with .Values.gateway.nginxConfig.httpSnippet }}\n        {{ . | nindent 2 }}\n        {{- end }}\n\n        server {\n          listen             8080;\n\n          {{- if .Values.gateway.basicAuth.enabled }}\n          auth_basic           \"Loki\";\n          auth_basic_user_file /etc/nginx/secrets/.htpasswd;\n          {{- end }}\n\n          location = / {\n            return 200 'OK';\n            auth_basic off;\n          }\n\n          location = /api/prom/push {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /api/prom/tail {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection \"upgrade\";\n          }\n\n          location ~ /api/prom/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /prometheus/api/v1/alerts.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /prometheus/api/v1/rules.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /loki/api/v1/push {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /loki/api/v1/tail {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection \"upgrade\";\n          }\n\n          location ~ /loki/api/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /admin/api/.* {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /compactor/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /distributor/.* {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /ring {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /ingester/.* {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /ruler/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /scheduler/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          {{- with .Values.gateway.nginxConfig.serverSnippet }}\n          {{ . | nindent 4 }}\n          {{- end }}\n        }\n      }\n\nserviceAccount:\n  # -- Specifies whether a ServiceAccount should be created\n  create: true\n  # -- The name of the ServiceAccount to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: null\n  # -- Image pull secrets for the service account\n  imagePullSecrets: []\n  # -- Annotations for the service account\n  annotations: {}\n  # -- Set this toggle to false to opt out of automounting API credentials for the service account\n  automountServiceAccountToken: true\n\n# RBAC configuration\nrbac:\n  # -- If pspEnabled true, a PodSecurityPolicy is created for K8s that use psp.\n  pspEnabled: false\n  # -- For OpenShift set pspEnabled to 'false' and sccEnabled to 'true' to use the SecurityContextConstraints.\n  sccEnabled: false\n\n# Monitoring section determines which monitoring features to enable\nmonitoring:\n  # Dashboards for monitoring Loki\n  dashboards:\n    # -- If enabled, create configmap with dashboards for monitoring Loki\n    enabled: true\n    # -- Alternative namespace to create dashboards ConfigMap in\n    namespace: null\n    # -- Additional annotations for the dashboards ConfigMap\n    annotations: {}\n    # -- Additional labels for the dashboards ConfigMap\n    labels: {}\n\n  # Recording rules for monitoring Loki, required for some dashboards\n  rules:\n    # -- If enabled, create PrometheusRule resource with Loki recording rules\n    enabled: true\n    # -- Include alerting rules\n    alerting: true\n    # -- Alternative namespace to create recording rules PrometheusRule resource in\n    namespace: null\n    # -- Additional annotations for the rules PrometheusRule resource\n    annotations: {}\n    # -- Additional labels for the rules PrometheusRule resource\n    labels: {}\n    # -- Additional groups to add to the rules file\n    additionalGroups: []\n    # - name: additional-loki-rules\n    #   rules:\n    #     - record: job:loki_request_duration_seconds_bucket:sum_rate\n    #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job)\n    #     - record: job_route:loki_request_duration_seconds_bucket:sum_rate\n    #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job, route)\n    #     - record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate\n    #       expr: sum(rate(container_cpu_usage_seconds_total[1m])) by (node, namespace, pod, container)\n\n  # Alerting rules for monitoring Loki\n  alerts:\n    # -- If enabled, create PrometheusRule resource with Loki alerting rules\n    enabled: true\n    # -- Alternative namespace to create alerting rules PrometheusRule resource in\n    namespace: null\n    # -- Additional annotations for the alerts PrometheusRule resource\n    annotations: {}\n    # -- Additional labels for the alerts PrometheusRule resource\n    labels: {}\n\n  # ServiceMonitor configuration\n  serviceMonitor:\n    # -- If enabled, ServiceMonitor resources for Prometheus Operator are created\n    enabled: true\n    # -- Alternative namespace for ServiceMonitor resources\n    namespace: null\n    # -- Namespace selector for ServiceMonitor resources\n    namespaceSelector: {}\n    # -- ServiceMonitor annotations\n    annotations: {}\n    # -- Additional ServiceMonitor labels\n    labels: {}\n    # -- ServiceMonitor scrape interval\n    interval: null\n    # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)\n    scrapeTimeout: null\n    # -- ServiceMonitor relabel configs to apply to samples before scraping\n    # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n    relabelings: []\n    # -- ServiceMonitor will use http by default, but you can pick https as well\n    scheme: http\n    # -- ServiceMonitor will use these tlsConfig settings to make the health check requests\n    tlsConfig: null\n\n  # Self monitoring determines whether Loki should scrape it's own logs.\n  # This feature currently relies on the Grafana Agent Operator being installed,\n  # which is installed by default using the grafana-agent-operator sub-chart.\n  # It will create custom resources for GrafanaAgent, LogsInstance, and PodLogs to configure\n  # scrape configs to scrape it's own logs with the labels expected by the included dashboards.\n  selfMonitoring:\n    enabled: false\n\n    # Grafana Agent configuration\n    grafanaAgent:\n      # -- Controls whether to install the Grafana Agent Operator and its CRDs.\n      # Note that helm will not install CRDs if this flag is enabled during an upgrade.\n      # In that case install the CRDs manually from https://github.com/grafana/agent/tree/main/production/operator/crds\n      installOperator: false\n      # -- Alternative namespace for Grafana Agent resources\n      namespace: null\n      # -- Grafana Agent annotations\n      annotations: {}\n      # -- Additional Grafana Agent labels\n      labels: {}\n      # -- Enable the config read api on port 8080 of the agent\n      enableConfigReadAPI: false\n\n    # PodLogs configuration\n    podLogs:\n      # -- Alternative namespace for PodLogs resources\n      namespace: null\n      # -- PodLogs annotations\n      annotations: {}\n      # -- Additional PodLogs labels\n      labels: {}\n      # -- PodLogs relabel configs to apply to samples before scraping\n      # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n      relabelings: []\n\n    # LogsInstance configuration\n    logsInstance:\n      # -- Alternative namespace for LogsInstance resources\n      namespace: null\n      # -- LogsInstance annotations\n      annotations: {}\n      # -- Additional LogsInstance labels\n      labels: {}\n\n# Configuration for the write\nwrite:\n  # -- Number of replicas for the write\n  replicas: 3\n  image:\n    # -- The Docker registry for the write image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the write image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the write image. Overrides `loki.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for write pods\n  priorityClassName: null\n  # -- Annotations for write pods\n  podAnnotations: {}\n  # -- Additional selector labels for each `write` pod\n  selectorLabels: {}\n  # -- Labels for ingestor service\n  serviceLabels: {}\n  # -- Additional CLI args for the write\n  extraArgs: []\n  # -- Environment variables to add to the write pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the write pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the write pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the write pods\n  extraVolumes: []\n  # -- Resource requests and limits for the write\n  resources: {}\n  # -- Grace period to allow the write to shutdown before it is killed. Especially for the ingestor,\n  # this must be increased. It must be long enough so writes can be gracefully shutdown flushing/transferring\n  # all data and to successfully leave the member ring on shutdown.\n  terminationGracePeriodSeconds: 300\n  # -- Affinity for write pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"loki.writeSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n  # -- Node selector for write pods\n  nodeSelector: {}\n  # -- Tolerations for write pods\n  tolerations: []\n  persistence:\n    # -- Size of persistent disk\n    size: 10Gi\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n\n# Configuration for the read node(s)\nread:\n  # -- Number of replicas for the read\n  replicas: 3\n  autoscaling:\n    # -- Enable autoscaling for the read, this is only used if `queryIndex.enabled: true`\n    enabled: false\n    # -- Minimum autoscaling replicas for the read\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the read\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the read\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the read\n    targetMemoryUtilizationPercentage:\n  image:\n    # -- The Docker registry for the read image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the read image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the read image. Overrides `loki.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for read pods\n  priorityClassName: null\n  # -- Annotations for read pods\n  podAnnotations: {}\n  # -- Additional selecto labels for each `read` pod\n  selectorLabels: {}\n  # -- Labels for read service\n  serviceLabels: {}\n  # -- Additional CLI args for the read\n  extraArgs: []\n  # -- Environment variables to add to the read pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the read pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the read pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the read pods\n  extraVolumes: []\n  # -- Resource requests and limits for the read\n  resources: {}\n  # -- Grace period to allow the read to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for read pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"loki.readSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n  # -- Node selector for read pods\n  nodeSelector: {}\n  # -- Tolerations for read pods\n  tolerations: []\n  persistence:\n    # -- Size of persistent disk\n    size: 10Gi\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n\n# Configuration for the gateway\ngateway:\n  # -- Specifies whether the gateway should be enabled\n  enabled: true\n  # -- Number of replicas for the gateway\n  replicas: 1\n  # -- Enable logging of 2xx and 3xx HTTP requests\n  verboseLogging: true\n  autoscaling:\n    # -- Enable autoscaling for the gateway\n    enabled: false\n    # -- Minimum autoscaling replicas for the gateway\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the gateway\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the gateway\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the gateway\n    targetMemoryUtilizationPercentage:\n  # -- See `kubectl explain deployment.spec.strategy` for more\n  # -- ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy\n  deploymentStrategy:\n    type: RollingUpdate\n  image:\n    # -- The Docker registry for the gateway image\n    registry: docker.io\n    # -- The gateway image repository\n    repository: nginxinc/nginx-unprivileged\n    # -- The gateway image tag\n    tag: 1.19-alpine\n    # -- The gateway image pull policy\n    pullPolicy: IfNotPresent\n  # -- The name of the PriorityClass for gateway pods\n  priorityClassName: null\n  # -- Annotations for gateway pods\n  podAnnotations: {}\n  # -- Additional CLI args for the gateway\n  extraArgs: []\n  # -- Environment variables to add to the gateway pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the gateway pods\n  extraEnvFrom: []\n  # -- Volumes to add to the gateway pods\n  extraVolumes: []\n  # -- Volume mounts to add to the gateway pods\n  extraVolumeMounts: []\n  # -- The SecurityContext for gateway containers\n  podSecurityContext:\n    fsGroup: 101\n    runAsGroup: 101\n    runAsNonRoot: true\n    runAsUser: 101\n  # -- The SecurityContext for gateway containers\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop:\n        - ALL\n    allowPrivilegeEscalation: false\n  # -- Resource requests and limits for the gateway\n  resources: {}\n  # -- Grace period to allow the gateway to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for gateway pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"loki.gatewaySelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n  # -- Node selector for gateway pods\n  nodeSelector: {}\n  # -- Tolerations for gateway pods\n  tolerations: []\n  # Gateway service configuration\n  service:\n    # -- Port of the gateway service\n    port: 80\n    # -- Type of the gateway service\n    type: ClusterIP\n    # -- ClusterIP of the gateway service\n    clusterIP: null\n    # -- (int) Node port if service type is NodePort\n    nodePort: null\n    # -- Load balancer IPO address if service type is LoadBalancer\n    loadBalancerIP: null\n    # -- Annotations for the gateway service\n    annotations: {}\n    # -- Labels for gateway service\n    labels: {}\n  # Gateway ingress configuration\n  ingress:\n    # -- Specifies whether an ingress for the gateway should be created\n    enabled: false\n    # -- Ingress Class Name. MAY be required for Kubernetes versions \u003e= 1.18\n    # ingressClassName: nginx\n    # -- Annotations for the gateway ingress\n    annotations: {}\n    # -- Hosts configuration for the gateway ingress\n    hosts:\n      - host: gateway.loki.example.com\n        paths:\n          - path: /\n            # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers\n            # pathType: Prefix\n    # -- TLS configuration for the gateway ingress\n    tls:\n      - secretName: loki-gateway-tls\n        hosts:\n          - gateway.loki.example.com\n  # Basic auth configuration\n  basicAuth:\n    # -- Enables basic authentication for the gateway\n    enabled: false\n    # -- The basic auth username for the gateway\n    username: null\n    # -- The basic auth password for the gateway\n    password: null\n    # -- Uses the specified username and password to compute a htpasswd using Sprig's `htpasswd` function.\n    # The value is templated using `tpl`. Override this to use a custom htpasswd, e.g. in case the default causes\n    # high CPU load.\n    htpasswd: \u003e-\n      {{ htpasswd (required \"'gateway.basicAuth.username' is required\" .Values.gateway.basicAuth.username) (required \"'gateway.basicAuth.password' is required\" .Values.gateway.basicAuth.password) }}\n\n    # -- Existing basic auth secret to use. Must contain '.htpasswd'\n    existingSecret: null\n  # Configures the readiness probe for the gateway\n  readinessProbe:\n    httpGet:\n      path: /\n      port: http\n    initialDelaySeconds: 15\n    timeoutSeconds: 1\n  nginxConfig:\n    # -- NGINX log format\n    logFormat: |-\n      main '$remote_addr - $remote_user [$time_local]  $status '\n              '\"$request\" $body_bytes_sent \"$http_referer\" '\n              '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n    # -- Allows appending custom configuration to the server block\n    serverSnippet: \"\"\n    # -- Allows appending custom configuration to the http block\n    httpSnippet: \"\"\n    # -- Config file contents for Nginx. Passed through the `tpl` function to allow templating\n    # @default -- See values.yaml\n    file: |\n      worker_processes  5;  ## Default: 1\n      error_log  /dev/stderr;\n      pid        /tmp/nginx.pid;\n      worker_rlimit_nofile 8192;\n\n      events {\n        worker_connections  4096;  ## Default: 1024\n      }\n\n      http {\n        client_body_temp_path /tmp/client_temp;\n        proxy_temp_path       /tmp/proxy_temp_path;\n        fastcgi_temp_path     /tmp/fastcgi_temp;\n        uwsgi_temp_path       /tmp/uwsgi_temp;\n        scgi_temp_path        /tmp/scgi_temp;\n\n        proxy_http_version    1.1;\n\n        default_type application/octet-stream;\n        log_format   {{ .Values.gateway.nginxConfig.logFormat }}\n\n        {{- if .Values.gateway.verboseLogging }}\n        access_log   /dev/stderr  main;\n        {{- else }}\n\n        map $status $loggable {\n          ~^[23]  0;\n          default 1;\n        }\n        access_log   /dev/stderr  main  if=$loggable;\n        {{- end }}\n\n        sendfile     on;\n        tcp_nopush   on;\n        resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};\n\n        {{- with .Values.gateway.nginxConfig.httpSnippet }}\n        {{ . | nindent 2 }}\n        {{- end }}\n\n        server {\n          listen             8080;\n\n          {{- if .Values.gateway.basicAuth.enabled }}\n          auth_basic           \"Loki\";\n          auth_basic_user_file /etc/nginx/secrets/.htpasswd;\n          {{- end }}\n\n          location = / {\n            return 200 'OK';\n            auth_basic off;\n          }\n\n          location = /api/prom/push {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /api/prom/tail {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection \"upgrade\";\n          }\n\n          location ~ /api/prom/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /loki/api/v1/push {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /loki/api/v1/tail {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection \"upgrade\";\n          }\n\n          location ~ /loki/api/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          {{- with .Values.gateway.nginxConfig.serverSnippet }}\n          {{ . | nindent 4 }}\n          {{- end }}\n        }\n      }\n\nnetworkPolicy:\n  # -- Specifies whether Network Policies should be created\n  enabled: false\n  metrics:\n    # -- Specifies the Pods which are allowed to access the metrics port.\n    # As this is cross-namespace communication, you also need the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespaces which are allowed to access the metrics port\n    namespaceSelector: {}\n    # -- Specifies specific network CIDRs which are allowed to access the metrics port.\n    # In case you use namespaceSelector, you also have to specify your kubelet networks here.\n    # The metrics ports are also used for probes.\n    cidrs: []\n  ingress:\n    # -- Specifies the Pods which are allowed to access the http port.\n    # As this is cross-namespace communication, you also need the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespaces which are allowed to access the http port\n    namespaceSelector: {}\n  alertmanager:\n    # -- Specify the alertmanager port used for alerting\n    port: 9093\n    # -- Specifies the alertmanager Pods.\n    # As this is cross-namespace communication, you also need the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespace the alertmanager is running in\n    namespaceSelector: {}\n  externalStorage:\n    # -- Specify the port used for external storage, e.g. AWS S3\n    ports: []\n    # -- Specifies specific network CIDRs you want to limit access to\n    cidrs: []\n  discovery:\n    # -- (int) Specify the port used for discovery\n    port: null\n    # -- Specifies the Pods labels used for discovery.\n    # As this is cross-namespace communication, you also need the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespace the discovery Pods are running in\n    namespaceSelector: {}\n\n# -------------------------------------\n# Configuration for `minio` child chart\n# -------------------------------------\nminio:\n  enabled: true\n  # accessKey: enterprise-logs\n  # secretKey: supersecret\n  buckets:\n    - name: chunks\n      policy: none\n      purge: false\n    - name: ruler\n      policy: none\n      purge: false\n    - name: admin\n      policy: none\n      purge: false\n  persistence:\n    size: 5Gi\n  resources:\n    requests:\n      cpu: 100m\n      memory: 128Mi\n",
            "vars": null
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.test-module",
      "mode": "data",
      "type": "template_file",
      "name": "prometheus",
      "provider": "provider[\"registry.terraform.io/hashicorp/template\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "ca7857f9bbecb776c7ee84d20340f861a5d3ced93a2b6848ec76a5ab9ef42b15",
            "rendered": "grafana:\n  enabled: true\n  namespaceOverride: \"\"\n\n  ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDatasources: false\n\n  ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDashboards: false\n\n  ## Deploy default dashboards\n  ##\n  defaultDashboardsEnabled: true\n\n  ## Timezone for the default dashboards\n  ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg\n  ##\n  defaultDashboardsTimezone: utc\n\n  adminPassword: prom-operator\n\n  rbac:\n    ## If true, Grafana PSPs will be created\n    ##\n    pspEnabled: false\n\n  ingress:\n    ## If true, Grafana Ingress will be created\n    ##\n    enabled: false\n\n    ## IngressClassName for Grafana Ingress.\n    ## Should be provided if Ingress is enable.\n    ##\n    # ingressClassName: nginx\n\n    ## Annotations for Grafana Ingress\n    ##\n    annotations:\n      {}\n      # kubernetes.io/ingress.class: nginx\n      # kubernetes.io/tls-acme: \"true\"\n\n    ## Labels to be added to the Ingress\n    ##\n    labels: {}\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enable.\n    ##\n    # hosts:\n    #   - grafana.domain.com\n    hosts: []\n\n    ## Path for grafana ingress\n    path: /\n\n    ## TLS configuration for grafana Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: grafana-general-tls\n    #   hosts:\n    #   - grafana.example.com\n\n  sidecar:\n    dashboards:\n      enabled: true\n      label: grafana_dashboard\n      labelValue: \"1\"\n\n      ## Annotations for Grafana dashboard configmaps\n      ##\n      annotations: {}\n      multicluster:\n        global:\n          enabled: false\n        etcd:\n          enabled: false\n      provider:\n        allowUiUpdates: false\n    datasources:\n      enabled: true\n      defaultDatasourceEnabled: true\n\n      uid: prometheus\n\n      ## URL of prometheus datasource\n      ##\n      # url: http://prometheus-stack-prometheus:9090/\n\n      # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default\n      # defaultDatasourceScrapeInterval: 15s\n\n      ## Annotations for Grafana datasource configmaps\n      ##\n      annotations: {}\n\n      ## Create datasource for each Pod of Prometheus StatefulSet;\n      ## this uses headless service `prometheus-operated` which is\n      ## created by Prometheus Operator\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/0fee93e12dc7c2ea1218f19ae25ec6b893460590/pkg/prometheus/statefulset.go#L255-L286\n      createPrometheusReplicasDatasources: false\n      label: grafana_datasource\n      labelValue: \"1\"\n\n      ## Field with internal link pointing to existing data source in Grafana.\n      ## Can be provisioned via additionalDataSources\n      exemplarTraceIdDestinations:\n        {}\n        # datasourceUid: Jaeger\n        # traceIdLabelName: trace_id\n\n  extraConfigmapMounts: []\n  # - name: certs-configmap\n  #   mountPath: /etc/grafana/ssl/\n  #   configMap: certs-configmap\n  #   readOnly: true\n\n  deleteDatasources: []\n  # - name: example-datasource\n  #   orgId: 1\n\n  ## Configure additional grafana datasources (passed through tpl)\n  ## ref: http://docs.grafana.org/administration/provisioning/#datasources\n  additionalDataSources:\n    - name: Loki\n      type: loki\n      access: proxy\n      url: http://loki-gateway.loki.svc\n      basicAuth: false\n      jsonData:\n        tlsSkipVerify: true\n        maxLines: 1000\n        derivedFields:\n          - datasourceUid: Tempo\n            matcherRegex: \"[t|T][R|r][A|a][C|c][E|e][i|I][d|D][:|=|\\\\s](\\\\w+)\"\n            name: \"TraceId\"\n            url: '\"${__value.raw}\"'\n\n    - name: Tempo\n      type: tempo\n      # Access mode - proxy (server in the UI) or direct (browser in the UI).\n      access: proxy\n      url: http://tempo-tempo-distributed-query-frontend.tempo.svc:3100\n      jsonData:\n        tlsSkipVerify: true\n        httpMethod: GET\n        tracesToLogs:\n          datasourceUid: Loki\n          tags: [\"job\", \"instance\", \"pod\", \"namespace\"]\n          mappedTags: [{ key: \"service.name\", value: \"service\" }]\n          mapTagNamesEnabled: true\n          spanStartTimeShift: \"1h\"\n          spanEndTimeShift: \"1h\"\n          filterByTraceID: true\n          filterBySpanID: true\n        # serviceMap:\n        #   datasourceUid: 'prometheus'\n        search:\n          hide: false\n        nodeGraph:\n          enabled: true\n        lokiSearch:\n          datasourceUid: \"Loki\"\n\n  ## Passed to grafana subchart and used by servicemonitor below\n  ##\n  service:\n    portName: http-web\n\n  serviceMonitor:\n    # If true, a ServiceMonitor CRD is created for a prometheus operator\n    # https://github.com/coreos/prometheus-operator\n    #\n    enabled: true\n\n    # Path to use for scraping metrics. Might be different if server.root_url is set\n    # in grafana.ini\n    path: \"/metrics\"\n\n    #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)\n\n    # labels for the ServiceMonitor\n    labels: {}\n\n    # Scrape interval. If not set, the Prometheus default scrape interval is used.\n    #\n    interval: \"\"\n    scheme: http\n    tlsConfig: {}\n    scrapeTimeout: 30s\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n## Component scraping the kube api server\n##\nkubeApiServer:\n  enabled: true\n  tlsConfig:\n    serverName: kubernetes\n    insecureSkipVerify: false\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    jobLabel: component\n    selector:\n      matchLabels:\n        component: apiserver\n        provider: kubernetes\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings:\n      # Drop excessively noisy apiserver buckets.\n      - action: drop\n        regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)\n        sourceLabels:\n          - __name__\n          - le\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels:\n    #     - __meta_kubernetes_namespace\n    #     - __meta_kubernetes_service_name\n    #     - __meta_kubernetes_endpoint_port_name\n    #   action: keep\n    #   regex: default;kubernetes;https\n    # - targetLabel: __address__\n    #   replacement: kubernetes.default.svc:443\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping the kubelet and kubelet-hosted cAdvisor\n##\nkubelet:\n  enabled: true\n  namespace: kube-system\n\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## Enable scraping the kubelet over https. For requirements to enable this see\n    ## https://github.com/prometheus-operator/prometheus-operator/issues/926\n    ##\n    https: true\n\n    ## Enable scraping /metrics/cadvisor from kubelet's service\n    ##\n    cAdvisor: true\n\n    ## Enable scraping /metrics/probes from kubelet's service\n    ##\n    probes: true\n\n    ## Enable scraping /metrics/resource from kubelet's service\n    ## This is disabled by default because container metrics are already exposed by cAdvisor\n    ##\n    resource: false\n    # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource\n    resourcePath: \"/metrics/resource/v1alpha1\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    cAdvisorMetricRelabelings:\n      # Drop less useful container CPU metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: \"container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)\"\n      # Drop less useful container / always zero filesystem metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: \"container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)\"\n      # Drop less useful / always zero container memory metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: \"container_memory_(mapped_file|swap)\"\n      # Drop less useful container process metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: \"container_(file_descriptors|tasks_state|threads_max)\"\n      # Drop container spec metrics that overlap with kube-state-metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: \"container_spec.*\"\n      # Drop cgroup metrics with no pod.\n      - sourceLabels: [id, pod]\n        action: drop\n        regex: \".+;\"\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    probesMetricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    ## metrics_path is required to match upstream rules and charts\n    cAdvisorRelabelings:\n      - sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    probesRelabelings:\n      - sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    resourceRelabelings:\n      - sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    ## metrics_path is required to match upstream rules and charts\n    relabelings:\n      - sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping the kube controller manager\n##\nkubeControllerManager:\n  enabled: true\n\n  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## If using kubeControllerManager.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change\n    ## of default port in Kubernetes 1.22.\n    ##\n    port: null\n    targetPort: null\n    # selector:\n    #   component: kube-controller-manager\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## Enable scraping kube-controller-manager over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version.\n    ##\n    https: null\n\n    # Skip TLS certificate validation when scraping\n    insecureSkipVerify: null\n\n    # Name of the server to use when validating TLS certificate\n    serverName: null\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping coreDns. Use either this or kubeDns\n##\ncoreDns:\n  enabled: true\n  service:\n    port: 9153\n    targetPort: 9153\n    # selector:\n    #   k8s-app: kube-dns\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping kubeDns. Use either this or coreDns\n##\nkubeDns:\n  enabled: false\n  service:\n    dnsmasq:\n      port: 10054\n      targetPort: 10054\n    skydns:\n      port: 10055\n      targetPort: 10055\n    # selector:\n    #   k8s-app: kube-dns\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    dnsmasqMetricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    dnsmasqRelabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping etcd\n##\nkubeEtcd:\n  enabled: true\n\n  ## If your etcd is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    port: 2379\n    targetPort: 2381\n    # selector:\n    #   component: etcd\n\n  ## Configure secure access to the etcd cluster by loading a secret into prometheus and\n  ## specifying security configuration below. For example, with a secret named etcd-client-cert\n  ##\n  ## serviceMonitor:\n  ##   scheme: https\n  ##   insecureSkipVerify: false\n  ##   serverName: localhost\n  ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n  ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n  ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n  ##\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n    scheme: http\n    insecureSkipVerify: false\n    serverName: \"\"\n    caFile: \"\"\n    certFile: \"\"\n    keyFile: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar",
            "template": "grafana:\n  enabled: true\n  namespaceOverride: \"\"\n\n  ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDatasources: false\n\n  ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDashboards: false\n\n  ## Deploy default dashboards\n  ##\n  defaultDashboardsEnabled: true\n\n  ## Timezone for the default dashboards\n  ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg\n  ##\n  defaultDashboardsTimezone: utc\n\n  adminPassword: prom-operator\n\n  rbac:\n    ## If true, Grafana PSPs will be created\n    ##\n    pspEnabled: false\n\n  ingress:\n    ## If true, Grafana Ingress will be created\n    ##\n    enabled: false\n\n    ## IngressClassName for Grafana Ingress.\n    ## Should be provided if Ingress is enable.\n    ##\n    # ingressClassName: nginx\n\n    ## Annotations for Grafana Ingress\n    ##\n    annotations:\n      {}\n      # kubernetes.io/ingress.class: nginx\n      # kubernetes.io/tls-acme: \"true\"\n\n    ## Labels to be added to the Ingress\n    ##\n    labels: {}\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enable.\n    ##\n    # hosts:\n    #   - grafana.domain.com\n    hosts: []\n\n    ## Path for grafana ingress\n    path: /\n\n    ## TLS configuration for grafana Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: grafana-general-tls\n    #   hosts:\n    #   - grafana.example.com\n\n  sidecar:\n    dashboards:\n      enabled: true\n      label: grafana_dashboard\n      labelValue: \"1\"\n\n      ## Annotations for Grafana dashboard configmaps\n      ##\n      annotations: {}\n      multicluster:\n        global:\n          enabled: false\n        etcd:\n          enabled: false\n      provider:\n        allowUiUpdates: false\n    datasources:\n      enabled: true\n      defaultDatasourceEnabled: true\n\n      uid: prometheus\n\n      ## URL of prometheus datasource\n      ##\n      # url: http://prometheus-stack-prometheus:9090/\n\n      # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default\n      # defaultDatasourceScrapeInterval: 15s\n\n      ## Annotations for Grafana datasource configmaps\n      ##\n      annotations: {}\n\n      ## Create datasource for each Pod of Prometheus StatefulSet;\n      ## this uses headless service `prometheus-operated` which is\n      ## created by Prometheus Operator\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/0fee93e12dc7c2ea1218f19ae25ec6b893460590/pkg/prometheus/statefulset.go#L255-L286\n      createPrometheusReplicasDatasources: false\n      label: grafana_datasource\n      labelValue: \"1\"\n\n      ## Field with internal link pointing to existing data source in Grafana.\n      ## Can be provisioned via additionalDataSources\n      exemplarTraceIdDestinations:\n        {}\n        # datasourceUid: Jaeger\n        # traceIdLabelName: trace_id\n\n  extraConfigmapMounts: []\n  # - name: certs-configmap\n  #   mountPath: /etc/grafana/ssl/\n  #   configMap: certs-configmap\n  #   readOnly: true\n\n  deleteDatasources: []\n  # - name: example-datasource\n  #   orgId: 1\n\n  ## Configure additional grafana datasources (passed through tpl)\n  ## ref: http://docs.grafana.org/administration/provisioning/#datasources\n  additionalDataSources:\n    - name: Loki\n      type: loki\n      access: proxy\n      url: http://loki-gateway.loki.svc\n      basicAuth: false\n      jsonData:\n        tlsSkipVerify: true\n        maxLines: 1000\n        derivedFields:\n          - datasourceUid: Tempo\n            matcherRegex: \"[t|T][R|r][A|a][C|c][E|e][i|I][d|D][:|=|\\\\s](\\\\w+)\"\n            name: \"TraceId\"\n            url: '\"$${__value.raw}\"'\n\n    - name: Tempo\n      type: tempo\n      # Access mode - proxy (server in the UI) or direct (browser in the UI).\n      access: proxy\n      url: http://tempo-tempo-distributed-query-frontend.tempo.svc:3100\n      jsonData:\n        tlsSkipVerify: true\n        httpMethod: GET\n        tracesToLogs:\n          datasourceUid: Loki\n          tags: [\"job\", \"instance\", \"pod\", \"namespace\"]\n          mappedTags: [{ key: \"service.name\", value: \"service\" }]\n          mapTagNamesEnabled: true\n          spanStartTimeShift: \"1h\"\n          spanEndTimeShift: \"1h\"\n          filterByTraceID: true\n          filterBySpanID: true\n        # serviceMap:\n        #   datasourceUid: 'prometheus'\n        search:\n          hide: false\n        nodeGraph:\n          enabled: true\n        lokiSearch:\n          datasourceUid: \"Loki\"\n\n  ## Passed to grafana subchart and used by servicemonitor below\n  ##\n  service:\n    portName: http-web\n\n  serviceMonitor:\n    # If true, a ServiceMonitor CRD is created for a prometheus operator\n    # https://github.com/coreos/prometheus-operator\n    #\n    enabled: true\n\n    # Path to use for scraping metrics. Might be different if server.root_url is set\n    # in grafana.ini\n    path: \"/metrics\"\n\n    #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)\n\n    # labels for the ServiceMonitor\n    labels: {}\n\n    # Scrape interval. If not set, the Prometheus default scrape interval is used.\n    #\n    interval: \"\"\n    scheme: http\n    tlsConfig: {}\n    scrapeTimeout: 30s\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n## Component scraping the kube api server\n##\nkubeApiServer:\n  enabled: true\n  tlsConfig:\n    serverName: kubernetes\n    insecureSkipVerify: false\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    jobLabel: component\n    selector:\n      matchLabels:\n        component: apiserver\n        provider: kubernetes\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings:\n      # Drop excessively noisy apiserver buckets.\n      - action: drop\n        regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)\n        sourceLabels:\n          - __name__\n          - le\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels:\n    #     - __meta_kubernetes_namespace\n    #     - __meta_kubernetes_service_name\n    #     - __meta_kubernetes_endpoint_port_name\n    #   action: keep\n    #   regex: default;kubernetes;https\n    # - targetLabel: __address__\n    #   replacement: kubernetes.default.svc:443\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping the kubelet and kubelet-hosted cAdvisor\n##\nkubelet:\n  enabled: true\n  namespace: kube-system\n\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## Enable scraping the kubelet over https. For requirements to enable this see\n    ## https://github.com/prometheus-operator/prometheus-operator/issues/926\n    ##\n    https: true\n\n    ## Enable scraping /metrics/cadvisor from kubelet's service\n    ##\n    cAdvisor: true\n\n    ## Enable scraping /metrics/probes from kubelet's service\n    ##\n    probes: true\n\n    ## Enable scraping /metrics/resource from kubelet's service\n    ## This is disabled by default because container metrics are already exposed by cAdvisor\n    ##\n    resource: false\n    # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource\n    resourcePath: \"/metrics/resource/v1alpha1\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    cAdvisorMetricRelabelings:\n      # Drop less useful container CPU metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: \"container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)\"\n      # Drop less useful container / always zero filesystem metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: \"container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)\"\n      # Drop less useful / always zero container memory metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: \"container_memory_(mapped_file|swap)\"\n      # Drop less useful container process metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: \"container_(file_descriptors|tasks_state|threads_max)\"\n      # Drop container spec metrics that overlap with kube-state-metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: \"container_spec.*\"\n      # Drop cgroup metrics with no pod.\n      - sourceLabels: [id, pod]\n        action: drop\n        regex: \".+;\"\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    probesMetricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    ## metrics_path is required to match upstream rules and charts\n    cAdvisorRelabelings:\n      - sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    probesRelabelings:\n      - sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    resourceRelabelings:\n      - sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    ## metrics_path is required to match upstream rules and charts\n    relabelings:\n      - sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping the kube controller manager\n##\nkubeControllerManager:\n  enabled: true\n\n  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## If using kubeControllerManager.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change\n    ## of default port in Kubernetes 1.22.\n    ##\n    port: null\n    targetPort: null\n    # selector:\n    #   component: kube-controller-manager\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## Enable scraping kube-controller-manager over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version.\n    ##\n    https: null\n\n    # Skip TLS certificate validation when scraping\n    insecureSkipVerify: null\n\n    # Name of the server to use when validating TLS certificate\n    serverName: null\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping coreDns. Use either this or kubeDns\n##\ncoreDns:\n  enabled: true\n  service:\n    port: 9153\n    targetPort: 9153\n    # selector:\n    #   k8s-app: kube-dns\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping kubeDns. Use either this or coreDns\n##\nkubeDns:\n  enabled: false\n  service:\n    dnsmasq:\n      port: 10054\n      targetPort: 10054\n    skydns:\n      port: 10055\n      targetPort: 10055\n    # selector:\n    #   k8s-app: kube-dns\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    dnsmasqMetricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    dnsmasqRelabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping etcd\n##\nkubeEtcd:\n  enabled: true\n\n  ## If your etcd is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    port: 2379\n    targetPort: 2381\n    # selector:\n    #   component: etcd\n\n  ## Configure secure access to the etcd cluster by loading a secret into prometheus and\n  ## specifying security configuration below. For example, with a secret named etcd-client-cert\n  ##\n  ## serviceMonitor:\n  ##   scheme: https\n  ##   insecureSkipVerify: false\n  ##   serverName: localhost\n  ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n  ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n  ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n  ##\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n    scheme: http\n    insecureSkipVerify: false\n    serverName: \"\"\n    caFile: \"\"\n    certFile: \"\"\n    keyFile: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar",
            "vars": null
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.test-module",
      "mode": "data",
      "type": "template_file",
      "name": "promtail",
      "provider": "provider[\"registry.terraform.io/hashicorp/template\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "6b09cd04c14ac63c4ac92a88e1e3277c5c7374dc577efa823347ba1f7a49ffd0",
            "rendered": "# -- Overrides the chart's name\nnameOverride: null\n\n# -- Overrides the chart's computed fullname\nfullnameOverride: null\n\ndaemonset:\n  # -- Deploys Promtail as a DaemonSet\n  enabled: true\n\ndeployment:\n  # -- Deploys Promtail as a Deployment\n  enabled: false\n  replicaCount: 1\n  autoscaling:\n    # -- Creates a HorizontalPodAutoscaler for the deployment\n    enabled: false\n    minReplicas: 1\n    maxReplicas: 10\n    targetCPUUtilizationPercentage: 80\n    targetMemoryUtilizationPercentage:\n\ninitContainer:\n  # -- Specifies whether the init container for setting inotify max user instances is to be enabled\n  enabled: false\n  image:\n    # -- The Docker registry for the init container\n    registry: docker.io\n    # -- Docker image repository for the init container\n    repository: busybox\n    # -- Docker tag for the init container\n    tag: 1.33\n    # -- Docker image pull policy for the init container image\n    pullPolicy: IfNotPresent\n  # -- The inotify max user instances to configure\n  fsInotifyMaxUserInstances: 128\n\nimage:\n  # -- The Docker registry\n  registry: docker.io\n  # -- Docker image repository\n  repository: grafana/promtail\n  # -- Overrides the image tag whose default is the chart's appVersion\n  tag: null\n  # -- Docker image pull policy\n  pullPolicy: IfNotPresent\n\n# -- Image pull secrets for Docker images\nimagePullSecrets: []\n\n# -- Annotations for the DaemonSet\nannotations: {}\n\n# -- The update strategy for the DaemonSet\nupdateStrategy: {}\n\n# -- Pod labels\npodLabels: {}\n\n# -- Pod annotations\npodAnnotations: {}\n#  prometheus.io/scrape: \"true\"\n#  prometheus.io/port: \"http-metrics\"\n\n# -- The name of the PriorityClass\npriorityClassName: null\n\n# -- Liveness probe\nlivenessProbe: {}\n\n# -- Readiness probe\n# @default -- See `values.yaml`\nreadinessProbe:\n  failureThreshold: 5\n  httpGet:\n    path: /ready\n    port: http-metrics\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  successThreshold: 1\n  timeoutSeconds: 1\n\n# -- Resource requests and limits\nresources: {}\n#  limits:\n#    cpu: 200m\n#    memory: 128Mi\n#  requests:\n#    cpu: 100m\n#    memory: 128Mi\n\n# -- The security context for pods\npodSecurityContext:\n  runAsUser: 0\n  runAsGroup: 0\n\n# -- The security context for containers\ncontainerSecurityContext:\n  readOnlyRootFilesystem: true\n  capabilities:\n    drop:\n      - ALL\n  allowPrivilegeEscalation: false\n\nrbac:\n  # -- Specifies whether RBAC resources are to be created\n  create: true\n  # -- Specifies whether a PodSecurityPolicy is to be created\n  pspEnabled: false\n\nserviceAccount:\n  # -- Specifies whether a ServiceAccount should be created\n  create: true\n  # -- The name of the ServiceAccount to use.\n  # If not set and `create` is true, a name is generated using the fullname template\n  name: null\n  # -- Image pull secrets for the service account\n  imagePullSecrets: []\n  # -- Annotations for the service account\n  annotations: {}\n\n# -- Node selector for pods\nnodeSelector: {}\n\n# -- Affinity configuration for pods\naffinity: {}\n\n# -- Tolerations for pods. By default, pods will be scheduled on master/control-plane nodes.\ntolerations:\n  - key: node-role.kubernetes.io/master\n    operator: Exists\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/control-plane\n    operator: Exists\n    effect: NoSchedule\n\n# -- Default volumes that are mounted into pods. In most cases, these should not be changed.\n# Use `extraVolumes`/`extraVolumeMounts` for additional custom volumes.\n# @default -- See `values.yaml`\ndefaultVolumes:\n  - name: run\n    hostPath:\n      path: /run/promtail\n  - name: containers\n    hostPath:\n      path: /var/lib/docker/containers\n  - name: pods\n    hostPath:\n      path: /var/log/pods\n\n# -- Default volume mounts. Corresponds to `volumes`.\n# @default -- See `values.yaml`\ndefaultVolumeMounts:\n  - name: run\n    mountPath: /run/promtail\n  - name: containers\n    mountPath: /var/lib/docker/containers\n    readOnly: true\n  - name: pods\n    mountPath: /var/log/pods\n    readOnly: true\n\n# Extra volumes to be added in addition to those specified under `defaultVolumes`.\nextraVolumes: []\n\n# Extra volume mounts together. Corresponds to `extraVolumes`.\nextraVolumeMounts: []\n\n# Extra args for the Promtail container.\nextraArgs: []\n# -- Example:\n# -- extraArgs:\n# --   - -client.external-labels=hostname=$(HOSTNAME)\n\n# -- Extra environment variables\nextraEnv: []\n\n# -- Extra environment variables from secrets or configmaps\nextraEnvFrom: []\n\n# ServiceMonitor configuration\nserviceMonitor:\n  # -- If enabled, ServiceMonitor resources for Prometheus Operator are created\n  enabled: false\n  # -- Alternative namespace for ServiceMonitor resources\n  namespace: null\n  # -- Namespace selector for ServiceMonitor resources\n  namespaceSelector: {}\n  # -- ServiceMonitor annotations\n  annotations: {}\n  # -- Additional ServiceMonitor labels\n  labels: {}\n  # -- ServiceMonitor scrape interval\n  interval: null\n  # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)\n  scrapeTimeout: null\n  # -- ServiceMonitor relabel configs to apply to samples before scraping\n  # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n  # (defines `relabel_configs`)\n  relabelings: []\n  # -- ServiceMonitor relabel configs to apply to samples as the last\n  # step before ingestion\n  # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n  # (defines `metric_relabel_configs`)\n  metricRelabelings: []\n\n# -- Configure additional ports and services. For each configured port, a corresponding service is created.\n# See values.yaml for details\nextraPorts: {}\n#  syslog:\n#    name: tcp-syslog\n#    containerPort: 1514\n#    protocol: TCP\n#    service:\n#      type: ClusterIP\n#      clusterIP: null\n#      port: 1514\n#      externalIPs: []\n#      nodePort: null\n#      annotations: {}\n#      labels: {}\n#      loadBalancerIP: null\n#      loadBalancerSourceRanges: []\n#      externalTrafficPolicy: null\n\n# -- PodSecurityPolicy configuration.\n# @default -- See `values.yaml`\npodSecurityPolicy:\n  privileged: true\n  allowPrivilegeEscalation: true\n  volumes:\n    - \"secret\"\n    - \"hostPath\"\n    - \"downwardAPI\"\n  hostNetwork: false\n  hostIPC: false\n  hostPID: false\n  runAsUser:\n    rule: \"RunAsAny\"\n  seLinux:\n    rule: \"RunAsAny\"\n  supplementalGroups:\n    rule: \"RunAsAny\"\n  fsGroup:\n    rule: \"RunAsAny\"\n  readOnlyRootFilesystem: true\n  requiredDropCapabilities:\n    - ALL\n\n# -- Section for crafting Promtails config file. The only directly relevant value is `config.file`\n# which is a templated string that references the other values and snippets below this key.\n# @default -- See `values.yaml`\nconfig:\n  # -- The log level of the Promtail server\n  # Must be reference in `config.file` to configure `server.log_level`\n  # See default config in `values.yaml`\n  logLevel: info\n  # -- The port of the Promtail server\n  # Must be reference in `config.file` to configure `server.http_listen_port`\n  # See default config in `values.yaml`\n  serverPort: 3101\n  # -- The config of clients of the Promtail server\n  # Must be reference in `config.file` to configure `clients`\n  # @default -- See `values.yaml`\n  clients:\n    - url: http://loki-gateway.loki.svc/loki/api/v1/push\n  # -- A section of reusable snippets that can be reference in `config.file`.\n  # Custom snippets may be added in order to reduce redundancy.\n  # This is especially helpful when multiple `kubernetes_sd_configs` are use which usually have large parts in common.\n  # @default -- See `values.yaml`\n  snippets:\n    pipelineStages:\n      - cri: {}\n    common:\n      - action: replace\n        source_labels:\n          - __meta_kubernetes_pod_node_name\n        target_label: node_name\n      - action: replace\n        source_labels:\n          - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        replacement: $1\n        separator: /\n        source_labels:\n          - namespace\n          - app\n        target_label: job\n      - action: replace\n        source_labels:\n          - __meta_kubernetes_pod_name\n        target_label: pod\n      - action: replace\n        source_labels:\n          - __meta_kubernetes_pod_container_name\n        target_label: container\n      - action: replace\n        replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n          - __meta_kubernetes_pod_uid\n          - __meta_kubernetes_pod_container_name\n        target_label: __path__\n      - action: replace\n        replacement: /var/log/pods/*$1/*.log\n        regex: true/(.*)\n        separator: /\n        source_labels:\n          - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash\n          - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash\n          - __meta_kubernetes_pod_container_name\n        target_label: __path__\n\n    # If set to true, adds an additional label for the scrape job.\n    # This helps debug the Promtail config.\n    addScrapeJobLabel: false\n\n    # -- You can put here any keys that will be directly added to the config file's 'server' block.\n    # @default -- empty\n    extraServerConfigs: \"\"\n\n    # -- You can put here any additional scrape configs you want to add to the config file.\n    # @default -- empty\n    extraScrapeConfigs: \"\"\n\n    # -- You can put here any additional relabel_configs to \"kubernetes-pods\" job\n    extraRelabelConfigs: []\n\n    scrapeConfigs: |\n      # See also https://github.com/grafana/loki/blob/master/production/ksonnet/promtail/scrape_config.libsonnet for reference\n      - job_name: kubernetes-pods\n        pipeline_stages:\n          {{- toYaml .Values.config.snippets.pipelineStages | nindent 4 }}\n        kubernetes_sd_configs:\n          - role: pod\n        relabel_configs:\n          - source_labels:\n              - __meta_kubernetes_pod_controller_name\n            regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?\n            action: replace\n            target_label: __tmp_controller_name\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_name\n              - __meta_kubernetes_pod_label_app\n              - __tmp_controller_name\n              - __meta_kubernetes_pod_name\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: app\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_instance\n              - __meta_kubernetes_pod_label_release\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: instance\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_component\n              - __meta_kubernetes_pod_label_component\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: component\n          {{- if .Values.config.snippets.addScrapeJobLabel }}\n          - replacement: kubernetes-pods\n            target_label: scrape_job\n          {{- end }}\n          {{- toYaml .Values.config.snippets.common | nindent 4 }}\n          {{- with .Values.config.snippets.extraRelabelConfigs }}\n          {{- toYaml . | nindent 4 }}\n          {{- end }}\n\n  # -- Config file contents for Promtail.\n  # Must be configured as string.\n  # It is templated so it can be assembled from reusable snippets in order to avoid redundancy.\n  # @default -- See `values.yaml`\n  file: |\n    server:\n      log_level: {{ .Values.config.logLevel }}\n      http_listen_port: {{ .Values.config.serverPort }}\n      {{- tpl .Values.config.snippets.extraServerConfigs . | nindent 2 }}\n\n    clients:\n      {{- tpl (toYaml .Values.config.clients) . | nindent 2 }}\n\n    positions:\n      filename: /run/promtail/positions.yaml\n\n    scrape_configs:\n      {{- tpl .Values.config.snippets.scrapeConfigs . | nindent 2 }}\n      {{- tpl .Values.config.snippets.extraScrapeConfigs . | nindent 2 }}\n\nnetworkPolicy:\n  # -- Specifies whether Network Policies should be created\n  enabled: false\n  metrics:\n    # -- Specifies the Pods which are allowed to access the metrics port.\n    # As this is cross-namespace communication, you also neeed the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespaces which are allowed to access the metrics port\n    namespaceSelector: {}\n    # -- Specifies specific network CIDRs which are allowed to access the metrics port.\n    # In case you use namespaceSelector, you also have to specify your kubelet networks here.\n    # The metrics ports are also used for probes.\n    cidrs: []\n  k8sApi:\n    # -- Specify the k8s API endpoint port\n    port: 8443\n    # -- Specifies specific network CIDRs you want to limit access to\n    cidrs: []\n\n# -- Extra K8s manifests to deploy\nextraObjects:\n  []\n  # - apiVersion: \"kubernetes-client.io/v1\"\n  #   kind: ExternalSecret\n  #   metadata:\n  #     name: promtail-secrets\n  #   spec:\n  #     backendType: gcpSecretsManager\n  #     data:\n  #       - key: promtail-oauth2-creds\n  #         name: client_secret\n",
            "template": "# -- Overrides the chart's name\nnameOverride: null\n\n# -- Overrides the chart's computed fullname\nfullnameOverride: null\n\ndaemonset:\n  # -- Deploys Promtail as a DaemonSet\n  enabled: true\n\ndeployment:\n  # -- Deploys Promtail as a Deployment\n  enabled: false\n  replicaCount: 1\n  autoscaling:\n    # -- Creates a HorizontalPodAutoscaler for the deployment\n    enabled: false\n    minReplicas: 1\n    maxReplicas: 10\n    targetCPUUtilizationPercentage: 80\n    targetMemoryUtilizationPercentage:\n\ninitContainer:\n  # -- Specifies whether the init container for setting inotify max user instances is to be enabled\n  enabled: false\n  image:\n    # -- The Docker registry for the init container\n    registry: docker.io\n    # -- Docker image repository for the init container\n    repository: busybox\n    # -- Docker tag for the init container\n    tag: 1.33\n    # -- Docker image pull policy for the init container image\n    pullPolicy: IfNotPresent\n  # -- The inotify max user instances to configure\n  fsInotifyMaxUserInstances: 128\n\nimage:\n  # -- The Docker registry\n  registry: docker.io\n  # -- Docker image repository\n  repository: grafana/promtail\n  # -- Overrides the image tag whose default is the chart's appVersion\n  tag: null\n  # -- Docker image pull policy\n  pullPolicy: IfNotPresent\n\n# -- Image pull secrets for Docker images\nimagePullSecrets: []\n\n# -- Annotations for the DaemonSet\nannotations: {}\n\n# -- The update strategy for the DaemonSet\nupdateStrategy: {}\n\n# -- Pod labels\npodLabels: {}\n\n# -- Pod annotations\npodAnnotations: {}\n#  prometheus.io/scrape: \"true\"\n#  prometheus.io/port: \"http-metrics\"\n\n# -- The name of the PriorityClass\npriorityClassName: null\n\n# -- Liveness probe\nlivenessProbe: {}\n\n# -- Readiness probe\n# @default -- See `values.yaml`\nreadinessProbe:\n  failureThreshold: 5\n  httpGet:\n    path: /ready\n    port: http-metrics\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  successThreshold: 1\n  timeoutSeconds: 1\n\n# -- Resource requests and limits\nresources: {}\n#  limits:\n#    cpu: 200m\n#    memory: 128Mi\n#  requests:\n#    cpu: 100m\n#    memory: 128Mi\n\n# -- The security context for pods\npodSecurityContext:\n  runAsUser: 0\n  runAsGroup: 0\n\n# -- The security context for containers\ncontainerSecurityContext:\n  readOnlyRootFilesystem: true\n  capabilities:\n    drop:\n      - ALL\n  allowPrivilegeEscalation: false\n\nrbac:\n  # -- Specifies whether RBAC resources are to be created\n  create: true\n  # -- Specifies whether a PodSecurityPolicy is to be created\n  pspEnabled: false\n\nserviceAccount:\n  # -- Specifies whether a ServiceAccount should be created\n  create: true\n  # -- The name of the ServiceAccount to use.\n  # If not set and `create` is true, a name is generated using the fullname template\n  name: null\n  # -- Image pull secrets for the service account\n  imagePullSecrets: []\n  # -- Annotations for the service account\n  annotations: {}\n\n# -- Node selector for pods\nnodeSelector: {}\n\n# -- Affinity configuration for pods\naffinity: {}\n\n# -- Tolerations for pods. By default, pods will be scheduled on master/control-plane nodes.\ntolerations:\n  - key: node-role.kubernetes.io/master\n    operator: Exists\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/control-plane\n    operator: Exists\n    effect: NoSchedule\n\n# -- Default volumes that are mounted into pods. In most cases, these should not be changed.\n# Use `extraVolumes`/`extraVolumeMounts` for additional custom volumes.\n# @default -- See `values.yaml`\ndefaultVolumes:\n  - name: run\n    hostPath:\n      path: /run/promtail\n  - name: containers\n    hostPath:\n      path: /var/lib/docker/containers\n  - name: pods\n    hostPath:\n      path: /var/log/pods\n\n# -- Default volume mounts. Corresponds to `volumes`.\n# @default -- See `values.yaml`\ndefaultVolumeMounts:\n  - name: run\n    mountPath: /run/promtail\n  - name: containers\n    mountPath: /var/lib/docker/containers\n    readOnly: true\n  - name: pods\n    mountPath: /var/log/pods\n    readOnly: true\n\n# Extra volumes to be added in addition to those specified under `defaultVolumes`.\nextraVolumes: []\n\n# Extra volume mounts together. Corresponds to `extraVolumes`.\nextraVolumeMounts: []\n\n# Extra args for the Promtail container.\nextraArgs: []\n# -- Example:\n# -- extraArgs:\n# --   - -client.external-labels=hostname=$(HOSTNAME)\n\n# -- Extra environment variables\nextraEnv: []\n\n# -- Extra environment variables from secrets or configmaps\nextraEnvFrom: []\n\n# ServiceMonitor configuration\nserviceMonitor:\n  # -- If enabled, ServiceMonitor resources for Prometheus Operator are created\n  enabled: false\n  # -- Alternative namespace for ServiceMonitor resources\n  namespace: null\n  # -- Namespace selector for ServiceMonitor resources\n  namespaceSelector: {}\n  # -- ServiceMonitor annotations\n  annotations: {}\n  # -- Additional ServiceMonitor labels\n  labels: {}\n  # -- ServiceMonitor scrape interval\n  interval: null\n  # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)\n  scrapeTimeout: null\n  # -- ServiceMonitor relabel configs to apply to samples before scraping\n  # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n  # (defines `relabel_configs`)\n  relabelings: []\n  # -- ServiceMonitor relabel configs to apply to samples as the last\n  # step before ingestion\n  # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n  # (defines `metric_relabel_configs`)\n  metricRelabelings: []\n\n# -- Configure additional ports and services. For each configured port, a corresponding service is created.\n# See values.yaml for details\nextraPorts: {}\n#  syslog:\n#    name: tcp-syslog\n#    containerPort: 1514\n#    protocol: TCP\n#    service:\n#      type: ClusterIP\n#      clusterIP: null\n#      port: 1514\n#      externalIPs: []\n#      nodePort: null\n#      annotations: {}\n#      labels: {}\n#      loadBalancerIP: null\n#      loadBalancerSourceRanges: []\n#      externalTrafficPolicy: null\n\n# -- PodSecurityPolicy configuration.\n# @default -- See `values.yaml`\npodSecurityPolicy:\n  privileged: true\n  allowPrivilegeEscalation: true\n  volumes:\n    - \"secret\"\n    - \"hostPath\"\n    - \"downwardAPI\"\n  hostNetwork: false\n  hostIPC: false\n  hostPID: false\n  runAsUser:\n    rule: \"RunAsAny\"\n  seLinux:\n    rule: \"RunAsAny\"\n  supplementalGroups:\n    rule: \"RunAsAny\"\n  fsGroup:\n    rule: \"RunAsAny\"\n  readOnlyRootFilesystem: true\n  requiredDropCapabilities:\n    - ALL\n\n# -- Section for crafting Promtails config file. The only directly relevant value is `config.file`\n# which is a templated string that references the other values and snippets below this key.\n# @default -- See `values.yaml`\nconfig:\n  # -- The log level of the Promtail server\n  # Must be reference in `config.file` to configure `server.log_level`\n  # See default config in `values.yaml`\n  logLevel: info\n  # -- The port of the Promtail server\n  # Must be reference in `config.file` to configure `server.http_listen_port`\n  # See default config in `values.yaml`\n  serverPort: 3101\n  # -- The config of clients of the Promtail server\n  # Must be reference in `config.file` to configure `clients`\n  # @default -- See `values.yaml`\n  clients:\n    - url: http://loki-gateway.loki.svc/loki/api/v1/push\n  # -- A section of reusable snippets that can be reference in `config.file`.\n  # Custom snippets may be added in order to reduce redundancy.\n  # This is especially helpful when multiple `kubernetes_sd_configs` are use which usually have large parts in common.\n  # @default -- See `values.yaml`\n  snippets:\n    pipelineStages:\n      - cri: {}\n    common:\n      - action: replace\n        source_labels:\n          - __meta_kubernetes_pod_node_name\n        target_label: node_name\n      - action: replace\n        source_labels:\n          - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        replacement: $1\n        separator: /\n        source_labels:\n          - namespace\n          - app\n        target_label: job\n      - action: replace\n        source_labels:\n          - __meta_kubernetes_pod_name\n        target_label: pod\n      - action: replace\n        source_labels:\n          - __meta_kubernetes_pod_container_name\n        target_label: container\n      - action: replace\n        replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n          - __meta_kubernetes_pod_uid\n          - __meta_kubernetes_pod_container_name\n        target_label: __path__\n      - action: replace\n        replacement: /var/log/pods/*$1/*.log\n        regex: true/(.*)\n        separator: /\n        source_labels:\n          - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash\n          - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash\n          - __meta_kubernetes_pod_container_name\n        target_label: __path__\n\n    # If set to true, adds an additional label for the scrape job.\n    # This helps debug the Promtail config.\n    addScrapeJobLabel: false\n\n    # -- You can put here any keys that will be directly added to the config file's 'server' block.\n    # @default -- empty\n    extraServerConfigs: \"\"\n\n    # -- You can put here any additional scrape configs you want to add to the config file.\n    # @default -- empty\n    extraScrapeConfigs: \"\"\n\n    # -- You can put here any additional relabel_configs to \"kubernetes-pods\" job\n    extraRelabelConfigs: []\n\n    scrapeConfigs: |\n      # See also https://github.com/grafana/loki/blob/master/production/ksonnet/promtail/scrape_config.libsonnet for reference\n      - job_name: kubernetes-pods\n        pipeline_stages:\n          {{- toYaml .Values.config.snippets.pipelineStages | nindent 4 }}\n        kubernetes_sd_configs:\n          - role: pod\n        relabel_configs:\n          - source_labels:\n              - __meta_kubernetes_pod_controller_name\n            regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?\n            action: replace\n            target_label: __tmp_controller_name\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_name\n              - __meta_kubernetes_pod_label_app\n              - __tmp_controller_name\n              - __meta_kubernetes_pod_name\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: app\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_instance\n              - __meta_kubernetes_pod_label_release\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: instance\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_component\n              - __meta_kubernetes_pod_label_component\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: component\n          {{- if .Values.config.snippets.addScrapeJobLabel }}\n          - replacement: kubernetes-pods\n            target_label: scrape_job\n          {{- end }}\n          {{- toYaml .Values.config.snippets.common | nindent 4 }}\n          {{- with .Values.config.snippets.extraRelabelConfigs }}\n          {{- toYaml . | nindent 4 }}\n          {{- end }}\n\n  # -- Config file contents for Promtail.\n  # Must be configured as string.\n  # It is templated so it can be assembled from reusable snippets in order to avoid redundancy.\n  # @default -- See `values.yaml`\n  file: |\n    server:\n      log_level: {{ .Values.config.logLevel }}\n      http_listen_port: {{ .Values.config.serverPort }}\n      {{- tpl .Values.config.snippets.extraServerConfigs . | nindent 2 }}\n\n    clients:\n      {{- tpl (toYaml .Values.config.clients) . | nindent 2 }}\n\n    positions:\n      filename: /run/promtail/positions.yaml\n\n    scrape_configs:\n      {{- tpl .Values.config.snippets.scrapeConfigs . | nindent 2 }}\n      {{- tpl .Values.config.snippets.extraScrapeConfigs . | nindent 2 }}\n\nnetworkPolicy:\n  # -- Specifies whether Network Policies should be created\n  enabled: false\n  metrics:\n    # -- Specifies the Pods which are allowed to access the metrics port.\n    # As this is cross-namespace communication, you also neeed the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespaces which are allowed to access the metrics port\n    namespaceSelector: {}\n    # -- Specifies specific network CIDRs which are allowed to access the metrics port.\n    # In case you use namespaceSelector, you also have to specify your kubelet networks here.\n    # The metrics ports are also used for probes.\n    cidrs: []\n  k8sApi:\n    # -- Specify the k8s API endpoint port\n    port: 8443\n    # -- Specifies specific network CIDRs you want to limit access to\n    cidrs: []\n\n# -- Extra K8s manifests to deploy\nextraObjects:\n  []\n  # - apiVersion: \"kubernetes-client.io/v1\"\n  #   kind: ExternalSecret\n  #   metadata:\n  #     name: promtail-secrets\n  #   spec:\n  #     backendType: gcpSecretsManager\n  #     data:\n  #       - key: promtail-oauth2-creds\n  #         name: client_secret\n",
            "vars": null
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.test-module",
      "mode": "data",
      "type": "template_file",
      "name": "tempo",
      "provider": "provider[\"registry.terraform.io/hashicorp/template\"]",
      "instances": [
        {
          "schema_version": 0,
          "attributes": {
            "filename": null,
            "id": "824f0dcb0caae7cf600a072eabcbaee5245e9e9bf1202b9bfd664487f7a6cf16",
            "rendered": "global:\n  image:\n    # -- Overrides the Docker registry globally for all images\n    registry: docker.io\n  # -- Overrides the priorityClassName for all pods\n  priorityClassName: null\n  # -- configures cluster domain (\"cluster.local\" by default)\n  clusterDomain: \"cluster.local\"\n  # -- configures DNS service name\n  dnsService: \"kube-dns\"\n  # -- configures DNS service namespace\n  dnsNamespace: \"kube-system\"\n# -- Overrides the chart's computed fullname\n# fullnameOverride: tempo\ntempo:\n  image:\n    # -- The Docker registry\n    registry: docker.io\n    # -- Docker image repository\n    repository: grafana/tempo\n    # -- Overrides the image tag whose default is the chart's appVersion\n    tag: null\n    pullPolicy: IfNotPresent\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http\n    initialDelaySeconds: 30\n    timeoutSeconds: 1\n  # -- Global labels for all tempo pods\n  podLabels: {}\n  # -- Common annotations for all pods\n  podAnnotations: {}\n  # -- SecurityContext holds pod-level security attributes and common container settings\n  securityContext: {}\n  #  capabilities:\n  #    drop:\n  #    - ALL\n  #  readOnlyRootFilesystem: true\n  #  runAsNonRoot: true\n  #  runAsUser: 1000\n  # -- Structured tempo configuration\n  structuredConfig: {}\n\nserviceAccount:\n  # -- Specifies whether a ServiceAccount should be created\n  create: true\n  # -- The name of the ServiceAccount to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: null\n  # -- Image pull secrets for the service account\n  imagePullSecrets: []\n  # -- Annotations for the service account\n  annotations: {}\n\nrbac:\n  # -- Specifies whether RBAC manifests should be created\n  create: false\n  # -- Specifies whether a PodSecurityPolicy should be created\n  pspEnabled: false\n\n# Configuration for the ingester\ningester:\n  # -- Annotations for the ingester StatefulSet\n  annotations: {}\n  # -- Number of replicas for the ingester\n  replicas: 3\n  autoscaling:\n    # -- Enable autoscaling for the ingester\n    enabled: false\n    # -- Minimum autoscaling replicas for the ingester\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the ingester\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the ingester\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the ingester\n    targetMemoryUtilizationPercentage:\n  image:\n    # -- The Docker registry for the ingester image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the ingester image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the ingester image. Overrides `tempo.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for ingester pods\n  priorityClassName: null\n  # -- Labels for ingester pods\n  podLabels: {}\n  # -- Annotations for ingester pods\n  podAnnotations: {}\n  # -- Additional CLI args for the ingester\n  extraArgs: []\n  # -- Environment variables to add to the ingester pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the ingester pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the ingester\n  resources: {}\n  # -- Grace period to allow the ingester to shutdown before it is killed. Especially for the ingestor,\n  # this must be increased. It must be long enough so ingesters can be gracefully shutdown flushing/transferring\n  # all data and to successfully leave the member ring on shutdown.\n  terminationGracePeriodSeconds: 300\n  # -- Affinity for ingester pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Soft node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.ingesterSelectorLabels\" . | nindent 12 }}\n            topologyKey: kubernetes.io/hostname\n        - weight: 75\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.ingesterSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for ingester pods\n  nodeSelector: {}\n  # -- Tolerations for ingester pods\n  tolerations: []\n  # -- Extra volumes for ingester pods\n  extraVolumeMounts: []\n  # -- Extra volumes for ingester deployment\n  extraVolumes: []\n  persistence:\n    # -- Enable creating PVCs which is required when using boltdb-shipper\n    enabled: false\n    # -- Size of persistent disk\n    size: 10Gi\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n  config:\n    # -- Number of copies of spans to store in the ingester ring\n    replication_factor: 3\n    # -- Amount of time a trace must be idle before flushing it to the wal.\n    trace_idle_period: null\n    # -- How often to sweep all tenants and move traces from live -\u003e wal -\u003e completed blocks.\n    flush_check_period: null\n    # -- Maximum size of a block before cutting it\n    max_block_bytes: null\n    # -- Maximum length of time before cutting a block\n    max_block_duration: null\n    # -- Duration to keep blocks in the ingester after they have been flushed\n    complete_block_timeout: null\n  service:\n    # -- Annotations for ingester service\n    annotations: {}\n\n# Configuration for the metrics-generator\nmetricsGenerator:\n  # -- Specifies whether a metrics-generator should be deployed\n  enabled: false\n  # -- Annotations for the metrics-generator StatefulSet\n  annotations: {}\n  # -- Number of replicas for the metrics-generator\n  replicas: 1\n  image:\n    # -- The Docker registry for the metrics-generator image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the metrics-generator image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the metrics-generator image. Overrides `tempo.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for metrics-generator pods\n  priorityClassName: null\n  # -- Labels for metrics-generator pods\n  podLabels: {}\n  # -- Annotations for metrics-generator pods\n  podAnnotations: {}\n  # -- Additional CLI args for the metrics-generator\n  extraArgs: []\n  # -- Environment variables to add to the metrics-generator pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the metrics-generator pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the metrics-generator\n  resources: {}\n  # -- Grace period to allow the metrics-generator to shutdown before it is killed. Especially for the ingestor,\n  # this must be increased. It must be long enough so metrics-generators can be gracefully shutdown flushing/transferring\n  # all data and to successfully leave the member ring on shutdown.\n  terminationGracePeriodSeconds: 300\n  # -- Affinity for metrics-generator pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.metricsGeneratorSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.metricsGeneratorSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for metrics-generator pods\n  nodeSelector: {}\n  # -- Tolerations for metrics-generator pods\n  tolerations: []\n  # -- Extra volumes for metrics-generator pods\n  extraVolumeMounts: []\n  # -- Extra volumes for metrics-generator deployment\n  extraVolumes: []\n  # -- Default ports\n  ports:\n    - name: grpc\n      port: 9095\n      service: true\n    - name: http-memberlist\n      port: 7946\n      service: false\n    - name: http\n      port: 3100\n      service: true\n  config:\n    #  MaxItems is the amount of edges that will be stored in the store.\n    service_graphs_max_items: 10000\n    storage_remote_write: []\n    # - url: http://cortex/api/v1/push\n    #   send_exemplars: true\n    #   headers:\n    #     x-scope-orgid: operations\n  service:\n    # -- Annotations for Metrics Generator service\n    annotations: {}\n\n# Configuration for the distributor\ndistributor:\n  # -- Number of replicas for the distributor\n  replicas: 1\n  autoscaling:\n    # -- Enable autoscaling for the distributor\n    enabled: false\n    # -- Minimum autoscaling replicas for the distributor\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the distributor\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the distributor\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the distributor\n    targetMemoryUtilizationPercentage:\n  image:\n    # -- The Docker registry for the ingester image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the ingester image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the ingester image. Overrides `tempo.image.tag`\n    tag: null\n  service:\n    # -- Annotations for distributor service\n    annotations: {}\n    # -- Type of service for the distributor\n    type: ClusterIP\n    # -- If type is LoadBalancer you can assign the IP to the LoadBalancer\n    loadBalancerIP: \"\"\n    # -- If type is LoadBalancer limit incoming traffic from IPs.\n    loadBalancerSourceRanges: []\n  # -- The name of the PriorityClass for distributor pods\n  priorityClassName: null\n  # -- Labels for distributor pods\n  podLabels: {}\n  # -- Annotations for distributor pods\n  podAnnotations: {}\n  # -- Additional CLI args for the distributor\n  extraArgs: []\n  # -- Environment variables to add to the distributor pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the distributor pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the distributor\n  resources: {}\n  # -- Grace period to allow the distributor to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for distributor pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.distributorSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.distributorSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for distributor pods\n  nodeSelector: {}\n  # -- Tolerations for distributor pods\n  tolerations: []\n  # -- Extra volumes for distributor pods\n  extraVolumeMounts: []\n  # -- Extra volumes for distributor deployment\n  extraVolumes: []\n  config:\n    # -- Enable to log every received trace id to help debug ingestion\n    log_received_traces: null\n    # -- Disables write extension with inactive ingesters\n    extend_writes: null\n    # -- List of tags that will not be extracted from trace data for search lookups\n    search_tags_deny_list: []\n\ncompactor:\n  # -- Number of replicas for the compactor\n  replicas: 1\n  image:\n    # -- The Docker registry for the compactor image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the compactor image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the compactor image. Overrides `tempo.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for compactor pods\n  priorityClassName: null\n  # -- Labels for compactor pods\n  podLabels: {}\n  # -- Annotations for compactor pods\n  podAnnotations: {}\n  # -- Additional CLI args for the compactor\n  extraArgs: []\n  # -- Environment variables to add to the compactor pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the compactor pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the compactor\n  resources: {}\n  # -- Grace period to allow the compactor to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Node selector for compactor pods\n  nodeSelector: {}\n  # -- Tolerations for compactor pods\n  tolerations: []\n  # -- Extra volumes for compactor pods\n  extraVolumeMounts: []\n  # -- Extra volumes for compactor deployment\n  extraVolumes: []\n  config:\n    compaction:\n      # -- Duration to keep blocks\n      block_retention: 48h\n  service:\n    # -- Annotations for compactor service\n    annotations: {}\n\n# Configuration for the querier\nquerier:\n  # -- Number of replicas for the querier\n  replicas: 1\n  image:\n    # -- The Docker registry for the querier image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the querier image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the querier image. Overrides `tempo.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for querier pods\n  priorityClassName: null\n  # -- Labels for querier pods\n  podLabels: {}\n  # -- Annotations for querier pods\n  podAnnotations: {}\n  # -- Additional CLI args for the querier\n  extraArgs: []\n  # -- Environment variables to add to the querier pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the querier pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the querier\n  resources: {}\n  # -- Grace period to allow the querier to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for querier pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.querierSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.querierSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for querier pods\n  nodeSelector: {}\n  # -- Tolerations for querier pods\n  tolerations: []\n  # -- Extra volumes for querier pods\n  extraVolumeMounts: []\n  # -- Extra volumes for querier deployment\n  extraVolumes: []\n  config:\n    frontend_worker:\n      # -- grpc client configuration\n      grpc_client_config: {}\n  service:\n    # -- Annotations for querier service\n    annotations: {}\n\n# Configuration for the query-frontend\nqueryFrontend:\n  query:\n    # -- Required for grafana version \u003c7.5 for compatibility with jaeger-ui. Doesn't work on ARM arch\n    enabled: true\n    image:\n      # -- The Docker registry for the query-frontend image. Overrides `tempo.image.registry`\n      registry: null\n      # -- Docker image repository for the query-frontend image. Overrides `tempo.image.repository`\n      repository: grafana/tempo-query\n      # -- Docker image tag for the query-frontend image. Overrides `tempo.image.tag`\n      tag: null\n    # -- Resource requests and limits for the query\n    resources: {}\n    # -- Additional CLI args for tempo-query pods\n    extraArgs: []\n    # -- Environment variables to add to the tempo-query pods\n    extraEnv: []\n    # -- Environment variables from secrets or configmaps to add to the tempo-query pods\n    extraEnvFrom: []\n    # -- Extra volumes for tempo-query pods\n    extraVolumeMounts: []\n    # -- Extra volumes for tempo-query deployment\n    extraVolumes: []\n    config: |\n      backend: 127.0.0.1:3100\n  # -- Number of replicas for the query-frontend\n  replicas: 1\n  autoscaling:\n    # -- Enable autoscaling for the query-frontend\n    enabled: false\n    # -- Minimum autoscaling replicas for the query-frontend\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the query-frontend\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the query-frontend\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the query-frontend\n    targetMemoryUtilizationPercentage:\n  image:\n    # -- The Docker registry for the query-frontend image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the query-frontend image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the query-frontend image. Overrides `tempo.image.tag`\n    tag: null\n  service:\n    # -- Annotations for queryFrontend service\n    annotations: {}\n    # -- Type of service for the queryFrontend\n    type: ClusterIP\n  serviceDiscovery:\n    # -- Annotations for queryFrontendDiscovery service\n    annotations: {}\n  # -- The name of the PriorityClass for query-frontend pods\n  priorityClassName: null\n  # -- Labels for queryFrontend pods\n  podLabels: {}\n  # -- Annotations for query-frontend pods\n  podAnnotations: {}\n  # -- Additional CLI args for the query-frontend\n  extraArgs: []\n  # -- Environment variables to add to the query-frontend pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the query-frontend pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the query-frontend\n  resources: {}\n  # -- Grace period to allow the query-frontend to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for query-frontend pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.queryFrontendSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.queryFrontendSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for query-frontend pods\n  nodeSelector: {}\n  # -- Tolerations for query-frontend pods\n  tolerations: []\n  # -- Extra volumes for query-frontend pods\n  extraVolumeMounts: []\n  # -- Extra volumes for query-frontend deployment\n  extraVolumes: []\n\nsearch:\n  # -- Enable Tempo search\n  enabled: true\n\nmultitenancyEnabled: false\n\ntraces:\n  jaeger:\n    grpc:\n      # -- Enable Tempo to ingest Jaeger GRPC traces\n      enabled: true\n      # -- Jaeger GRPC receiver config\n      receiverConfig: {}\n    thriftBinary:\n      # -- Enable Tempo to ingest Jaeger Thrift Binary traces\n      enabled: true\n      # -- Jaeger Thrift Binary receiver config\n      receiverConfig: {}\n    thriftCompact:\n      # -- Enable Tempo to ingest Jaeger Thrift Compact traces\n      enabled: true\n      # -- Jaeger Thrift Compact receiver config\n      receiverConfig: {}\n    thriftHttp:\n      # -- Enable Tempo to ingest Jaeger Thrift HTTP traces\n      enabled: true\n      # -- Jaeger Thrift HTTP receiver config\n      receiverConfig: {}\n  zipkin:\n    # -- Enable Tempo to ingest Zipkin traces\n    enabled: false\n    # -- Zipkin receiver config\n    receiverConfig: {}\n  otlp:\n    http:\n      # -- Enable Tempo to ingest Open Telemetry HTTP traces\n      enabled: true\n      # -- HTTP receiver advanced config\n      receiverConfig: {}\n    grpc:\n      # -- Enable Tempo to ingest Open Telemetry GRPC traces\n      enabled: true\n      # -- GRPC receiver advanced config\n      receiverConfig: {}\n  opencensus:\n    # -- Enable Tempo to ingest Open Census traces\n    enabled: false\n    # -- Open Census receiver config\n    receiverConfig: {}\n  # -- Enable Tempo to ingest traces from Kafka. Reference: https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/kafkareceiver\n  kafka: {}\n\nconfig: |\n  multitenancy_enabled: {{ .Values.multitenancyEnabled }}\n  search_enabled: {{ .Values.search.enabled }}\n  metrics_generator_enabled: {{ .Values.metricsGenerator.enabled }}\n  compactor:\n    compaction:\n      block_retention: {{ .Values.compactor.config.compaction.block_retention }}\n    ring:\n      kvstore:\n        store: memberlist\n  {{- if .Values.metricsGenerator.enabled }}\n  metrics_generator:\n    ring:\n      kvstore:\n        store: memberlist\n    processor:\n      service_graphs:\n        max_items: {{ .Values.metricsGenerator.config.service_graphs_max_items }}\n    storage:\n      path: /var/tempo/wal\n      remote_write:\n        {{- toYaml .Values.metricsGenerator.config.storage_remote_write | nindent 6}}\n  {{- end }}\n  distributor:\n    ring:\n      kvstore:\n        store: memberlist\n    receivers:\n      {{- if  or (.Values.traces.jaeger.thriftCompact.enabled) (.Values.traces.jaeger.thriftBinary.enabled) (.Values.traces.jaeger.thriftHttp.enabled) (.Values.traces.jaeger.grpc.enabled) }}\n      jaeger:\n        protocols:\n          {{- if .Values.traces.jaeger.thriftCompact.enabled }}\n          thrift_compact:\n            {{- $mergedJaegerThriftCompactConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:6831\") .Values.traces.jaeger.thriftCompact.receiverConfig }}\n            {{- toYaml $mergedJaegerThriftCompactConfig | nindent 10 }}\n          {{- end }}\n          {{- if .Values.traces.jaeger.thriftBinary.enabled }}\n          thrift_binary:\n            {{- $mergedJaegerThriftBinaryConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:6832\") .Values.traces.jaeger.thriftBinary.receiverConfig }}\n            {{- toYaml $mergedJaegerThriftBinaryConfig | nindent 10 }}\n          {{- end }}\n          {{- if .Values.traces.jaeger.thriftHttp.enabled }}\n          thrift_http:\n            {{- $mergedJaegerThriftHttpConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:14268\") .Values.traces.jaeger.thriftHttp.receiverConfig }}\n            {{- toYaml $mergedJaegerThriftHttpConfig | nindent 10 }}\n          {{- end }}\n          {{- if .Values.traces.jaeger.grpc.enabled }}\n          grpc:\n            {{- $mergedJaegerGrpcConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:14250\") .Values.traces.jaeger.grpc.receiverConfig }}\n            {{- toYaml $mergedJaegerGrpcConfig | nindent 10 }}\n          {{- end }}\n      {{- end }}\n      {{- if .Values.traces.zipkin.enabled }}\n      zipkin:\n        {{- $mergedZipkinReceiverConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:9411\") .Values.traces.zipkin.receiverConfig }}\n        {{- toYaml $mergedZipkinReceiverConfig | nindent 6 }}\n      {{- end }}\n      {{- if or (.Values.traces.otlp.http.enabled) (.Values.traces.otlp.grpc.enabled) }}\n      otlp:\n        protocols:\n          {{- if .Values.traces.otlp.http.enabled }}\n          http:\n            {{- $mergedOtlpHttpReceiverConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:4318\") .Values.traces.otlp.http.receiverConfig }}\n            {{- toYaml $mergedOtlpHttpReceiverConfig | nindent 10 }}\n          {{- end }}\n          {{- if .Values.traces.otlp.grpc.enabled }}\n          grpc:\n            {{- $mergedOtlpGrpcReceiverConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:4317\") .Values.traces.otlp.grpc.receiverConfig }}\n            {{- toYaml $mergedOtlpGrpcReceiverConfig | nindent 10 }}\n          {{- end }}\n      {{- end }}\n      {{- if .Values.traces.opencensus.enabled }}\n      opencensus:\n        {{- $mergedOpencensusReceiverConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:55678\") .Values.traces.opencensus.receiverConfig }}\n        {{- toYaml $mergedOpencensusReceiverConfig | nindent 6 }}\n      {{- end }}\n      {{- if .Values.traces.kafka }}\n      kafka:\n        {{- toYaml .Values.traces.kafka | nindent 6 }}\n      {{- end }}\n    {{- if .Values.distributor.config.log_received_traces }}\n    log_received_traces: {{ .Values.distributor.config.log_received_traces }}\n    {{- end }}\n    {{- if .Values.distributor.config.extend_writes }}\n    extend_writes: {{ .Values.distributor.config.extend_writes }}\n    {{- end }}\n    {{- if .Values.distributor.config.search_tags_deny_list }}\n    search_tags_deny_list:\n      {{- with .Values.distributor.config.search_tags_deny_list }}\n      {{- toYaml . | nindent 4 }}\n      {{- end }}\n    {{- end }}\n  querier:\n    frontend_worker:\n      frontend_address: {{ include \"tempo.queryFrontendFullname\" . }}-discovery:9095\n      {{- if .Values.querier.config.frontend_worker.grpc_client_config }}\n      grpc_client_config:\n        {{- toYaml .Values.querier.config.frontend_worker.grpc_client_config | nindent 6 }}\n      {{- end }}\n  ingester:\n    lifecycler:\n      ring:\n        replication_factor: {{ .Values.ingester.config.replication_factor }}\n        kvstore:\n          store: memberlist\n      tokens_file_path: /var/tempo/tokens.json\n    {{- if .Values.ingester.config.trace_idle_period }}\n    trace_idle_period: {{ .Values.ingester.config.trace_idle_period }}\n    {{- end }}\n    {{- if .Values.ingester.config.flush_check_period }}\n    flush_check_period: {{ .Values.ingester.config.flush_check_period }}\n    {{- end }}\n    {{- if .Values.ingester.config.max_block_bytes }}\n    max_block_bytes: {{ .Values.ingester.config.max_block_bytes }}\n    {{- end }}\n    {{- if .Values.ingester.config.max_block_duration }}\n    max_block_duration: {{ .Values.ingester.config.max_block_duration }}\n    {{- end }}\n    {{- if .Values.ingester.config.complete_block_timeout }}\n    complete_block_timeout: {{ .Values.ingester.config.complete_block_timeout }}\n    {{- end }}\n  memberlist:\n    abort_if_cluster_join_fails: false\n    join_members:\n      - {{ include \"tempo.fullname\" . }}-gossip-ring\n  overrides:\n    {{- toYaml .Values.global_overrides | nindent 2 }}\n  server:\n    http_listen_port: {{ .Values.server.httpListenPort }}\n    log_level: {{ .Values.server.logLevel }}\n    log_format: {{ .Values.server.logFormat }}\n    grpc_server_max_recv_msg_size: {{ .Values.server.grpc_server_max_recv_msg_size }}\n    grpc_server_max_send_msg_size: {{ .Values.server.grpc_server_max_send_msg_size }}\n  storage:\n    trace:\n      backend: {{.Values.storage.trace.backend}}\n      {{- if eq .Values.storage.trace.backend \"gcs\"}}\n      gcs:\n        {{- toYaml .Values.storage.trace.gcs | nindent 6}}\n      {{- end}}\n      {{- if eq .Values.storage.trace.backend \"s3\"}}\n      s3:\n        {{- toYaml .Values.storage.trace.s3 | nindent 6}}\n      {{- end}}\n      {{- if eq .Values.storage.trace.backend \"azure\"}}\n      azure:\n        {{- toYaml .Values.storage.trace.azure | nindent 6}}\n      {{- end}}\n      blocklist_poll: 5m\n      local:\n        path: /var/tempo/traces\n      wal:\n        path: /var/tempo/wal\n      cache: memcached\n      memcached:\n        consistent_hash: true\n        host: {{ include \"tempo.fullname\" . }}-memcached\n        service: memcached-client\n        timeout: 500ms\n\n# Set Tempo server configuration\n# Refers to https://grafana.com/docs/tempo/latest/configuration/#server\nserver:\n  # --  HTTP server listen host\n  httpListenPort: 3100\n  # -- Log level. Can be set to trace, debug, info (default), warn error, fatal, panic\n  logLevel: info\n  # -- Log format. Can be set to logfmt (default) or json.\n  logFormat: logfmt\n  # -- Max gRPC message size that can be received\n  grpc_server_max_recv_msg_size: 4194304\n  # -- Max gRPC message size that can be sent\n  grpc_server_max_send_msg_size: 4194304\n# To configure a different storage backend instead of local storage:\n# storage:\n#   trace:\n#     backend: azure\n#     azure:\n#       container-name:\n#       storage-account-name:\n#       storage-account-key:\nstorage:\n  trace:\n    # -- The supported storage backends are gcs, s3 and azure, as specified in https://grafana.com/docs/tempo/latest/configuration/#storage\n    backend: local\n    local:\n      path: /var/tempo/traces\n    wal:\n      path: /var/tempo/wal\n\n# Global overrides\nglobal_overrides:\n  per_tenant_override_config: /conf/overrides.yaml\n  # metrics_generator_processors:\n  #   - service-graphs\n  #   - span-metrics\n\n# Per tenants overrides\noverrides: |\n  overrides: {}\n\n# memcached is for all of the Tempo pieces to coordinate with each other.\n# you can use your self memcacherd by set enable: false and host + service\nmemcached:\n  # -- Specified whether the memcached cachce should be enabled\n  enabled: true\n  image:\n    # -- The Docker registry for the Memcached image. Overrides `global.image.registry`\n    registry: null\n    # -- Memcached Docker image repository\n    repository: memcached\n    # -- Memcached Docker image tag\n    tag: 1.5.17-alpine\n    # -- Memcached Docker image pull policy\n    pullPolicy: IfNotPresent\n  host: memcached\n  # Number of replicas for memchached\n  replicas: 1\n  # -- Additional CLI args for memcached\n  extraArgs: []\n  # -- Environment variables to add to memcached pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to memcached pods\n  extraEnvFrom: []\n  # -- Labels for memcached pods\n  podLabels: {}\n  # -- Annotations for memcached pods\n  podAnnotations: {}\n  # -- Resource requests and limits for memcached\n  resources: {}\n  # -- Affinity for memcached pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.memcachedSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.memcachedSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  service:\n    # -- Annotations for memcached service\n    annotations: {}\n\nmemcachedExporter:\n  # -- Specifies whether the Memcached Exporter should be enabled\n  enabled: false\n  image:\n    # -- The Docker registry for the Memcached Exporter image. Overrides `global.image.registry`\n    registry: null\n    # -- Memcached Exporter Docker image repository\n    repository: prom/memcached-exporter\n    # -- Memcached Exporter Docker image tag\n    tag: v0.8.0\n    # -- Memcached Exporter Docker image pull policy\n    pullPolicy: IfNotPresent\n    # -- Memcached Exporter resource requests and limits\n  resources: {}\n\n# ServiceMonitor configuration\nserviceMonitor:\n  # -- If enabled, ServiceMonitor resources for Prometheus Operator are created\n  enabled: false\n  # -- Alternative namespace for ServiceMonitor resources\n  namespace: null\n  # -- Namespace selector for ServiceMonitor resources\n  namespaceSelector: {}\n  # -- ServiceMonitor annotations\n  annotations: {}\n  # -- Additional ServiceMonitor labels\n  labels: {}\n  # -- ServiceMonitor scrape interval\n  interval: null\n  # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)\n  scrapeTimeout: null\n  # -- ServiceMonitor relabel configs to apply to samples before scraping\n  # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n  relabelings: []\n  # -- ServiceMonitor metric relabel configs to apply to samples before ingestion\n  # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint\n  metricRelabelings: []\n  # -- ServiceMonitor will use http by default, but you can pick https as well\n  scheme: http\n  # -- ServiceMonitor will use these tlsConfig settings to make the health check requests\n  tlsConfig: null\n\n# Rules for the Prometheus Operator\nprometheusRule:\n  # -- If enabled, a PrometheusRule resource for Prometheus Operator is created\n  enabled: false\n  # -- Alternative namespace for the PrometheusRule resource\n  namespace: null\n  # -- PrometheusRule annotations\n  annotations: {}\n  # -- Additional PrometheusRule labels\n  labels: {}\n  # -- Contents of Prometheus rules file\n  groups: []\n  # - name: loki-rules\n  #   rules:\n  #     - record: job:loki_request_duration_seconds_bucket:sum_rate\n  #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job)\n  #     - record: job_route:loki_request_duration_seconds_bucket:sum_rate\n  #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job, route)\n  #     - record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate\n  #       expr: sum(rate(container_cpu_usage_seconds_total[1m])) by (node, namespace, pod, container)\n\n# Configuration for the gateway\ngateway:\n  # -- Specifies whether the gateway should be enabled\n  enabled: false\n  # -- Number of replicas for the gateway\n  replicas: 1\n  autoscaling:\n    # -- Enable autoscaling for the gateway\n    enabled: false\n    # -- Minimum autoscaling replicas for the gateway\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the gateway\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the gateway\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the gateway\n    targetMemoryUtilizationPercentage:\n  # -- Enable logging of 2xx and 3xx HTTP requests\n  verboseLogging: true\n  image:\n    # -- The Docker registry for the gateway image. Overrides `global.image.registry`\n    registry: null\n    # -- The gateway image repository\n    repository: nginxinc/nginx-unprivileged\n    # -- The gateway image tag\n    tag: 1.19-alpine\n    # -- The gateway image pull policy\n    pullPolicy: IfNotPresent\n  # -- The name of the PriorityClass for gateway pods\n  priorityClassName: null\n  # -- Labels for gateway pods\n  podLabels: {}\n  # -- Annotations for gateway pods\n  podAnnotations: {}\n  # -- Additional CLI args for the gateway\n  extraArgs: []\n  # -- Environment variables to add to the gateway pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the gateway pods\n  extraEnvFrom: []\n  # -- Volumes to add to the gateway pods\n  extraVolumes: []\n  # -- Volume mounts to add to the gateway pods\n  extraVolumeMounts: []\n  # -- Resource requests and limits for the gateway\n  resources: {}\n  # -- Grace period to allow the gateway to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for gateway pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.gatewaySelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.gatewaySelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for gateway pods\n  nodeSelector: {}\n  # -- Tolerations for gateway pods\n  tolerations: []\n  # Gateway service configuration\n  service:\n    # -- Port of the gateway service\n    port: 80\n    # -- Type of the gateway service\n    type: ClusterIP\n    # -- ClusterIP of the gateway service\n    clusterIP: null\n    # -- Node port if service type is NodePort\n    nodePort: null\n    # -- Load balancer IPO address if service type is LoadBalancer\n    loadBalancerIP: null\n    # -- Annotations for the gateway service\n    annotations: {}\n    # -- Labels for gateway service\n    labels: {}\n  # Gateway ingress configuration\n  ingress:\n    # -- Specifies whether an ingress for the gateway should be created\n    enabled: false\n    # -- Ingress Class Name. MAY be required for Kubernetes versions \u003e= 1.18\n    # ingressClassName: nginx\n    # -- Annotations for the gateway ingress\n    annotations: {}\n    # -- Hosts configuration for the gateway ingress\n    hosts:\n      - host: gateway.tempo.example.com\n        paths:\n          - path: /\n            # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers\n            # pathType: Prefix\n    # -- TLS configuration for the gateway ingress\n    tls:\n      - secretName: tempo-gateway-tls\n        hosts:\n          - gateway.tempo.example.com\n  # Basic auth configuration\n  basicAuth:\n    # -- Enables basic authentication for the gateway\n    enabled: false\n    # -- The basic auth username for the gateway\n    username: null\n    # -- The basic auth password for the gateway\n    password: null\n    # -- Uses the specified username and password to compute a htpasswd using Sprig's `htpasswd` function.\n    # The value is templated using `tpl`. Override this to use a custom htpasswd, e.g. in case the default causes\n    # high CPU load.\n    htpasswd: \u003e-\n      {{ htpasswd (required \"'gateway.basicAuth.username' is required\" .Values.gateway.basicAuth.username) (required \"'gateway.basicAuth.password' is required\" .Values.gateway.basicAuth.password) }}\n    # -- Existing basic auth secret to use. Must contain '.htpasswd'\n    existingSecret: null\n  # Configures the readiness probe for the gateway\n  readinessProbe:\n    httpGet:\n      path: /\n      port: http\n    initialDelaySeconds: 15\n    timeoutSeconds: 1\n  nginxConfig:\n    # -- NGINX log format\n    logFormat: |-\n      main '$remote_addr - $remote_user [$time_local]  $status '\n              '\"$request\" $body_bytes_sent \"$http_referer\" '\n              '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n    # -- Allows appending custom configuration to the server block\n    serverSnippet: \"\"\n    # -- Allows appending custom configuration to the http block\n    httpSnippet: \"\"\n    # -- Config file contents for Nginx. Passed through the `tpl` function to allow templating\n    # @default -- See values.yaml\n    file: |\n      worker_processes  5;  ## Default: 1\n      error_log  /dev/stderr;\n      pid        /tmp/nginx.pid;\n      worker_rlimit_nofile 8192;\n\n      events {\n        worker_connections  4096;  ## Default: 1024\n      }\n\n      http {\n        client_body_temp_path /tmp/client_temp;\n        proxy_temp_path       /tmp/proxy_temp_path;\n        fastcgi_temp_path     /tmp/fastcgi_temp;\n        uwsgi_temp_path       /tmp/uwsgi_temp;\n        scgi_temp_path        /tmp/scgi_temp;\n\n        proxy_http_version    1.1;\n\n        default_type application/octet-stream;\n        log_format   {{ .Values.gateway.nginxConfig.logFormat }}\n\n        {{- if .Values.gateway.verboseLogging }}\n        access_log   /dev/stderr  main;\n        {{- else }}\n\n        map $status $loggable {\n          ~^[23]  0;\n          default 1;\n        }\n        access_log   /dev/stderr  main  if=$loggable;\n        {{- end }}\n\n        sendfile     on;\n        tcp_nopush   on;\n        resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};\n\n        {{- with .Values.gateway.nginxConfig.httpSnippet }}\n        {{ . | nindent 2 }}\n        {{- end }}\n\n        server {\n          listen             8080;\n\n          {{- if .Values.gateway.basicAuth.enabled }}\n          auth_basic           \"Tempo\";\n          auth_basic_user_file /etc/nginx/secrets/.htpasswd;\n          {{- end }}\n\n          location = / {\n            return 200 'OK';\n            auth_basic off;\n          }\n\n          location = /jaeger/api/traces {\n            proxy_pass       http://{{ include \"tempo.distributorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:14268/api/traces;\n          }\n\n          location = /zipkin/spans {\n            proxy_pass       http://{{ include \"tempo.distributorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:9411/spans;\n          }\n\n          location = /otlp/v1/traces {\n            proxy_pass       http://{{ include \"tempo.distributorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:4318/v1/traces;\n          }\n\n          location ^~ /api {\n            proxy_pass       http://{{ include \"tempo.queryFrontendFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /flush {\n            proxy_pass       http://{{ include \"tempo.ingesterFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /shutdown {\n            proxy_pass       http://{{ include \"tempo.ingesterFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /distributor/ring {\n            proxy_pass       http://{{ include \"tempo.distributorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /ingester/ring {\n            proxy_pass       http://{{ include \"tempo.distributorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /compactor/ring {\n            proxy_pass       http://{{ include \"tempo.compactorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          {{- with .Values.gateway.nginxConfig.serverSnippet }}\n          {{ . | nindent 4 }}\n          {{- end }}\n        }\n      }\n",
            "template": "global:\n  image:\n    # -- Overrides the Docker registry globally for all images\n    registry: docker.io\n  # -- Overrides the priorityClassName for all pods\n  priorityClassName: null\n  # -- configures cluster domain (\"cluster.local\" by default)\n  clusterDomain: \"cluster.local\"\n  # -- configures DNS service name\n  dnsService: \"kube-dns\"\n  # -- configures DNS service namespace\n  dnsNamespace: \"kube-system\"\n# -- Overrides the chart's computed fullname\n# fullnameOverride: tempo\ntempo:\n  image:\n    # -- The Docker registry\n    registry: docker.io\n    # -- Docker image repository\n    repository: grafana/tempo\n    # -- Overrides the image tag whose default is the chart's appVersion\n    tag: null\n    pullPolicy: IfNotPresent\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http\n    initialDelaySeconds: 30\n    timeoutSeconds: 1\n  # -- Global labels for all tempo pods\n  podLabels: {}\n  # -- Common annotations for all pods\n  podAnnotations: {}\n  # -- SecurityContext holds pod-level security attributes and common container settings\n  securityContext: {}\n  #  capabilities:\n  #    drop:\n  #    - ALL\n  #  readOnlyRootFilesystem: true\n  #  runAsNonRoot: true\n  #  runAsUser: 1000\n  # -- Structured tempo configuration\n  structuredConfig: {}\n\nserviceAccount:\n  # -- Specifies whether a ServiceAccount should be created\n  create: true\n  # -- The name of the ServiceAccount to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: null\n  # -- Image pull secrets for the service account\n  imagePullSecrets: []\n  # -- Annotations for the service account\n  annotations: {}\n\nrbac:\n  # -- Specifies whether RBAC manifests should be created\n  create: false\n  # -- Specifies whether a PodSecurityPolicy should be created\n  pspEnabled: false\n\n# Configuration for the ingester\ningester:\n  # -- Annotations for the ingester StatefulSet\n  annotations: {}\n  # -- Number of replicas for the ingester\n  replicas: 3\n  autoscaling:\n    # -- Enable autoscaling for the ingester\n    enabled: false\n    # -- Minimum autoscaling replicas for the ingester\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the ingester\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the ingester\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the ingester\n    targetMemoryUtilizationPercentage:\n  image:\n    # -- The Docker registry for the ingester image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the ingester image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the ingester image. Overrides `tempo.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for ingester pods\n  priorityClassName: null\n  # -- Labels for ingester pods\n  podLabels: {}\n  # -- Annotations for ingester pods\n  podAnnotations: {}\n  # -- Additional CLI args for the ingester\n  extraArgs: []\n  # -- Environment variables to add to the ingester pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the ingester pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the ingester\n  resources: {}\n  # -- Grace period to allow the ingester to shutdown before it is killed. Especially for the ingestor,\n  # this must be increased. It must be long enough so ingesters can be gracefully shutdown flushing/transferring\n  # all data and to successfully leave the member ring on shutdown.\n  terminationGracePeriodSeconds: 300\n  # -- Affinity for ingester pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Soft node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.ingesterSelectorLabels\" . | nindent 12 }}\n            topologyKey: kubernetes.io/hostname\n        - weight: 75\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.ingesterSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for ingester pods\n  nodeSelector: {}\n  # -- Tolerations for ingester pods\n  tolerations: []\n  # -- Extra volumes for ingester pods\n  extraVolumeMounts: []\n  # -- Extra volumes for ingester deployment\n  extraVolumes: []\n  persistence:\n    # -- Enable creating PVCs which is required when using boltdb-shipper\n    enabled: false\n    # -- Size of persistent disk\n    size: 10Gi\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n  config:\n    # -- Number of copies of spans to store in the ingester ring\n    replication_factor: 3\n    # -- Amount of time a trace must be idle before flushing it to the wal.\n    trace_idle_period: null\n    # -- How often to sweep all tenants and move traces from live -\u003e wal -\u003e completed blocks.\n    flush_check_period: null\n    # -- Maximum size of a block before cutting it\n    max_block_bytes: null\n    # -- Maximum length of time before cutting a block\n    max_block_duration: null\n    # -- Duration to keep blocks in the ingester after they have been flushed\n    complete_block_timeout: null\n  service:\n    # -- Annotations for ingester service\n    annotations: {}\n\n# Configuration for the metrics-generator\nmetricsGenerator:\n  # -- Specifies whether a metrics-generator should be deployed\n  enabled: false\n  # -- Annotations for the metrics-generator StatefulSet\n  annotations: {}\n  # -- Number of replicas for the metrics-generator\n  replicas: 1\n  image:\n    # -- The Docker registry for the metrics-generator image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the metrics-generator image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the metrics-generator image. Overrides `tempo.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for metrics-generator pods\n  priorityClassName: null\n  # -- Labels for metrics-generator pods\n  podLabels: {}\n  # -- Annotations for metrics-generator pods\n  podAnnotations: {}\n  # -- Additional CLI args for the metrics-generator\n  extraArgs: []\n  # -- Environment variables to add to the metrics-generator pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the metrics-generator pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the metrics-generator\n  resources: {}\n  # -- Grace period to allow the metrics-generator to shutdown before it is killed. Especially for the ingestor,\n  # this must be increased. It must be long enough so metrics-generators can be gracefully shutdown flushing/transferring\n  # all data and to successfully leave the member ring on shutdown.\n  terminationGracePeriodSeconds: 300\n  # -- Affinity for metrics-generator pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.metricsGeneratorSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.metricsGeneratorSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for metrics-generator pods\n  nodeSelector: {}\n  # -- Tolerations for metrics-generator pods\n  tolerations: []\n  # -- Extra volumes for metrics-generator pods\n  extraVolumeMounts: []\n  # -- Extra volumes for metrics-generator deployment\n  extraVolumes: []\n  # -- Default ports\n  ports:\n    - name: grpc\n      port: 9095\n      service: true\n    - name: http-memberlist\n      port: 7946\n      service: false\n    - name: http\n      port: 3100\n      service: true\n  config:\n    #  MaxItems is the amount of edges that will be stored in the store.\n    service_graphs_max_items: 10000\n    storage_remote_write: []\n    # - url: http://cortex/api/v1/push\n    #   send_exemplars: true\n    #   headers:\n    #     x-scope-orgid: operations\n  service:\n    # -- Annotations for Metrics Generator service\n    annotations: {}\n\n# Configuration for the distributor\ndistributor:\n  # -- Number of replicas for the distributor\n  replicas: 1\n  autoscaling:\n    # -- Enable autoscaling for the distributor\n    enabled: false\n    # -- Minimum autoscaling replicas for the distributor\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the distributor\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the distributor\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the distributor\n    targetMemoryUtilizationPercentage:\n  image:\n    # -- The Docker registry for the ingester image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the ingester image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the ingester image. Overrides `tempo.image.tag`\n    tag: null\n  service:\n    # -- Annotations for distributor service\n    annotations: {}\n    # -- Type of service for the distributor\n    type: ClusterIP\n    # -- If type is LoadBalancer you can assign the IP to the LoadBalancer\n    loadBalancerIP: \"\"\n    # -- If type is LoadBalancer limit incoming traffic from IPs.\n    loadBalancerSourceRanges: []\n  # -- The name of the PriorityClass for distributor pods\n  priorityClassName: null\n  # -- Labels for distributor pods\n  podLabels: {}\n  # -- Annotations for distributor pods\n  podAnnotations: {}\n  # -- Additional CLI args for the distributor\n  extraArgs: []\n  # -- Environment variables to add to the distributor pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the distributor pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the distributor\n  resources: {}\n  # -- Grace period to allow the distributor to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for distributor pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.distributorSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.distributorSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for distributor pods\n  nodeSelector: {}\n  # -- Tolerations for distributor pods\n  tolerations: []\n  # -- Extra volumes for distributor pods\n  extraVolumeMounts: []\n  # -- Extra volumes for distributor deployment\n  extraVolumes: []\n  config:\n    # -- Enable to log every received trace id to help debug ingestion\n    log_received_traces: null\n    # -- Disables write extension with inactive ingesters\n    extend_writes: null\n    # -- List of tags that will not be extracted from trace data for search lookups\n    search_tags_deny_list: []\n\ncompactor:\n  # -- Number of replicas for the compactor\n  replicas: 1\n  image:\n    # -- The Docker registry for the compactor image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the compactor image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the compactor image. Overrides `tempo.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for compactor pods\n  priorityClassName: null\n  # -- Labels for compactor pods\n  podLabels: {}\n  # -- Annotations for compactor pods\n  podAnnotations: {}\n  # -- Additional CLI args for the compactor\n  extraArgs: []\n  # -- Environment variables to add to the compactor pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the compactor pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the compactor\n  resources: {}\n  # -- Grace period to allow the compactor to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Node selector for compactor pods\n  nodeSelector: {}\n  # -- Tolerations for compactor pods\n  tolerations: []\n  # -- Extra volumes for compactor pods\n  extraVolumeMounts: []\n  # -- Extra volumes for compactor deployment\n  extraVolumes: []\n  config:\n    compaction:\n      # -- Duration to keep blocks\n      block_retention: 48h\n  service:\n    # -- Annotations for compactor service\n    annotations: {}\n\n# Configuration for the querier\nquerier:\n  # -- Number of replicas for the querier\n  replicas: 1\n  image:\n    # -- The Docker registry for the querier image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the querier image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the querier image. Overrides `tempo.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for querier pods\n  priorityClassName: null\n  # -- Labels for querier pods\n  podLabels: {}\n  # -- Annotations for querier pods\n  podAnnotations: {}\n  # -- Additional CLI args for the querier\n  extraArgs: []\n  # -- Environment variables to add to the querier pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the querier pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the querier\n  resources: {}\n  # -- Grace period to allow the querier to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for querier pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.querierSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.querierSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for querier pods\n  nodeSelector: {}\n  # -- Tolerations for querier pods\n  tolerations: []\n  # -- Extra volumes for querier pods\n  extraVolumeMounts: []\n  # -- Extra volumes for querier deployment\n  extraVolumes: []\n  config:\n    frontend_worker:\n      # -- grpc client configuration\n      grpc_client_config: {}\n  service:\n    # -- Annotations for querier service\n    annotations: {}\n\n# Configuration for the query-frontend\nqueryFrontend:\n  query:\n    # -- Required for grafana version \u003c7.5 for compatibility with jaeger-ui. Doesn't work on ARM arch\n    enabled: true\n    image:\n      # -- The Docker registry for the query-frontend image. Overrides `tempo.image.registry`\n      registry: null\n      # -- Docker image repository for the query-frontend image. Overrides `tempo.image.repository`\n      repository: grafana/tempo-query\n      # -- Docker image tag for the query-frontend image. Overrides `tempo.image.tag`\n      tag: null\n    # -- Resource requests and limits for the query\n    resources: {}\n    # -- Additional CLI args for tempo-query pods\n    extraArgs: []\n    # -- Environment variables to add to the tempo-query pods\n    extraEnv: []\n    # -- Environment variables from secrets or configmaps to add to the tempo-query pods\n    extraEnvFrom: []\n    # -- Extra volumes for tempo-query pods\n    extraVolumeMounts: []\n    # -- Extra volumes for tempo-query deployment\n    extraVolumes: []\n    config: |\n      backend: 127.0.0.1:3100\n  # -- Number of replicas for the query-frontend\n  replicas: 1\n  autoscaling:\n    # -- Enable autoscaling for the query-frontend\n    enabled: false\n    # -- Minimum autoscaling replicas for the query-frontend\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the query-frontend\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the query-frontend\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the query-frontend\n    targetMemoryUtilizationPercentage:\n  image:\n    # -- The Docker registry for the query-frontend image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the query-frontend image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the query-frontend image. Overrides `tempo.image.tag`\n    tag: null\n  service:\n    # -- Annotations for queryFrontend service\n    annotations: {}\n    # -- Type of service for the queryFrontend\n    type: ClusterIP\n  serviceDiscovery:\n    # -- Annotations for queryFrontendDiscovery service\n    annotations: {}\n  # -- The name of the PriorityClass for query-frontend pods\n  priorityClassName: null\n  # -- Labels for queryFrontend pods\n  podLabels: {}\n  # -- Annotations for query-frontend pods\n  podAnnotations: {}\n  # -- Additional CLI args for the query-frontend\n  extraArgs: []\n  # -- Environment variables to add to the query-frontend pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the query-frontend pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the query-frontend\n  resources: {}\n  # -- Grace period to allow the query-frontend to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for query-frontend pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.queryFrontendSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.queryFrontendSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for query-frontend pods\n  nodeSelector: {}\n  # -- Tolerations for query-frontend pods\n  tolerations: []\n  # -- Extra volumes for query-frontend pods\n  extraVolumeMounts: []\n  # -- Extra volumes for query-frontend deployment\n  extraVolumes: []\n\nsearch:\n  # -- Enable Tempo search\n  enabled: true\n\nmultitenancyEnabled: false\n\ntraces:\n  jaeger:\n    grpc:\n      # -- Enable Tempo to ingest Jaeger GRPC traces\n      enabled: true\n      # -- Jaeger GRPC receiver config\n      receiverConfig: {}\n    thriftBinary:\n      # -- Enable Tempo to ingest Jaeger Thrift Binary traces\n      enabled: true\n      # -- Jaeger Thrift Binary receiver config\n      receiverConfig: {}\n    thriftCompact:\n      # -- Enable Tempo to ingest Jaeger Thrift Compact traces\n      enabled: true\n      # -- Jaeger Thrift Compact receiver config\n      receiverConfig: {}\n    thriftHttp:\n      # -- Enable Tempo to ingest Jaeger Thrift HTTP traces\n      enabled: true\n      # -- Jaeger Thrift HTTP receiver config\n      receiverConfig: {}\n  zipkin:\n    # -- Enable Tempo to ingest Zipkin traces\n    enabled: false\n    # -- Zipkin receiver config\n    receiverConfig: {}\n  otlp:\n    http:\n      # -- Enable Tempo to ingest Open Telemetry HTTP traces\n      enabled: true\n      # -- HTTP receiver advanced config\n      receiverConfig: {}\n    grpc:\n      # -- Enable Tempo to ingest Open Telemetry GRPC traces\n      enabled: true\n      # -- GRPC receiver advanced config\n      receiverConfig: {}\n  opencensus:\n    # -- Enable Tempo to ingest Open Census traces\n    enabled: false\n    # -- Open Census receiver config\n    receiverConfig: {}\n  # -- Enable Tempo to ingest traces from Kafka. Reference: https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/kafkareceiver\n  kafka: {}\n\nconfig: |\n  multitenancy_enabled: {{ .Values.multitenancyEnabled }}\n  search_enabled: {{ .Values.search.enabled }}\n  metrics_generator_enabled: {{ .Values.metricsGenerator.enabled }}\n  compactor:\n    compaction:\n      block_retention: {{ .Values.compactor.config.compaction.block_retention }}\n    ring:\n      kvstore:\n        store: memberlist\n  {{- if .Values.metricsGenerator.enabled }}\n  metrics_generator:\n    ring:\n      kvstore:\n        store: memberlist\n    processor:\n      service_graphs:\n        max_items: {{ .Values.metricsGenerator.config.service_graphs_max_items }}\n    storage:\n      path: /var/tempo/wal\n      remote_write:\n        {{- toYaml .Values.metricsGenerator.config.storage_remote_write | nindent 6}}\n  {{- end }}\n  distributor:\n    ring:\n      kvstore:\n        store: memberlist\n    receivers:\n      {{- if  or (.Values.traces.jaeger.thriftCompact.enabled) (.Values.traces.jaeger.thriftBinary.enabled) (.Values.traces.jaeger.thriftHttp.enabled) (.Values.traces.jaeger.grpc.enabled) }}\n      jaeger:\n        protocols:\n          {{- if .Values.traces.jaeger.thriftCompact.enabled }}\n          thrift_compact:\n            {{- $mergedJaegerThriftCompactConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:6831\") .Values.traces.jaeger.thriftCompact.receiverConfig }}\n            {{- toYaml $mergedJaegerThriftCompactConfig | nindent 10 }}\n          {{- end }}\n          {{- if .Values.traces.jaeger.thriftBinary.enabled }}\n          thrift_binary:\n            {{- $mergedJaegerThriftBinaryConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:6832\") .Values.traces.jaeger.thriftBinary.receiverConfig }}\n            {{- toYaml $mergedJaegerThriftBinaryConfig | nindent 10 }}\n          {{- end }}\n          {{- if .Values.traces.jaeger.thriftHttp.enabled }}\n          thrift_http:\n            {{- $mergedJaegerThriftHttpConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:14268\") .Values.traces.jaeger.thriftHttp.receiverConfig }}\n            {{- toYaml $mergedJaegerThriftHttpConfig | nindent 10 }}\n          {{- end }}\n          {{- if .Values.traces.jaeger.grpc.enabled }}\n          grpc:\n            {{- $mergedJaegerGrpcConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:14250\") .Values.traces.jaeger.grpc.receiverConfig }}\n            {{- toYaml $mergedJaegerGrpcConfig | nindent 10 }}\n          {{- end }}\n      {{- end }}\n      {{- if .Values.traces.zipkin.enabled }}\n      zipkin:\n        {{- $mergedZipkinReceiverConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:9411\") .Values.traces.zipkin.receiverConfig }}\n        {{- toYaml $mergedZipkinReceiverConfig | nindent 6 }}\n      {{- end }}\n      {{- if or (.Values.traces.otlp.http.enabled) (.Values.traces.otlp.grpc.enabled) }}\n      otlp:\n        protocols:\n          {{- if .Values.traces.otlp.http.enabled }}\n          http:\n            {{- $mergedOtlpHttpReceiverConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:4318\") .Values.traces.otlp.http.receiverConfig }}\n            {{- toYaml $mergedOtlpHttpReceiverConfig | nindent 10 }}\n          {{- end }}\n          {{- if .Values.traces.otlp.grpc.enabled }}\n          grpc:\n            {{- $mergedOtlpGrpcReceiverConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:4317\") .Values.traces.otlp.grpc.receiverConfig }}\n            {{- toYaml $mergedOtlpGrpcReceiverConfig | nindent 10 }}\n          {{- end }}\n      {{- end }}\n      {{- if .Values.traces.opencensus.enabled }}\n      opencensus:\n        {{- $mergedOpencensusReceiverConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:55678\") .Values.traces.opencensus.receiverConfig }}\n        {{- toYaml $mergedOpencensusReceiverConfig | nindent 6 }}\n      {{- end }}\n      {{- if .Values.traces.kafka }}\n      kafka:\n        {{- toYaml .Values.traces.kafka | nindent 6 }}\n      {{- end }}\n    {{- if .Values.distributor.config.log_received_traces }}\n    log_received_traces: {{ .Values.distributor.config.log_received_traces }}\n    {{- end }}\n    {{- if .Values.distributor.config.extend_writes }}\n    extend_writes: {{ .Values.distributor.config.extend_writes }}\n    {{- end }}\n    {{- if .Values.distributor.config.search_tags_deny_list }}\n    search_tags_deny_list:\n      {{- with .Values.distributor.config.search_tags_deny_list }}\n      {{- toYaml . | nindent 4 }}\n      {{- end }}\n    {{- end }}\n  querier:\n    frontend_worker:\n      frontend_address: {{ include \"tempo.queryFrontendFullname\" . }}-discovery:9095\n      {{- if .Values.querier.config.frontend_worker.grpc_client_config }}\n      grpc_client_config:\n        {{- toYaml .Values.querier.config.frontend_worker.grpc_client_config | nindent 6 }}\n      {{- end }}\n  ingester:\n    lifecycler:\n      ring:\n        replication_factor: {{ .Values.ingester.config.replication_factor }}\n        kvstore:\n          store: memberlist\n      tokens_file_path: /var/tempo/tokens.json\n    {{- if .Values.ingester.config.trace_idle_period }}\n    trace_idle_period: {{ .Values.ingester.config.trace_idle_period }}\n    {{- end }}\n    {{- if .Values.ingester.config.flush_check_period }}\n    flush_check_period: {{ .Values.ingester.config.flush_check_period }}\n    {{- end }}\n    {{- if .Values.ingester.config.max_block_bytes }}\n    max_block_bytes: {{ .Values.ingester.config.max_block_bytes }}\n    {{- end }}\n    {{- if .Values.ingester.config.max_block_duration }}\n    max_block_duration: {{ .Values.ingester.config.max_block_duration }}\n    {{- end }}\n    {{- if .Values.ingester.config.complete_block_timeout }}\n    complete_block_timeout: {{ .Values.ingester.config.complete_block_timeout }}\n    {{- end }}\n  memberlist:\n    abort_if_cluster_join_fails: false\n    join_members:\n      - {{ include \"tempo.fullname\" . }}-gossip-ring\n  overrides:\n    {{- toYaml .Values.global_overrides | nindent 2 }}\n  server:\n    http_listen_port: {{ .Values.server.httpListenPort }}\n    log_level: {{ .Values.server.logLevel }}\n    log_format: {{ .Values.server.logFormat }}\n    grpc_server_max_recv_msg_size: {{ .Values.server.grpc_server_max_recv_msg_size }}\n    grpc_server_max_send_msg_size: {{ .Values.server.grpc_server_max_send_msg_size }}\n  storage:\n    trace:\n      backend: {{.Values.storage.trace.backend}}\n      {{- if eq .Values.storage.trace.backend \"gcs\"}}\n      gcs:\n        {{- toYaml .Values.storage.trace.gcs | nindent 6}}\n      {{- end}}\n      {{- if eq .Values.storage.trace.backend \"s3\"}}\n      s3:\n        {{- toYaml .Values.storage.trace.s3 | nindent 6}}\n      {{- end}}\n      {{- if eq .Values.storage.trace.backend \"azure\"}}\n      azure:\n        {{- toYaml .Values.storage.trace.azure | nindent 6}}\n      {{- end}}\n      blocklist_poll: 5m\n      local:\n        path: /var/tempo/traces\n      wal:\n        path: /var/tempo/wal\n      cache: memcached\n      memcached:\n        consistent_hash: true\n        host: {{ include \"tempo.fullname\" . }}-memcached\n        service: memcached-client\n        timeout: 500ms\n\n# Set Tempo server configuration\n# Refers to https://grafana.com/docs/tempo/latest/configuration/#server\nserver:\n  # --  HTTP server listen host\n  httpListenPort: 3100\n  # -- Log level. Can be set to trace, debug, info (default), warn error, fatal, panic\n  logLevel: info\n  # -- Log format. Can be set to logfmt (default) or json.\n  logFormat: logfmt\n  # -- Max gRPC message size that can be received\n  grpc_server_max_recv_msg_size: 4194304\n  # -- Max gRPC message size that can be sent\n  grpc_server_max_send_msg_size: 4194304\n# To configure a different storage backend instead of local storage:\n# storage:\n#   trace:\n#     backend: azure\n#     azure:\n#       container-name:\n#       storage-account-name:\n#       storage-account-key:\nstorage:\n  trace:\n    # -- The supported storage backends are gcs, s3 and azure, as specified in https://grafana.com/docs/tempo/latest/configuration/#storage\n    backend: local\n    local:\n      path: /var/tempo/traces\n    wal:\n      path: /var/tempo/wal\n\n# Global overrides\nglobal_overrides:\n  per_tenant_override_config: /conf/overrides.yaml\n  # metrics_generator_processors:\n  #   - service-graphs\n  #   - span-metrics\n\n# Per tenants overrides\noverrides: |\n  overrides: {}\n\n# memcached is for all of the Tempo pieces to coordinate with each other.\n# you can use your self memcacherd by set enable: false and host + service\nmemcached:\n  # -- Specified whether the memcached cachce should be enabled\n  enabled: true\n  image:\n    # -- The Docker registry for the Memcached image. Overrides `global.image.registry`\n    registry: null\n    # -- Memcached Docker image repository\n    repository: memcached\n    # -- Memcached Docker image tag\n    tag: 1.5.17-alpine\n    # -- Memcached Docker image pull policy\n    pullPolicy: IfNotPresent\n  host: memcached\n  # Number of replicas for memchached\n  replicas: 1\n  # -- Additional CLI args for memcached\n  extraArgs: []\n  # -- Environment variables to add to memcached pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to memcached pods\n  extraEnvFrom: []\n  # -- Labels for memcached pods\n  podLabels: {}\n  # -- Annotations for memcached pods\n  podAnnotations: {}\n  # -- Resource requests and limits for memcached\n  resources: {}\n  # -- Affinity for memcached pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.memcachedSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.memcachedSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  service:\n    # -- Annotations for memcached service\n    annotations: {}\n\nmemcachedExporter:\n  # -- Specifies whether the Memcached Exporter should be enabled\n  enabled: false\n  image:\n    # -- The Docker registry for the Memcached Exporter image. Overrides `global.image.registry`\n    registry: null\n    # -- Memcached Exporter Docker image repository\n    repository: prom/memcached-exporter\n    # -- Memcached Exporter Docker image tag\n    tag: v0.8.0\n    # -- Memcached Exporter Docker image pull policy\n    pullPolicy: IfNotPresent\n    # -- Memcached Exporter resource requests and limits\n  resources: {}\n\n# ServiceMonitor configuration\nserviceMonitor:\n  # -- If enabled, ServiceMonitor resources for Prometheus Operator are created\n  enabled: false\n  # -- Alternative namespace for ServiceMonitor resources\n  namespace: null\n  # -- Namespace selector for ServiceMonitor resources\n  namespaceSelector: {}\n  # -- ServiceMonitor annotations\n  annotations: {}\n  # -- Additional ServiceMonitor labels\n  labels: {}\n  # -- ServiceMonitor scrape interval\n  interval: null\n  # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)\n  scrapeTimeout: null\n  # -- ServiceMonitor relabel configs to apply to samples before scraping\n  # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n  relabelings: []\n  # -- ServiceMonitor metric relabel configs to apply to samples before ingestion\n  # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint\n  metricRelabelings: []\n  # -- ServiceMonitor will use http by default, but you can pick https as well\n  scheme: http\n  # -- ServiceMonitor will use these tlsConfig settings to make the health check requests\n  tlsConfig: null\n\n# Rules for the Prometheus Operator\nprometheusRule:\n  # -- If enabled, a PrometheusRule resource for Prometheus Operator is created\n  enabled: false\n  # -- Alternative namespace for the PrometheusRule resource\n  namespace: null\n  # -- PrometheusRule annotations\n  annotations: {}\n  # -- Additional PrometheusRule labels\n  labels: {}\n  # -- Contents of Prometheus rules file\n  groups: []\n  # - name: loki-rules\n  #   rules:\n  #     - record: job:loki_request_duration_seconds_bucket:sum_rate\n  #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job)\n  #     - record: job_route:loki_request_duration_seconds_bucket:sum_rate\n  #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job, route)\n  #     - record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate\n  #       expr: sum(rate(container_cpu_usage_seconds_total[1m])) by (node, namespace, pod, container)\n\n# Configuration for the gateway\ngateway:\n  # -- Specifies whether the gateway should be enabled\n  enabled: false\n  # -- Number of replicas for the gateway\n  replicas: 1\n  autoscaling:\n    # -- Enable autoscaling for the gateway\n    enabled: false\n    # -- Minimum autoscaling replicas for the gateway\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the gateway\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the gateway\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the gateway\n    targetMemoryUtilizationPercentage:\n  # -- Enable logging of 2xx and 3xx HTTP requests\n  verboseLogging: true\n  image:\n    # -- The Docker registry for the gateway image. Overrides `global.image.registry`\n    registry: null\n    # -- The gateway image repository\n    repository: nginxinc/nginx-unprivileged\n    # -- The gateway image tag\n    tag: 1.19-alpine\n    # -- The gateway image pull policy\n    pullPolicy: IfNotPresent\n  # -- The name of the PriorityClass for gateway pods\n  priorityClassName: null\n  # -- Labels for gateway pods\n  podLabels: {}\n  # -- Annotations for gateway pods\n  podAnnotations: {}\n  # -- Additional CLI args for the gateway\n  extraArgs: []\n  # -- Environment variables to add to the gateway pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the gateway pods\n  extraEnvFrom: []\n  # -- Volumes to add to the gateway pods\n  extraVolumes: []\n  # -- Volume mounts to add to the gateway pods\n  extraVolumeMounts: []\n  # -- Resource requests and limits for the gateway\n  resources: {}\n  # -- Grace period to allow the gateway to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for gateway pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.gatewaySelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.gatewaySelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for gateway pods\n  nodeSelector: {}\n  # -- Tolerations for gateway pods\n  tolerations: []\n  # Gateway service configuration\n  service:\n    # -- Port of the gateway service\n    port: 80\n    # -- Type of the gateway service\n    type: ClusterIP\n    # -- ClusterIP of the gateway service\n    clusterIP: null\n    # -- Node port if service type is NodePort\n    nodePort: null\n    # -- Load balancer IPO address if service type is LoadBalancer\n    loadBalancerIP: null\n    # -- Annotations for the gateway service\n    annotations: {}\n    # -- Labels for gateway service\n    labels: {}\n  # Gateway ingress configuration\n  ingress:\n    # -- Specifies whether an ingress for the gateway should be created\n    enabled: false\n    # -- Ingress Class Name. MAY be required for Kubernetes versions \u003e= 1.18\n    # ingressClassName: nginx\n    # -- Annotations for the gateway ingress\n    annotations: {}\n    # -- Hosts configuration for the gateway ingress\n    hosts:\n      - host: gateway.tempo.example.com\n        paths:\n          - path: /\n            # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers\n            # pathType: Prefix\n    # -- TLS configuration for the gateway ingress\n    tls:\n      - secretName: tempo-gateway-tls\n        hosts:\n          - gateway.tempo.example.com\n  # Basic auth configuration\n  basicAuth:\n    # -- Enables basic authentication for the gateway\n    enabled: false\n    # -- The basic auth username for the gateway\n    username: null\n    # -- The basic auth password for the gateway\n    password: null\n    # -- Uses the specified username and password to compute a htpasswd using Sprig's `htpasswd` function.\n    # The value is templated using `tpl`. Override this to use a custom htpasswd, e.g. in case the default causes\n    # high CPU load.\n    htpasswd: \u003e-\n      {{ htpasswd (required \"'gateway.basicAuth.username' is required\" .Values.gateway.basicAuth.username) (required \"'gateway.basicAuth.password' is required\" .Values.gateway.basicAuth.password) }}\n    # -- Existing basic auth secret to use. Must contain '.htpasswd'\n    existingSecret: null\n  # Configures the readiness probe for the gateway\n  readinessProbe:\n    httpGet:\n      path: /\n      port: http\n    initialDelaySeconds: 15\n    timeoutSeconds: 1\n  nginxConfig:\n    # -- NGINX log format\n    logFormat: |-\n      main '$remote_addr - $remote_user [$time_local]  $status '\n              '\"$request\" $body_bytes_sent \"$http_referer\" '\n              '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n    # -- Allows appending custom configuration to the server block\n    serverSnippet: \"\"\n    # -- Allows appending custom configuration to the http block\n    httpSnippet: \"\"\n    # -- Config file contents for Nginx. Passed through the `tpl` function to allow templating\n    # @default -- See values.yaml\n    file: |\n      worker_processes  5;  ## Default: 1\n      error_log  /dev/stderr;\n      pid        /tmp/nginx.pid;\n      worker_rlimit_nofile 8192;\n\n      events {\n        worker_connections  4096;  ## Default: 1024\n      }\n\n      http {\n        client_body_temp_path /tmp/client_temp;\n        proxy_temp_path       /tmp/proxy_temp_path;\n        fastcgi_temp_path     /tmp/fastcgi_temp;\n        uwsgi_temp_path       /tmp/uwsgi_temp;\n        scgi_temp_path        /tmp/scgi_temp;\n\n        proxy_http_version    1.1;\n\n        default_type application/octet-stream;\n        log_format   {{ .Values.gateway.nginxConfig.logFormat }}\n\n        {{- if .Values.gateway.verboseLogging }}\n        access_log   /dev/stderr  main;\n        {{- else }}\n\n        map $status $loggable {\n          ~^[23]  0;\n          default 1;\n        }\n        access_log   /dev/stderr  main  if=$loggable;\n        {{- end }}\n\n        sendfile     on;\n        tcp_nopush   on;\n        resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};\n\n        {{- with .Values.gateway.nginxConfig.httpSnippet }}\n        {{ . | nindent 2 }}\n        {{- end }}\n\n        server {\n          listen             8080;\n\n          {{- if .Values.gateway.basicAuth.enabled }}\n          auth_basic           \"Tempo\";\n          auth_basic_user_file /etc/nginx/secrets/.htpasswd;\n          {{- end }}\n\n          location = / {\n            return 200 'OK';\n            auth_basic off;\n          }\n\n          location = /jaeger/api/traces {\n            proxy_pass       http://{{ include \"tempo.distributorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:14268/api/traces;\n          }\n\n          location = /zipkin/spans {\n            proxy_pass       http://{{ include \"tempo.distributorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:9411/spans;\n          }\n\n          location = /otlp/v1/traces {\n            proxy_pass       http://{{ include \"tempo.distributorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:4318/v1/traces;\n          }\n\n          location ^~ /api {\n            proxy_pass       http://{{ include \"tempo.queryFrontendFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /flush {\n            proxy_pass       http://{{ include \"tempo.ingesterFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /shutdown {\n            proxy_pass       http://{{ include \"tempo.ingesterFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /distributor/ring {\n            proxy_pass       http://{{ include \"tempo.distributorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /ingester/ring {\n            proxy_pass       http://{{ include \"tempo.distributorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /compactor/ring {\n            proxy_pass       http://{{ include \"tempo.compactorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          {{- with .Values.gateway.nginxConfig.serverSnippet }}\n          {{ . | nindent 4 }}\n          {{- end }}\n        }\n      }\n",
            "vars": null
          },
          "sensitive_attributes": []
        }
      ]
    },
    {
      "module": "module.test-module",
      "mode": "managed",
      "type": "helm_release",
      "name": "loki",
      "provider": "module.test-module.provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "loki-simple-scalable",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "loki-simple-scalable",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "2.6.1",
                "chart": "loki-simple-scalable",
                "name": "loki-simple-scalable",
                "namespace": "loki",
                "revision": 1,
                "values": "{\"enterprise\":{\"adminApi\":{\"enabled\":true},\"config\":\"{{- if .Values.enterprise.adminApi.enabled }}\\n{{- if or .Values.minio.enabled (eq .Values.loki.storage.type \\\"s3\\\") (eq .Values.loki.storage.type \\\"gcs\\\") }}\\nadmin_client:\\n  storage:\\n    s3:\\n      bucket_name: {{ .Values.loki.storage.bucketNames.admin }}\\n{{- end }}\\n{{- end }}\\nauth:\\n  type: {{ .Values.enterprise.adminApi.enabled | ternary \\\"enterprise\\\" \\\"trust\\\" }}\\nauth_enabled: {{ .Values.loki.auth_enabled }}\\ncluster_name: {{ .Release.Name }}\\nlicense:\\n  path: /etc/loki/license/license.jwt\\n\",\"enabled\":false,\"externalLicenseName\":null,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"docker.io\",\"repository\":\"grafana/enterprise-logs\",\"tag\":\"v1.4.0\"},\"license\":{\"contents\":\"NOTAVALIDLICENSE\"},\"nginxConfig\":{\"file\":\"worker_processes  5;  ## Default: 1\\nerror_log  /dev/stderr;\\npid        /tmp/nginx.pid;\\nworker_rlimit_nofile 8192;\\n\\nevents {\\n  worker_connections  4096;  ## Default: 1024\\n}\\n\\nhttp {\\n  client_body_temp_path /tmp/client_temp;\\n  proxy_temp_path       /tmp/proxy_temp_path;\\n  fastcgi_temp_path     /tmp/fastcgi_temp;\\n  uwsgi_temp_path       /tmp/uwsgi_temp;\\n  scgi_temp_path        /tmp/scgi_temp;\\n\\n  proxy_http_version    1.1;\\n\\n  default_type application/octet-stream;\\n  log_format   {{ .Values.gateway.nginxConfig.logFormat }}\\n\\n  {{- if .Values.gateway.verboseLogging }}\\n  access_log   /dev/stderr  main;\\n  {{- else }}\\n\\n  map $status $loggable {\\n    ~^[23]  0;\\n    default 1;\\n  }\\n  access_log   /dev/stderr  main  if=$loggable;\\n  {{- end }}\\n\\n  sendfile     on;\\n  tcp_nopush   on;\\n  resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};\\n\\n  {{- with .Values.gateway.nginxConfig.httpSnippet }}\\n  {{ . | nindent 2 }}\\n  {{- end }}\\n\\n  server {\\n    listen             8080;\\n\\n    {{- if .Values.gateway.basicAuth.enabled }}\\n    auth_basic           \\\"Loki\\\";\\n    auth_basic_user_file /etc/nginx/secrets/.htpasswd;\\n    {{- end }}\\n\\n    location = / {\\n      return 200 'OK';\\n      auth_basic off;\\n    }\\n\\n    location = /api/prom/push {\\n      proxy_pass       http://{{ include \\\"loki.writeFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location = /api/prom/tail {\\n      proxy_pass       http://{{ include \\\"loki.readFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n      proxy_set_header Upgrade $http_upgrade;\\n      proxy_set_header Connection \\\"upgrade\\\";\\n    }\\n\\n    location ~ /api/prom/.* {\\n      proxy_pass       http://{{ include \\\"loki.readFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location ~ /prometheus/api/v1/alerts.* {\\n      proxy_pass       http://{{ include \\\"loki.readFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location ~ /prometheus/api/v1/rules.* {\\n      proxy_pass       http://{{ include \\\"loki.readFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location = /loki/api/v1/push {\\n      proxy_pass       http://{{ include \\\"loki.writeFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location = /loki/api/v1/tail {\\n      proxy_pass       http://{{ include \\\"loki.readFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n      proxy_set_header Upgrade $http_upgrade;\\n      proxy_set_header Connection \\\"upgrade\\\";\\n    }\\n\\n    location ~ /loki/api/.* {\\n      proxy_pass       http://{{ include \\\"loki.readFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location ~ /admin/api/.* {\\n      proxy_pass       http://{{ include \\\"loki.writeFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location ~ /compactor/.* {\\n      proxy_pass       http://{{ include \\\"loki.readFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location ~ /distributor/.* {\\n      proxy_pass       http://{{ include \\\"loki.writeFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location ~ /ring {\\n      proxy_pass       http://{{ include \\\"loki.writeFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location ~ /ingester/.* {\\n      proxy_pass       http://{{ include \\\"loki.writeFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location ~ /ruler/.* {\\n      proxy_pass       http://{{ include \\\"loki.readFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location ~ /scheduler/.* {\\n      proxy_pass       http://{{ include \\\"loki.readFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    {{- with .Values.gateway.nginxConfig.serverSnippet }}\\n    {{ . | nindent 4 }}\\n    {{- end }}\\n  }\\n}\\n\"},\"tokengen\":{\"adminTokenSecret\":\"gel-admin-token\",\"annotations\":{},\"enabled\":true,\"env\":[],\"extraArgs\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"labels\":{},\"securityContext\":{\"fsGroup\":10001,\"runAsGroup\":10001,\"runAsNonRoot\":true,\"runAsUser\":10001}},\"useExternalLicense\":false,\"version\":\"v1.5.0\"},\"fullnameOverride\":null,\"gateway\":{\"affinity\":\"podAntiAffinity:\\n  requiredDuringSchedulingIgnoredDuringExecution:\\n    - labelSelector:\\n        matchLabels:\\n          {{- include \\\"loki.gatewaySelectorLabels\\\" . | nindent 10 }}\\n      topologyKey: kubernetes.io/hostname\\n\",\"autoscaling\":{\"enabled\":false,\"maxReplicas\":3,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":60,\"targetMemoryUtilizationPercentage\":null},\"basicAuth\":{\"enabled\":false,\"existingSecret\":null,\"htpasswd\":\"{{ htpasswd (required \\\"'gateway.basicAuth.username' is required\\\" .Values.gateway.basicAuth.username) (required \\\"'gateway.basicAuth.password' is required\\\" .Values.gateway.basicAuth.password) }}\",\"password\":null,\"username\":null},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"deploymentStrategy\":{\"type\":\"RollingUpdate\"},\"enabled\":true,\"extraArgs\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"docker.io\",\"repository\":\"nginxinc/nginx-unprivileged\",\"tag\":\"1.19-alpine\"},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[{\"host\":\"gateway.loki.example.com\",\"paths\":[{\"path\":\"/\"}]}],\"tls\":[{\"hosts\":[\"gateway.loki.example.com\"],\"secretName\":\"loki-gateway-tls\"}]},\"nginxConfig\":{\"file\":\"worker_processes  5;  ## Default: 1\\nerror_log  /dev/stderr;\\npid        /tmp/nginx.pid;\\nworker_rlimit_nofile 8192;\\n\\nevents {\\n  worker_connections  4096;  ## Default: 1024\\n}\\n\\nhttp {\\n  client_body_temp_path /tmp/client_temp;\\n  proxy_temp_path       /tmp/proxy_temp_path;\\n  fastcgi_temp_path     /tmp/fastcgi_temp;\\n  uwsgi_temp_path       /tmp/uwsgi_temp;\\n  scgi_temp_path        /tmp/scgi_temp;\\n\\n  proxy_http_version    1.1;\\n\\n  default_type application/octet-stream;\\n  log_format   {{ .Values.gateway.nginxConfig.logFormat }}\\n\\n  {{- if .Values.gateway.verboseLogging }}\\n  access_log   /dev/stderr  main;\\n  {{- else }}\\n\\n  map $status $loggable {\\n    ~^[23]  0;\\n    default 1;\\n  }\\n  access_log   /dev/stderr  main  if=$loggable;\\n  {{- end }}\\n\\n  sendfile     on;\\n  tcp_nopush   on;\\n  resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};\\n\\n  {{- with .Values.gateway.nginxConfig.httpSnippet }}\\n  {{ . | nindent 2 }}\\n  {{- end }}\\n\\n  server {\\n    listen             8080;\\n\\n    {{- if .Values.gateway.basicAuth.enabled }}\\n    auth_basic           \\\"Loki\\\";\\n    auth_basic_user_file /etc/nginx/secrets/.htpasswd;\\n    {{- end }}\\n\\n    location = / {\\n      return 200 'OK';\\n      auth_basic off;\\n    }\\n\\n    location = /api/prom/push {\\n      proxy_pass       http://{{ include \\\"loki.writeFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location = /api/prom/tail {\\n      proxy_pass       http://{{ include \\\"loki.readFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n      proxy_set_header Upgrade $http_upgrade;\\n      proxy_set_header Connection \\\"upgrade\\\";\\n    }\\n\\n    location ~ /api/prom/.* {\\n      proxy_pass       http://{{ include \\\"loki.readFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location = /loki/api/v1/push {\\n      proxy_pass       http://{{ include \\\"loki.writeFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location = /loki/api/v1/tail {\\n      proxy_pass       http://{{ include \\\"loki.readFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n      proxy_set_header Upgrade $http_upgrade;\\n      proxy_set_header Connection \\\"upgrade\\\";\\n    }\\n\\n    location ~ /loki/api/.* {\\n      proxy_pass       http://{{ include \\\"loki.readFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    {{- with .Values.gateway.nginxConfig.serverSnippet }}\\n    {{ . | nindent 4 }}\\n    {{- end }}\\n  }\\n}\\n\",\"httpSnippet\":\"\",\"logFormat\":\"main '$remote_addr - $remote_user [$time_local]  $status '\\n        '\\\"$request\\\" $body_bytes_sent \\\"$http_referer\\\" '\\n        '\\\"$http_user_agent\\\" \\\"$http_x_forwarded_for\\\"';\",\"serverSnippet\":\"\"},\"nodeSelector\":{},\"podAnnotations\":{},\"podSecurityContext\":{\"fsGroup\":101,\"runAsGroup\":101,\"runAsNonRoot\":true,\"runAsUser\":101},\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/\",\"port\":\"http\"},\"initialDelaySeconds\":15,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{},\"clusterIP\":null,\"labels\":{},\"loadBalancerIP\":null,\"nodePort\":null,\"port\":80,\"type\":\"ClusterIP\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"verboseLogging\":true},\"global\":{\"clusterDomain\":\"cluster.local\",\"dnsNamespace\":\"kube-system\",\"dnsService\":\"kube-dns\",\"image\":{\"registry\":null},\"priorityClassName\":null},\"imagePullSecrets\":[],\"loki\":{\"auth_enabled\":false,\"commonConfig\":{\"path_prefix\":\"/var/loki\",\"replication_factor\":3},\"config\":\"{{- if .Values.enterprise.enabled}}\\n{{- tpl .Values.enterprise.config . }}\\n{{- else }}\\nauth_enabled: {{ .Values.loki.auth_enabled }}\\n{{- end }}\\n\\nserver:\\n  http_listen_port: 3100\\n  grpc_listen_port: 9095\\n\\nmemberlist:\\n  join_members:\\n    - {{ include \\\"loki.name\\\" . }}-memberlist\\n\\n{{- if .Values.loki.commonConfig}}\\ncommon:\\n{{- toYaml .Values.loki.commonConfig | nindent 2}}\\n  storage:\\n  {{- include \\\"loki.commonStorageConfig\\\" . | nindent 4}}\\n{{- end}}\\n\\nlimits_config:\\n  enforce_metric_name: false\\n  reject_old_samples: true\\n  reject_old_samples_max_age: 168h\\n  max_cache_freshness_per_query: 10m\\n  split_queries_by_interval: 15m\\n\\n{{- with .Values.loki.memcached.chunk_cache }}\\n{{- if and .enabled .host }}\\nchunk_store_config:\\n  chunk_cache_config:\\n    memcached:\\n      batch_size: {{ .batch_size }}\\n      parallelism: {{ .parallelism }}\\n    memcached_client:\\n      host: {{ .host }}\\n      service: {{ .service }}\\n{{- end }}\\n{{- end }}\\n\\n{{- if .Values.loki.schemaConfig}}\\nschema_config:\\n{{- toYaml .Values.loki.schemaConfig | nindent 2}}\\n{{- else }}\\nschema_config:\\n  configs:\\n    - from: 2022-01-11\\n      store: boltdb-shipper\\n      {{- if eq .Values.loki.storage.type \\\"s3\\\" }}\\n      object_store: s3\\n      {{- else if eq .Values.loki.storage.type \\\"gcs\\\" }}\\n      object_store: gcs\\n      {{- else }}\\n      object_store: filesystem\\n      {{- end }}\\n      schema: v12\\n      index:\\n        prefix: loki_index_\\n        period: 24h\\n{{- end }}\\n\\n{{- if or .Values.minio.enabled (eq .Values.loki.storage.type \\\"s3\\\") (eq .Values.loki.storage.type \\\"gcs\\\") }}\\nruler:\\n  storage:\\n  {{- include \\\"loki.rulerStorageConfig\\\" . | nindent 4}}\\n{{- end -}}\\n\\n{{- with .Values.loki.memcached.results_cache }}\\nquery_range:\\n  align_queries_with_step: true\\n  {{- if and .enabled .host }}\\n  cache_results: {{ .enabled }}\\n  results_cache:\\n    cache:\\n      default_validity: {{ .default_validity }}\\n      memcached_client:\\n        host: {{ .host }}\\n        service: {{ .service }}\\n        timeout: {{ .timeout }}\\n  {{- end }}\\n{{- end }}\\n\\n{{- with .Values.loki.storage_config }}\\nstorage_config:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\\n{{- with .Values.loki.query_scheduler }}\\nquery_scheduler:\\n  {{- tpl (. | toYaml) $ | nindent 4 }}\\n{{- end }}\\n\",\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"existingSecretForConfig\":\"\",\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"docker.io\",\"repository\":\"grafana/loki\",\"tag\":null},\"memcached\":{\"chunk_cache\":{\"batch_size\":256,\"enabled\":false,\"host\":\"\",\"parallelism\":10,\"service\":\"memcached-client\"},\"results_cache\":{\"default_validity\":\"12h\",\"enabled\":false,\"host\":\"\",\"service\":\"memcached-client\",\"timeout\":\"500ms\"}},\"podAnnotations\":{},\"podSecurityContext\":{\"fsGroup\":10001,\"runAsGroup\":10001,\"runAsNonRoot\":true,\"runAsUser\":10001},\"query_scheduler\":{},\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":30,\"timeoutSeconds\":1},\"revisionHistoryLimit\":10,\"schemaConfig\":{},\"storage\":{\"bucketNames\":{\"admin\":\"admin\",\"chunks\":\"chunks\",\"ruler\":\"ruler\"},\"gcs\":{\"chunkBufferSize\":0,\"enableHttp2\":true,\"requestTimeout\":\"0s\"},\"local\":{\"chunks_directory\":\"/var/loki/chunks\",\"rules_directory\":\"/var/loki/rules\"},\"s3\":{\"accessKeyId\":null,\"endpoint\":null,\"insecure\":false,\"region\":null,\"s3\":null,\"s3ForcePathStyle\":false,\"secretAccessKey\":null},\"type\":\"s3\"},\"storage_config\":{\"hedging\":{\"at\":\"250ms\",\"max_per_second\":20,\"up_to\":3}},\"structuredConfig\":{}},\"minio\":{\"buckets\":[{\"name\":\"chunks\",\"policy\":\"none\",\"purge\":false},{\"name\":\"ruler\",\"policy\":\"none\",\"purge\":false},{\"name\":\"admin\",\"policy\":\"none\",\"purge\":false}],\"enabled\":true,\"persistence\":{\"size\":\"5Gi\"},\"resources\":{\"requests\":{\"cpu\":\"100m\",\"memory\":\"128Mi\"}}},\"monitoring\":{\"alerts\":{\"annotations\":{},\"enabled\":true,\"labels\":{},\"namespace\":null},\"dashboards\":{\"annotations\":{},\"enabled\":true,\"labels\":{},\"namespace\":null},\"rules\":{\"additionalGroups\":[],\"alerting\":true,\"annotations\":{},\"enabled\":true,\"labels\":{},\"namespace\":null},\"selfMonitoring\":{\"enabled\":false,\"grafanaAgent\":{\"annotations\":{},\"enableConfigReadAPI\":false,\"installOperator\":false,\"labels\":{},\"namespace\":null},\"logsInstance\":{\"annotations\":{},\"labels\":{},\"namespace\":null},\"podLogs\":{\"annotations\":{},\"labels\":{},\"namespace\":null,\"relabelings\":[]}},\"serviceMonitor\":{\"annotations\":{},\"enabled\":true,\"interval\":null,\"labels\":{},\"namespace\":null,\"namespaceSelector\":{},\"relabelings\":[],\"scheme\":\"http\",\"scrapeTimeout\":null,\"tlsConfig\":null}},\"nameOverride\":null,\"networkPolicy\":{\"alertmanager\":{\"namespaceSelector\":{},\"podSelector\":{},\"port\":9093},\"discovery\":{\"namespaceSelector\":{},\"podSelector\":{},\"port\":null},\"enabled\":false,\"externalStorage\":{\"cidrs\":[],\"ports\":[]},\"ingress\":{\"namespaceSelector\":{},\"podSelector\":{}},\"metrics\":{\"cidrs\":[],\"namespaceSelector\":{},\"podSelector\":{}}},\"rbac\":{\"pspEnabled\":false,\"sccEnabled\":false},\"read\":{\"affinity\":\"podAntiAffinity:\\n  requiredDuringSchedulingIgnoredDuringExecution:\\n    - labelSelector:\\n        matchLabels:\\n          {{- include \\\"loki.readSelectorLabels\\\" . | nindent 10 }}\\n      topologyKey: kubernetes.io/hostname\\n\",\"autoscaling\":{\"enabled\":false,\"maxReplicas\":3,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":60,\"targetMemoryUtilizationPercentage\":null},\"extraArgs\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"nodeSelector\":{},\"persistence\":{\"size\":\"10Gi\",\"storageClass\":null},\"podAnnotations\":{},\"priorityClassName\":null,\"replicas\":3,\"resources\":{},\"selectorLabels\":{},\"serviceLabels\":{},\"terminationGracePeriodSeconds\":30,\"tolerations\":[]},\"serviceAccount\":{\"annotations\":{},\"automountServiceAccountToken\":true,\"create\":true,\"imagePullSecrets\":[],\"name\":null},\"write\":{\"affinity\":\"podAntiAffinity:\\n  requiredDuringSchedulingIgnoredDuringExecution:\\n    - labelSelector:\\n        matchLabels:\\n          {{- include \\\"loki.writeSelectorLabels\\\" . | nindent 10 }}\\n      topologyKey: kubernetes.io/hostname\\n\",\"extraArgs\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"nodeSelector\":{},\"persistence\":{\"size\":\"10Gi\",\"storageClass\":null},\"podAnnotations\":{},\"priorityClassName\":null,\"replicas\":3,\"resources\":{},\"selectorLabels\":{},\"serviceLabels\":{},\"terminationGracePeriodSeconds\":300,\"tolerations\":[]}}",
                "version": "1.8.5"
              }
            ],
            "name": "loki-simple-scalable",
            "namespace": "loki",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://grafana.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "---\nglobal:\n  image:\n    # -- Overrides the Docker registry globally for all images\n    registry: null\n  # -- Overrides the priorityClassName for all pods\n  priorityClassName: null\n  # -- configures cluster domain (\"cluster.local\" by default)\n  clusterDomain: \"cluster.local\"\n  # -- configures DNS service name\n  dnsService: \"kube-dns\"\n  # -- configures DNS service namespace\n  dnsNamespace: \"kube-system\"\n\n# -- Overrides the chart's name\nnameOverride: null\n\n# -- Overrides the chart's computed fullname\nfullnameOverride: null\n\n# -- Image pull secrets for Docker images\nimagePullSecrets: []\n\nloki:\n  # Configures the readiness probe for all of the Loki pods\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http-metrics\n    initialDelaySeconds: 30\n    timeoutSeconds: 1\n  image:\n    # -- The Docker registry\n    registry: docker.io\n    # -- Docker image repository\n    repository: grafana/loki\n    # -- Overrides the image tag whose default is the chart's appVersion\n    tag: null\n    # -- Docker image pull policy\n    pullPolicy: IfNotPresent\n  # -- Common annotations for all pods\n  podAnnotations: {}\n  # -- The number of old ReplicaSets to retain to allow rollback\n  revisionHistoryLimit: 10\n  # -- The SecurityContext for Loki pods\n  podSecurityContext:\n    fsGroup: 10001\n    runAsGroup: 10001\n    runAsNonRoot: true\n    runAsUser: 10001\n  # -- The SecurityContext for Loki containers\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop:\n        - ALL\n    allowPrivilegeEscalation: false\n  # -- Specify an existing secret containing loki configuration. If non-empty, overrides `loki.config`\n  existingSecretForConfig: \"\"\n  # -- Config file contents for Loki\n  # @default -- See values.yaml\n  config: |\n    {{- if .Values.enterprise.enabled}}\n    {{- tpl .Values.enterprise.config . }}\n    {{- else }}\n    auth_enabled: {{ .Values.loki.auth_enabled }}\n    {{- end }}\n\n    server:\n      http_listen_port: 3100\n      grpc_listen_port: 9095\n\n    memberlist:\n      join_members:\n        - {{ include \"loki.name\" . }}-memberlist\n\n    {{- if .Values.loki.commonConfig}}\n    common:\n    {{- toYaml .Values.loki.commonConfig | nindent 2}}\n      storage:\n      {{- include \"loki.commonStorageConfig\" . | nindent 4}}\n    {{- end}}\n\n    limits_config:\n      enforce_metric_name: false\n      reject_old_samples: true\n      reject_old_samples_max_age: 168h\n      max_cache_freshness_per_query: 10m\n      split_queries_by_interval: 15m\n\n    {{- with .Values.loki.memcached.chunk_cache }}\n    {{- if and .enabled .host }}\n    chunk_store_config:\n      chunk_cache_config:\n        memcached:\n          batch_size: {{ .batch_size }}\n          parallelism: {{ .parallelism }}\n        memcached_client:\n          host: {{ .host }}\n          service: {{ .service }}\n    {{- end }}\n    {{- end }}\n\n    {{- if .Values.loki.schemaConfig}}\n    schema_config:\n    {{- toYaml .Values.loki.schemaConfig | nindent 2}}\n    {{- else }}\n    schema_config:\n      configs:\n        - from: 2022-01-11\n          store: boltdb-shipper\n          {{- if eq .Values.loki.storage.type \"s3\" }}\n          object_store: s3\n          {{- else if eq .Values.loki.storage.type \"gcs\" }}\n          object_store: gcs\n          {{- else }}\n          object_store: filesystem\n          {{- end }}\n          schema: v12\n          index:\n            prefix: loki_index_\n            period: 24h\n    {{- end }}\n\n    {{- if or .Values.minio.enabled (eq .Values.loki.storage.type \"s3\") (eq .Values.loki.storage.type \"gcs\") }}\n    ruler:\n      storage:\n      {{- include \"loki.rulerStorageConfig\" . | nindent 4}}\n    {{- end -}}\n\n    {{- with .Values.loki.memcached.results_cache }}\n    query_range:\n      align_queries_with_step: true\n      {{- if and .enabled .host }}\n      cache_results: {{ .enabled }}\n      results_cache:\n        cache:\n          default_validity: {{ .default_validity }}\n          memcached_client:\n            host: {{ .host }}\n            service: {{ .service }}\n            timeout: {{ .timeout }}\n      {{- end }}\n    {{- end }}\n\n    {{- with .Values.loki.storage_config }}\n    storage_config:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n    {{- with .Values.loki.query_scheduler }}\n    query_scheduler:\n      {{- tpl (. | toYaml) $ | nindent 4 }}\n    {{- end }}\n\n  # Should authentication be enabled\n  auth_enabled: false\n\n  # -- Check https://grafana.com/docs/loki/latest/configuration/#common_config for more info on how to provide a common configuration\n  commonConfig:\n    path_prefix: /var/loki\n    replication_factor: 3\n\n  storage:\n    bucketNames:\n      chunks: chunks\n      ruler: ruler\n      admin: admin\n    type: s3\n    s3:\n      s3: null\n      endpoint: null\n      region: null\n      secretAccessKey: null\n      accessKeyId: null\n      s3ForcePathStyle: false\n      insecure: false\n    gcs:\n      chunkBufferSize: 0\n      requestTimeout: \"0s\"\n      enableHttp2: true\n    local:\n      chunks_directory: /var/loki/chunks\n      rules_directory: /var/loki/rules\n\n  # -- Configure memcached as an external cache for chunk and results cache. Disabled by default\n  # must enable and specify a host for each cache you would like to use.\n  memcached:\n    chunk_cache:\n      enabled: false\n      host: \"\"\n      service: \"memcached-client\"\n      batch_size: 256\n      parallelism: 10\n    results_cache:\n      enabled: false\n      host: \"\"\n      service: \"memcached-client\"\n      timeout: \"500ms\"\n      default_validity: \"12h\"\n\n  # -- Check https://grafana.com/docs/loki/latest/configuration/#schema_config for more info on how to configure schemas\n  schemaConfig: {}\n\n  # -- Structured loki configuration, takes precedence over `loki.config`, `loki.schemaConfig`, `loki.storageConfig`\n  structuredConfig: {}\n\n  # -- Additional query scheduler config\n  query_scheduler: {}\n\n  # -- Additional storage config\n  storage_config:\n    hedging:\n      at: \"250ms\"\n      max_per_second: 20\n      up_to: 3\n\nenterprise:\n  # Enable enterprise features, license must be provided\n  enabled: false\n\n  # Default verion of GEL to deploy\n  version: v1.5.0\n\n  # -- Grafana Enterprise Logs license\n  # In order to use Grafana Enterprise Logs features, you will need to provide\n  # the contents of your Grafana Enterprise Logs license, either by providing the\n  # contents of the license.jwt, or the name Kubernetes Secret that contains your\n  # license.jwt.\n  # To set the license contents, use the flag `--set-file 'license.contents=./license.jwt'`\n  license:\n    contents: \"NOTAVALIDLICENSE\"\n\n  # -- Set to true when providing an external license\n  useExternalLicense: false\n\n  # -- Name of external licesne secret to use\n  externalLicenseName: null\n\n  # -- If enabled, the correct admin_client storage will be configured. If disabled while running enterprise,\n  # make sure auth is set to `type: trust`, or that `auth_enabled` is set to `false`.\n  adminApi:\n    enabled: true\n\n  # enterprise specific sections of the config.yaml file\n  config: |\n    {{- if .Values.enterprise.adminApi.enabled }}\n    {{- if or .Values.minio.enabled (eq .Values.loki.storage.type \"s3\") (eq .Values.loki.storage.type \"gcs\") }}\n    admin_client:\n      storage:\n        s3:\n          bucket_name: {{ .Values.loki.storage.bucketNames.admin }}\n    {{- end }}\n    {{- end }}\n    auth:\n      type: {{ .Values.enterprise.adminApi.enabled | ternary \"enterprise\" \"trust\" }}\n    auth_enabled: {{ .Values.loki.auth_enabled }}\n    cluster_name: {{ .Release.Name }}\n    license:\n      path: /etc/loki/license/license.jwt\n\n  image:\n    # -- The Docker registry\n    registry: docker.io\n    # -- Docker image repository\n    repository: grafana/enterprise-logs\n    # -- Overrides the image tag whose default is the chart's appVersion\n    tag: v1.4.0\n    # -- Docker image pull policy\n    pullPolicy: IfNotPresent\n\n  # -- Configuration for `tokengen` target\n  tokengen:\n    # -- Whether the job should be part of the deployment\n    enabled: true\n    # -- Name of the secret to store the admin token in\n    adminTokenSecret: \"gel-admin-token\"\n    # -- Additional CLI arguments for the `tokengen` target\n    extraArgs: []\n    # -- Additional Kubernetes environment\n    env: []\n    # -- Additional labels for the `tokengen` Job\n    labels: {}\n    # -- Additional annotations for the `tokengen` Job\n    annotations: {}\n    # -- Additional volumes for Pods\n    extraVolumes: []\n    # -- Additional volume mounts for Pods\n    extraVolumeMounts: []\n    # -- Run containers as user `enterprise-logs(uid=10001)`\n    securityContext:\n      runAsNonRoot: true\n      runAsGroup: 10001\n      runAsUser: 10001\n      fsGroup: 10001\n\n  nginxConfig:\n    file: |\n      worker_processes  5;  ## Default: 1\n      error_log  /dev/stderr;\n      pid        /tmp/nginx.pid;\n      worker_rlimit_nofile 8192;\n\n      events {\n        worker_connections  4096;  ## Default: 1024\n      }\n\n      http {\n        client_body_temp_path /tmp/client_temp;\n        proxy_temp_path       /tmp/proxy_temp_path;\n        fastcgi_temp_path     /tmp/fastcgi_temp;\n        uwsgi_temp_path       /tmp/uwsgi_temp;\n        scgi_temp_path        /tmp/scgi_temp;\n\n        proxy_http_version    1.1;\n\n        default_type application/octet-stream;\n        log_format   {{ .Values.gateway.nginxConfig.logFormat }}\n\n        {{- if .Values.gateway.verboseLogging }}\n        access_log   /dev/stderr  main;\n        {{- else }}\n\n        map $status $loggable {\n          ~^[23]  0;\n          default 1;\n        }\n        access_log   /dev/stderr  main  if=$loggable;\n        {{- end }}\n\n        sendfile     on;\n        tcp_nopush   on;\n        resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};\n\n        {{- with .Values.gateway.nginxConfig.httpSnippet }}\n        {{ . | nindent 2 }}\n        {{- end }}\n\n        server {\n          listen             8080;\n\n          {{- if .Values.gateway.basicAuth.enabled }}\n          auth_basic           \"Loki\";\n          auth_basic_user_file /etc/nginx/secrets/.htpasswd;\n          {{- end }}\n\n          location = / {\n            return 200 'OK';\n            auth_basic off;\n          }\n\n          location = /api/prom/push {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /api/prom/tail {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection \"upgrade\";\n          }\n\n          location ~ /api/prom/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /prometheus/api/v1/alerts.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /prometheus/api/v1/rules.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /loki/api/v1/push {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /loki/api/v1/tail {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection \"upgrade\";\n          }\n\n          location ~ /loki/api/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /admin/api/.* {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /compactor/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /distributor/.* {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /ring {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /ingester/.* {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /ruler/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location ~ /scheduler/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          {{- with .Values.gateway.nginxConfig.serverSnippet }}\n          {{ . | nindent 4 }}\n          {{- end }}\n        }\n      }\n\nserviceAccount:\n  # -- Specifies whether a ServiceAccount should be created\n  create: true\n  # -- The name of the ServiceAccount to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: null\n  # -- Image pull secrets for the service account\n  imagePullSecrets: []\n  # -- Annotations for the service account\n  annotations: {}\n  # -- Set this toggle to false to opt out of automounting API credentials for the service account\n  automountServiceAccountToken: true\n\n# RBAC configuration\nrbac:\n  # -- If pspEnabled true, a PodSecurityPolicy is created for K8s that use psp.\n  pspEnabled: false\n  # -- For OpenShift set pspEnabled to 'false' and sccEnabled to 'true' to use the SecurityContextConstraints.\n  sccEnabled: false\n\n# Monitoring section determines which monitoring features to enable\nmonitoring:\n  # Dashboards for monitoring Loki\n  dashboards:\n    # -- If enabled, create configmap with dashboards for monitoring Loki\n    enabled: true\n    # -- Alternative namespace to create dashboards ConfigMap in\n    namespace: null\n    # -- Additional annotations for the dashboards ConfigMap\n    annotations: {}\n    # -- Additional labels for the dashboards ConfigMap\n    labels: {}\n\n  # Recording rules for monitoring Loki, required for some dashboards\n  rules:\n    # -- If enabled, create PrometheusRule resource with Loki recording rules\n    enabled: true\n    # -- Include alerting rules\n    alerting: true\n    # -- Alternative namespace to create recording rules PrometheusRule resource in\n    namespace: null\n    # -- Additional annotations for the rules PrometheusRule resource\n    annotations: {}\n    # -- Additional labels for the rules PrometheusRule resource\n    labels: {}\n    # -- Additional groups to add to the rules file\n    additionalGroups: []\n    # - name: additional-loki-rules\n    #   rules:\n    #     - record: job:loki_request_duration_seconds_bucket:sum_rate\n    #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job)\n    #     - record: job_route:loki_request_duration_seconds_bucket:sum_rate\n    #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job, route)\n    #     - record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate\n    #       expr: sum(rate(container_cpu_usage_seconds_total[1m])) by (node, namespace, pod, container)\n\n  # Alerting rules for monitoring Loki\n  alerts:\n    # -- If enabled, create PrometheusRule resource with Loki alerting rules\n    enabled: true\n    # -- Alternative namespace to create alerting rules PrometheusRule resource in\n    namespace: null\n    # -- Additional annotations for the alerts PrometheusRule resource\n    annotations: {}\n    # -- Additional labels for the alerts PrometheusRule resource\n    labels: {}\n\n  # ServiceMonitor configuration\n  serviceMonitor:\n    # -- If enabled, ServiceMonitor resources for Prometheus Operator are created\n    enabled: true\n    # -- Alternative namespace for ServiceMonitor resources\n    namespace: null\n    # -- Namespace selector for ServiceMonitor resources\n    namespaceSelector: {}\n    # -- ServiceMonitor annotations\n    annotations: {}\n    # -- Additional ServiceMonitor labels\n    labels: {}\n    # -- ServiceMonitor scrape interval\n    interval: null\n    # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)\n    scrapeTimeout: null\n    # -- ServiceMonitor relabel configs to apply to samples before scraping\n    # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n    relabelings: []\n    # -- ServiceMonitor will use http by default, but you can pick https as well\n    scheme: http\n    # -- ServiceMonitor will use these tlsConfig settings to make the health check requests\n    tlsConfig: null\n\n  # Self monitoring determines whether Loki should scrape it's own logs.\n  # This feature currently relies on the Grafana Agent Operator being installed,\n  # which is installed by default using the grafana-agent-operator sub-chart.\n  # It will create custom resources for GrafanaAgent, LogsInstance, and PodLogs to configure\n  # scrape configs to scrape it's own logs with the labels expected by the included dashboards.\n  selfMonitoring:\n    enabled: false\n\n    # Grafana Agent configuration\n    grafanaAgent:\n      # -- Controls whether to install the Grafana Agent Operator and its CRDs.\n      # Note that helm will not install CRDs if this flag is enabled during an upgrade.\n      # In that case install the CRDs manually from https://github.com/grafana/agent/tree/main/production/operator/crds\n      installOperator: false\n      # -- Alternative namespace for Grafana Agent resources\n      namespace: null\n      # -- Grafana Agent annotations\n      annotations: {}\n      # -- Additional Grafana Agent labels\n      labels: {}\n      # -- Enable the config read api on port 8080 of the agent\n      enableConfigReadAPI: false\n\n    # PodLogs configuration\n    podLogs:\n      # -- Alternative namespace for PodLogs resources\n      namespace: null\n      # -- PodLogs annotations\n      annotations: {}\n      # -- Additional PodLogs labels\n      labels: {}\n      # -- PodLogs relabel configs to apply to samples before scraping\n      # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n      relabelings: []\n\n    # LogsInstance configuration\n    logsInstance:\n      # -- Alternative namespace for LogsInstance resources\n      namespace: null\n      # -- LogsInstance annotations\n      annotations: {}\n      # -- Additional LogsInstance labels\n      labels: {}\n\n# Configuration for the write\nwrite:\n  # -- Number of replicas for the write\n  replicas: 3\n  image:\n    # -- The Docker registry for the write image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the write image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the write image. Overrides `loki.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for write pods\n  priorityClassName: null\n  # -- Annotations for write pods\n  podAnnotations: {}\n  # -- Additional selector labels for each `write` pod\n  selectorLabels: {}\n  # -- Labels for ingestor service\n  serviceLabels: {}\n  # -- Additional CLI args for the write\n  extraArgs: []\n  # -- Environment variables to add to the write pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the write pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the write pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the write pods\n  extraVolumes: []\n  # -- Resource requests and limits for the write\n  resources: {}\n  # -- Grace period to allow the write to shutdown before it is killed. Especially for the ingestor,\n  # this must be increased. It must be long enough so writes can be gracefully shutdown flushing/transferring\n  # all data and to successfully leave the member ring on shutdown.\n  terminationGracePeriodSeconds: 300\n  # -- Affinity for write pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"loki.writeSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n  # -- Node selector for write pods\n  nodeSelector: {}\n  # -- Tolerations for write pods\n  tolerations: []\n  persistence:\n    # -- Size of persistent disk\n    size: 10Gi\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n\n# Configuration for the read node(s)\nread:\n  # -- Number of replicas for the read\n  replicas: 3\n  autoscaling:\n    # -- Enable autoscaling for the read, this is only used if `queryIndex.enabled: true`\n    enabled: false\n    # -- Minimum autoscaling replicas for the read\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the read\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the read\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the read\n    targetMemoryUtilizationPercentage:\n  image:\n    # -- The Docker registry for the read image. Overrides `loki.image.registry`\n    registry: null\n    # -- Docker image repository for the read image. Overrides `loki.image.repository`\n    repository: null\n    # -- Docker image tag for the read image. Overrides `loki.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for read pods\n  priorityClassName: null\n  # -- Annotations for read pods\n  podAnnotations: {}\n  # -- Additional selecto labels for each `read` pod\n  selectorLabels: {}\n  # -- Labels for read service\n  serviceLabels: {}\n  # -- Additional CLI args for the read\n  extraArgs: []\n  # -- Environment variables to add to the read pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the read pods\n  extraEnvFrom: []\n  # -- Volume mounts to add to the read pods\n  extraVolumeMounts: []\n  # -- Volumes to add to the read pods\n  extraVolumes: []\n  # -- Resource requests and limits for the read\n  resources: {}\n  # -- Grace period to allow the read to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for read pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"loki.readSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n  # -- Node selector for read pods\n  nodeSelector: {}\n  # -- Tolerations for read pods\n  tolerations: []\n  persistence:\n    # -- Size of persistent disk\n    size: 10Gi\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n\n# Configuration for the gateway\ngateway:\n  # -- Specifies whether the gateway should be enabled\n  enabled: true\n  # -- Number of replicas for the gateway\n  replicas: 1\n  # -- Enable logging of 2xx and 3xx HTTP requests\n  verboseLogging: true\n  autoscaling:\n    # -- Enable autoscaling for the gateway\n    enabled: false\n    # -- Minimum autoscaling replicas for the gateway\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the gateway\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the gateway\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the gateway\n    targetMemoryUtilizationPercentage:\n  # -- See `kubectl explain deployment.spec.strategy` for more\n  # -- ref: https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#strategy\n  deploymentStrategy:\n    type: RollingUpdate\n  image:\n    # -- The Docker registry for the gateway image\n    registry: docker.io\n    # -- The gateway image repository\n    repository: nginxinc/nginx-unprivileged\n    # -- The gateway image tag\n    tag: 1.19-alpine\n    # -- The gateway image pull policy\n    pullPolicy: IfNotPresent\n  # -- The name of the PriorityClass for gateway pods\n  priorityClassName: null\n  # -- Annotations for gateway pods\n  podAnnotations: {}\n  # -- Additional CLI args for the gateway\n  extraArgs: []\n  # -- Environment variables to add to the gateway pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the gateway pods\n  extraEnvFrom: []\n  # -- Volumes to add to the gateway pods\n  extraVolumes: []\n  # -- Volume mounts to add to the gateway pods\n  extraVolumeMounts: []\n  # -- The SecurityContext for gateway containers\n  podSecurityContext:\n    fsGroup: 101\n    runAsGroup: 101\n    runAsNonRoot: true\n    runAsUser: 101\n  # -- The SecurityContext for gateway containers\n  containerSecurityContext:\n    readOnlyRootFilesystem: true\n    capabilities:\n      drop:\n        - ALL\n    allowPrivilegeEscalation: false\n  # -- Resource requests and limits for the gateway\n  resources: {}\n  # -- Grace period to allow the gateway to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for gateway pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"loki.gatewaySelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n  # -- Node selector for gateway pods\n  nodeSelector: {}\n  # -- Tolerations for gateway pods\n  tolerations: []\n  # Gateway service configuration\n  service:\n    # -- Port of the gateway service\n    port: 80\n    # -- Type of the gateway service\n    type: ClusterIP\n    # -- ClusterIP of the gateway service\n    clusterIP: null\n    # -- (int) Node port if service type is NodePort\n    nodePort: null\n    # -- Load balancer IPO address if service type is LoadBalancer\n    loadBalancerIP: null\n    # -- Annotations for the gateway service\n    annotations: {}\n    # -- Labels for gateway service\n    labels: {}\n  # Gateway ingress configuration\n  ingress:\n    # -- Specifies whether an ingress for the gateway should be created\n    enabled: false\n    # -- Ingress Class Name. MAY be required for Kubernetes versions \u003e= 1.18\n    # ingressClassName: nginx\n    # -- Annotations for the gateway ingress\n    annotations: {}\n    # -- Hosts configuration for the gateway ingress\n    hosts:\n      - host: gateway.loki.example.com\n        paths:\n          - path: /\n            # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers\n            # pathType: Prefix\n    # -- TLS configuration for the gateway ingress\n    tls:\n      - secretName: loki-gateway-tls\n        hosts:\n          - gateway.loki.example.com\n  # Basic auth configuration\n  basicAuth:\n    # -- Enables basic authentication for the gateway\n    enabled: false\n    # -- The basic auth username for the gateway\n    username: null\n    # -- The basic auth password for the gateway\n    password: null\n    # -- Uses the specified username and password to compute a htpasswd using Sprig's `htpasswd` function.\n    # The value is templated using `tpl`. Override this to use a custom htpasswd, e.g. in case the default causes\n    # high CPU load.\n    htpasswd: \u003e-\n      {{ htpasswd (required \"'gateway.basicAuth.username' is required\" .Values.gateway.basicAuth.username) (required \"'gateway.basicAuth.password' is required\" .Values.gateway.basicAuth.password) }}\n\n    # -- Existing basic auth secret to use. Must contain '.htpasswd'\n    existingSecret: null\n  # Configures the readiness probe for the gateway\n  readinessProbe:\n    httpGet:\n      path: /\n      port: http\n    initialDelaySeconds: 15\n    timeoutSeconds: 1\n  nginxConfig:\n    # -- NGINX log format\n    logFormat: |-\n      main '$remote_addr - $remote_user [$time_local]  $status '\n              '\"$request\" $body_bytes_sent \"$http_referer\" '\n              '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n    # -- Allows appending custom configuration to the server block\n    serverSnippet: \"\"\n    # -- Allows appending custom configuration to the http block\n    httpSnippet: \"\"\n    # -- Config file contents for Nginx. Passed through the `tpl` function to allow templating\n    # @default -- See values.yaml\n    file: |\n      worker_processes  5;  ## Default: 1\n      error_log  /dev/stderr;\n      pid        /tmp/nginx.pid;\n      worker_rlimit_nofile 8192;\n\n      events {\n        worker_connections  4096;  ## Default: 1024\n      }\n\n      http {\n        client_body_temp_path /tmp/client_temp;\n        proxy_temp_path       /tmp/proxy_temp_path;\n        fastcgi_temp_path     /tmp/fastcgi_temp;\n        uwsgi_temp_path       /tmp/uwsgi_temp;\n        scgi_temp_path        /tmp/scgi_temp;\n\n        proxy_http_version    1.1;\n\n        default_type application/octet-stream;\n        log_format   {{ .Values.gateway.nginxConfig.logFormat }}\n\n        {{- if .Values.gateway.verboseLogging }}\n        access_log   /dev/stderr  main;\n        {{- else }}\n\n        map $status $loggable {\n          ~^[23]  0;\n          default 1;\n        }\n        access_log   /dev/stderr  main  if=$loggable;\n        {{- end }}\n\n        sendfile     on;\n        tcp_nopush   on;\n        resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};\n\n        {{- with .Values.gateway.nginxConfig.httpSnippet }}\n        {{ . | nindent 2 }}\n        {{- end }}\n\n        server {\n          listen             8080;\n\n          {{- if .Values.gateway.basicAuth.enabled }}\n          auth_basic           \"Loki\";\n          auth_basic_user_file /etc/nginx/secrets/.htpasswd;\n          {{- end }}\n\n          location = / {\n            return 200 'OK';\n            auth_basic off;\n          }\n\n          location = /api/prom/push {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /api/prom/tail {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection \"upgrade\";\n          }\n\n          location ~ /api/prom/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /loki/api/v1/push {\n            proxy_pass       http://{{ include \"loki.writeFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /loki/api/v1/tail {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n            proxy_set_header Upgrade $http_upgrade;\n            proxy_set_header Connection \"upgrade\";\n          }\n\n          location ~ /loki/api/.* {\n            proxy_pass       http://{{ include \"loki.readFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          {{- with .Values.gateway.nginxConfig.serverSnippet }}\n          {{ . | nindent 4 }}\n          {{- end }}\n        }\n      }\n\nnetworkPolicy:\n  # -- Specifies whether Network Policies should be created\n  enabled: false\n  metrics:\n    # -- Specifies the Pods which are allowed to access the metrics port.\n    # As this is cross-namespace communication, you also need the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespaces which are allowed to access the metrics port\n    namespaceSelector: {}\n    # -- Specifies specific network CIDRs which are allowed to access the metrics port.\n    # In case you use namespaceSelector, you also have to specify your kubelet networks here.\n    # The metrics ports are also used for probes.\n    cidrs: []\n  ingress:\n    # -- Specifies the Pods which are allowed to access the http port.\n    # As this is cross-namespace communication, you also need the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespaces which are allowed to access the http port\n    namespaceSelector: {}\n  alertmanager:\n    # -- Specify the alertmanager port used for alerting\n    port: 9093\n    # -- Specifies the alertmanager Pods.\n    # As this is cross-namespace communication, you also need the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespace the alertmanager is running in\n    namespaceSelector: {}\n  externalStorage:\n    # -- Specify the port used for external storage, e.g. AWS S3\n    ports: []\n    # -- Specifies specific network CIDRs you want to limit access to\n    cidrs: []\n  discovery:\n    # -- (int) Specify the port used for discovery\n    port: null\n    # -- Specifies the Pods labels used for discovery.\n    # As this is cross-namespace communication, you also need the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespace the discovery Pods are running in\n    namespaceSelector: {}\n\n# -------------------------------------\n# Configuration for `minio` child chart\n# -------------------------------------\nminio:\n  enabled: true\n  # accessKey: enterprise-logs\n  # secretKey: supersecret\n  buckets:\n    - name: chunks\n      policy: none\n      purge: false\n    - name: ruler\n      policy: none\n      purge: false\n    - name: admin\n      policy: none\n      purge: false\n  persistence:\n    size: 5Gi\n  resources:\n    requests:\n      cpu: 100m\n      memory: 128Mi\n"
            ],
            "verify": false,
            "version": "1.8.5",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.test-module.data.template_file.loki"
          ]
        }
      ]
    },
    {
      "module": "module.test-module",
      "mode": "managed",
      "type": "helm_release",
      "name": "myapp",
      "provider": "module.test-module.provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "myapp",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "myapp",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "1.16.0",
                "chart": "myapp",
                "name": "myapp",
                "namespace": "myapp",
                "revision": 1,
                "values": "{}",
                "version": "0.1.0"
              }
            ],
            "name": "myapp",
            "namespace": "myapp",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": ".terraform/modules/test-module/helm/",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": null,
            "verify": false,
            "version": "0.1.0",
            "wait": false,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.test-module.data.template_file.loki",
            "module.test-module.data.template_file.prometheus",
            "module.test-module.data.template_file.promtail",
            "module.test-module.data.template_file.tempo",
            "module.test-module.helm_release.loki",
            "module.test-module.helm_release.prometheus",
            "module.test-module.helm_release.promtail",
            "module.test-module.helm_release.tempo"
          ]
        }
      ]
    },
    {
      "module": "module.test-module",
      "mode": "managed",
      "type": "helm_release",
      "name": "prometheus",
      "provider": "module.test-module.provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "kube-prometheus-stack",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "kube-prometheus-stack",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "0.58.0",
                "chart": "kube-prometheus-stack",
                "name": "kube-prometheus-stack",
                "namespace": "monitoring",
                "revision": 1,
                "values": "{\"coreDns\":{\"enabled\":true,\"service\":{\"port\":9153,\"targetPort\":9153},\"serviceMonitor\":{\"additionalLabels\":{},\"interval\":\"\",\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[]}},\"grafana\":{\"additionalDataSources\":[{\"access\":\"proxy\",\"basicAuth\":false,\"jsonData\":{\"derivedFields\":[{\"datasourceUid\":\"Tempo\",\"matcherRegex\":\"[t|T][R|r][A|a][C|c][E|e][i|I][d|D][:|=|\\\\s](\\\\w+)\",\"name\":\"TraceId\",\"url\":\"\\\"${__value.raw}\\\"\"}],\"maxLines\":1000,\"tlsSkipVerify\":true},\"name\":\"Loki\",\"type\":\"loki\",\"url\":\"http://loki-gateway.loki.svc\"},{\"access\":\"proxy\",\"jsonData\":{\"httpMethod\":\"GET\",\"lokiSearch\":{\"datasourceUid\":\"Loki\"},\"nodeGraph\":{\"enabled\":true},\"search\":{\"hide\":false},\"tlsSkipVerify\":true,\"tracesToLogs\":{\"datasourceUid\":\"Loki\",\"filterBySpanID\":true,\"filterByTraceID\":true,\"mapTagNamesEnabled\":true,\"mappedTags\":[{\"key\":\"service.name\",\"value\":\"service\"}],\"spanEndTimeShift\":\"1h\",\"spanStartTimeShift\":\"1h\",\"tags\":[\"job\",\"instance\",\"pod\",\"namespace\"]}},\"name\":\"Tempo\",\"type\":\"tempo\",\"url\":\"http://tempo-tempo-distributed-query-frontend.tempo.svc:3100\"}],\"adminPassword\":\"prom-operator\",\"defaultDashboardsEnabled\":true,\"defaultDashboardsTimezone\":\"utc\",\"deleteDatasources\":[],\"enabled\":true,\"extraConfigmapMounts\":[],\"forceDeployDashboards\":false,\"forceDeployDatasources\":false,\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[],\"labels\":{},\"path\":\"/\",\"tls\":[]},\"namespaceOverride\":\"\",\"rbac\":{\"pspEnabled\":false},\"service\":{\"portName\":\"http-web\"},\"serviceMonitor\":{\"enabled\":true,\"interval\":\"\",\"labels\":{},\"path\":\"/metrics\",\"relabelings\":[],\"scheme\":\"http\",\"scrapeTimeout\":\"30s\",\"tlsConfig\":{}},\"sidecar\":{\"dashboards\":{\"annotations\":{},\"enabled\":true,\"label\":\"grafana_dashboard\",\"labelValue\":\"1\",\"multicluster\":{\"etcd\":{\"enabled\":false},\"global\":{\"enabled\":false}},\"provider\":{\"allowUiUpdates\":false}},\"datasources\":{\"annotations\":{},\"createPrometheusReplicasDatasources\":false,\"defaultDatasourceEnabled\":true,\"enabled\":true,\"exemplarTraceIdDestinations\":{},\"label\":\"grafana_datasource\",\"labelValue\":\"1\",\"uid\":\"prometheus\"}}},\"kubeApiServer\":{\"enabled\":true,\"serviceMonitor\":{\"additionalLabels\":{},\"interval\":\"\",\"jobLabel\":\"component\",\"metricRelabelings\":[{\"action\":\"drop\",\"regex\":\"apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)\",\"sourceLabels\":[\"__name__\",\"le\"]}],\"proxyUrl\":\"\",\"relabelings\":[],\"selector\":{\"matchLabels\":{\"component\":\"apiserver\",\"provider\":\"kubernetes\"}}},\"tlsConfig\":{\"insecureSkipVerify\":false,\"serverName\":\"kubernetes\"}},\"kubeControllerManager\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"port\":null,\"targetPort\":null},\"serviceMonitor\":{\"additionalLabels\":{},\"enabled\":true,\"https\":null,\"insecureSkipVerify\":null,\"interval\":\"\",\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"serverName\":null}},\"kubeDns\":{\"enabled\":false,\"service\":{\"dnsmasq\":{\"port\":10054,\"targetPort\":10054},\"skydns\":{\"port\":10055,\"targetPort\":10055}},\"serviceMonitor\":{\"additionalLabels\":{},\"dnsmasqMetricRelabelings\":[],\"dnsmasqRelabelings\":[],\"interval\":\"\",\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[]}},\"kubeEtcd\":{\"enabled\":true,\"endpoints\":[],\"service\":{\"enabled\":true,\"port\":2379,\"targetPort\":2381},\"serviceMonitor\":{\"additionalLabels\":{},\"caFile\":\"\",\"certFile\":\"\",\"enabled\":true,\"insecureSkipVerify\":false,\"interval\":\"\",\"keyFile\":\"\",\"metricRelabelings\":[],\"proxyUrl\":\"\",\"relabelings\":[],\"scheme\":\"http\",\"serverName\":\"\"}},\"kubelet\":{\"enabled\":true,\"namespace\":\"kube-system\",\"serviceMonitor\":{\"additionalLabels\":{},\"cAdvisor\":true,\"cAdvisorMetricRelabelings\":[{\"action\":\"drop\",\"regex\":\"container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_memory_(mapped_file|swap)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_(file_descriptors|tasks_state|threads_max)\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\"container_spec.*\",\"sourceLabels\":[\"__name__\"]},{\"action\":\"drop\",\"regex\":\".+;\",\"sourceLabels\":[\"id\",\"pod\"]}],\"cAdvisorRelabelings\":[{\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"https\":true,\"interval\":\"\",\"metricRelabelings\":[],\"probes\":true,\"probesMetricRelabelings\":[],\"probesRelabelings\":[{\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"proxyUrl\":\"\",\"relabelings\":[{\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}],\"resource\":false,\"resourcePath\":\"/metrics/resource/v1alpha1\",\"resourceRelabelings\":[{\"sourceLabels\":[\"__metrics_path__\"],\"targetLabel\":\"metrics_path\"}]}}}",
                "version": "39.2.1"
              }
            ],
            "name": "kube-prometheus-stack",
            "namespace": "monitoring",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://prometheus-community.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 665,
            "values": [
              "grafana:\n  enabled: true\n  namespaceOverride: \"\"\n\n  ## ForceDeployDatasources Create datasource configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDatasources: false\n\n  ## ForceDeployDashboard Create dashboard configmap even if grafana deployment has been disabled\n  ##\n  forceDeployDashboards: false\n\n  ## Deploy default dashboards\n  ##\n  defaultDashboardsEnabled: true\n\n  ## Timezone for the default dashboards\n  ## Other options are: browser or a specific timezone, i.e. Europe/Luxembourg\n  ##\n  defaultDashboardsTimezone: utc\n\n  adminPassword: prom-operator\n\n  rbac:\n    ## If true, Grafana PSPs will be created\n    ##\n    pspEnabled: false\n\n  ingress:\n    ## If true, Grafana Ingress will be created\n    ##\n    enabled: false\n\n    ## IngressClassName for Grafana Ingress.\n    ## Should be provided if Ingress is enable.\n    ##\n    # ingressClassName: nginx\n\n    ## Annotations for Grafana Ingress\n    ##\n    annotations:\n      {}\n      # kubernetes.io/ingress.class: nginx\n      # kubernetes.io/tls-acme: \"true\"\n\n    ## Labels to be added to the Ingress\n    ##\n    labels: {}\n\n    ## Hostnames.\n    ## Must be provided if Ingress is enable.\n    ##\n    # hosts:\n    #   - grafana.domain.com\n    hosts: []\n\n    ## Path for grafana ingress\n    path: /\n\n    ## TLS configuration for grafana Ingress\n    ## Secret must be manually created in the namespace\n    ##\n    tls: []\n    # - secretName: grafana-general-tls\n    #   hosts:\n    #   - grafana.example.com\n\n  sidecar:\n    dashboards:\n      enabled: true\n      label: grafana_dashboard\n      labelValue: \"1\"\n\n      ## Annotations for Grafana dashboard configmaps\n      ##\n      annotations: {}\n      multicluster:\n        global:\n          enabled: false\n        etcd:\n          enabled: false\n      provider:\n        allowUiUpdates: false\n    datasources:\n      enabled: true\n      defaultDatasourceEnabled: true\n\n      uid: prometheus\n\n      ## URL of prometheus datasource\n      ##\n      # url: http://prometheus-stack-prometheus:9090/\n\n      # If not defined, will use prometheus.prometheusSpec.scrapeInterval or its default\n      # defaultDatasourceScrapeInterval: 15s\n\n      ## Annotations for Grafana datasource configmaps\n      ##\n      annotations: {}\n\n      ## Create datasource for each Pod of Prometheus StatefulSet;\n      ## this uses headless service `prometheus-operated` which is\n      ## created by Prometheus Operator\n      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/0fee93e12dc7c2ea1218f19ae25ec6b893460590/pkg/prometheus/statefulset.go#L255-L286\n      createPrometheusReplicasDatasources: false\n      label: grafana_datasource\n      labelValue: \"1\"\n\n      ## Field with internal link pointing to existing data source in Grafana.\n      ## Can be provisioned via additionalDataSources\n      exemplarTraceIdDestinations:\n        {}\n        # datasourceUid: Jaeger\n        # traceIdLabelName: trace_id\n\n  extraConfigmapMounts: []\n  # - name: certs-configmap\n  #   mountPath: /etc/grafana/ssl/\n  #   configMap: certs-configmap\n  #   readOnly: true\n\n  deleteDatasources: []\n  # - name: example-datasource\n  #   orgId: 1\n\n  ## Configure additional grafana datasources (passed through tpl)\n  ## ref: http://docs.grafana.org/administration/provisioning/#datasources\n  additionalDataSources:\n    - name: Loki\n      type: loki\n      access: proxy\n      url: http://loki-gateway.loki.svc\n      basicAuth: false\n      jsonData:\n        tlsSkipVerify: true\n        maxLines: 1000\n        derivedFields:\n          - datasourceUid: Tempo\n            matcherRegex: \"[t|T][R|r][A|a][C|c][E|e][i|I][d|D][:|=|\\\\s](\\\\w+)\"\n            name: \"TraceId\"\n            url: '\"${__value.raw}\"'\n\n    - name: Tempo\n      type: tempo\n      # Access mode - proxy (server in the UI) or direct (browser in the UI).\n      access: proxy\n      url: http://tempo-tempo-distributed-query-frontend.tempo.svc:3100\n      jsonData:\n        tlsSkipVerify: true\n        httpMethod: GET\n        tracesToLogs:\n          datasourceUid: Loki\n          tags: [\"job\", \"instance\", \"pod\", \"namespace\"]\n          mappedTags: [{ key: \"service.name\", value: \"service\" }]\n          mapTagNamesEnabled: true\n          spanStartTimeShift: \"1h\"\n          spanEndTimeShift: \"1h\"\n          filterByTraceID: true\n          filterBySpanID: true\n        # serviceMap:\n        #   datasourceUid: 'prometheus'\n        search:\n          hide: false\n        nodeGraph:\n          enabled: true\n        lokiSearch:\n          datasourceUid: \"Loki\"\n\n  ## Passed to grafana subchart and used by servicemonitor below\n  ##\n  service:\n    portName: http-web\n\n  serviceMonitor:\n    # If true, a ServiceMonitor CRD is created for a prometheus operator\n    # https://github.com/coreos/prometheus-operator\n    #\n    enabled: true\n\n    # Path to use for scraping metrics. Might be different if server.root_url is set\n    # in grafana.ini\n    path: \"/metrics\"\n\n    #  namespace: monitoring  (defaults to use the namespace this chart is deployed to)\n\n    # labels for the ServiceMonitor\n    labels: {}\n\n    # Scrape interval. If not set, the Prometheus default scrape interval is used.\n    #\n    interval: \"\"\n    scheme: http\n    tlsConfig: {}\n    scrapeTimeout: 30s\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n## Component scraping the kube api server\n##\nkubeApiServer:\n  enabled: true\n  tlsConfig:\n    serverName: kubernetes\n    insecureSkipVerify: false\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    jobLabel: component\n    selector:\n      matchLabels:\n        component: apiserver\n        provider: kubernetes\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings:\n      # Drop excessively noisy apiserver buckets.\n      - action: drop\n        regex: apiserver_request_duration_seconds_bucket;(0.15|0.2|0.3|0.35|0.4|0.45|0.6|0.7|0.8|0.9|1.25|1.5|1.75|2|3|3.5|4|4.5|6|7|8|9|15|25|40|50)\n        sourceLabels:\n          - __name__\n          - le\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels:\n    #     - __meta_kubernetes_namespace\n    #     - __meta_kubernetes_service_name\n    #     - __meta_kubernetes_endpoint_port_name\n    #   action: keep\n    #   regex: default;kubernetes;https\n    # - targetLabel: __address__\n    #   replacement: kubernetes.default.svc:443\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping the kubelet and kubelet-hosted cAdvisor\n##\nkubelet:\n  enabled: true\n  namespace: kube-system\n\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## Enable scraping the kubelet over https. For requirements to enable this see\n    ## https://github.com/prometheus-operator/prometheus-operator/issues/926\n    ##\n    https: true\n\n    ## Enable scraping /metrics/cadvisor from kubelet's service\n    ##\n    cAdvisor: true\n\n    ## Enable scraping /metrics/probes from kubelet's service\n    ##\n    probes: true\n\n    ## Enable scraping /metrics/resource from kubelet's service\n    ## This is disabled by default because container metrics are already exposed by cAdvisor\n    ##\n    resource: false\n    # From kubernetes 1.18, /metrics/resource/v1alpha1 renamed to /metrics/resource\n    resourcePath: \"/metrics/resource/v1alpha1\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    cAdvisorMetricRelabelings:\n      # Drop less useful container CPU metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: \"container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)\"\n      # Drop less useful container / always zero filesystem metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: \"container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)\"\n      # Drop less useful / always zero container memory metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: \"container_memory_(mapped_file|swap)\"\n      # Drop less useful container process metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: \"container_(file_descriptors|tasks_state|threads_max)\"\n      # Drop container spec metrics that overlap with kube-state-metrics.\n      - sourceLabels: [__name__]\n        action: drop\n        regex: \"container_spec.*\"\n      # Drop cgroup metrics with no pod.\n      - sourceLabels: [id, pod]\n        action: drop\n        regex: \".+;\"\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    probesMetricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    ## metrics_path is required to match upstream rules and charts\n    cAdvisorRelabelings:\n      - sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    probesRelabelings:\n      - sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    resourceRelabelings:\n      - sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - sourceLabels: [__name__, image]\n    #   separator: ;\n    #   regex: container_([a-z_]+);\n    #   replacement: $1\n    #   action: drop\n    # - sourceLabels: [__name__]\n    #   separator: ;\n    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)\n    #   replacement: $1\n    #   action: drop\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    ## metrics_path is required to match upstream rules and charts\n    relabelings:\n      - sourceLabels: [__metrics_path__]\n        targetLabel: metrics_path\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping the kube controller manager\n##\nkubeControllerManager:\n  enabled: true\n\n  ## If your kube controller manager is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## If using kubeControllerManager.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version due to change\n    ## of default port in Kubernetes 1.22.\n    ##\n    port: null\n    targetPort: null\n    # selector:\n    #   component: kube-controller-manager\n\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## Enable scraping kube-controller-manager over https.\n    ## Requires proper certs (not self-signed) and delegated authentication/authorization checks.\n    ## If null or unset, the value is determined dynamically based on target Kubernetes version.\n    ##\n    https: null\n\n    # Skip TLS certificate validation when scraping\n    insecureSkipVerify: null\n\n    # Name of the server to use when validating TLS certificate\n    serverName: null\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping coreDns. Use either this or kubeDns\n##\ncoreDns:\n  enabled: true\n  service:\n    port: 9153\n    targetPort: 9153\n    # selector:\n    #   k8s-app: kube-dns\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping kubeDns. Use either this or coreDns\n##\nkubeDns:\n  enabled: false\n  service:\n    dnsmasq:\n      port: 10054\n      targetPort: 10054\n    skydns:\n      port: 10055\n      targetPort: 10055\n    # selector:\n    #   k8s-app: kube-dns\n  serviceMonitor:\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    dnsmasqMetricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    dnsmasqRelabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar\n\n## Component scraping etcd\n##\nkubeEtcd:\n  enabled: true\n\n  ## If your etcd is not deployed as a pod, specify IPs it can be found on\n  ##\n  endpoints: []\n  # - 10.141.4.22\n  # - 10.141.4.23\n  # - 10.141.4.24\n\n  ## Etcd service. If using kubeEtcd.endpoints only the port and targetPort are used\n  ##\n  service:\n    enabled: true\n    port: 2379\n    targetPort: 2381\n    # selector:\n    #   component: etcd\n\n  ## Configure secure access to the etcd cluster by loading a secret into prometheus and\n  ## specifying security configuration below. For example, with a secret named etcd-client-cert\n  ##\n  ## serviceMonitor:\n  ##   scheme: https\n  ##   insecureSkipVerify: false\n  ##   serverName: localhost\n  ##   caFile: /etc/prometheus/secrets/etcd-client-cert/etcd-ca\n  ##   certFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client\n  ##   keyFile: /etc/prometheus/secrets/etcd-client-cert/etcd-client-key\n  ##\n  serviceMonitor:\n    enabled: true\n    ## Scrape interval. If not set, the Prometheus default scrape interval is used.\n    ##\n    interval: \"\"\n    ## proxyUrl: URL of a proxy that should be used for scraping.\n    ##\n    proxyUrl: \"\"\n    scheme: http\n    insecureSkipVerify: false\n    serverName: \"\"\n    caFile: \"\"\n    certFile: \"\"\n    keyFile: \"\"\n\n    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    metricRelabelings: []\n    # - action: keep\n    #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'\n    #   sourceLabels: [__name__]\n\n    ## RelabelConfigs to apply to samples before scraping\n    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig\n    ##\n    relabelings: []\n    # - sourceLabels: [__meta_kubernetes_pod_node_name]\n    #   separator: ;\n    #   regex: ^(.*)$\n    #   targetLabel: nodename\n    #   replacement: $1\n    #   action: replace\n\n    ## Additional labels\n    ##\n    additionalLabels: {}\n    #  foo: bar"
            ],
            "verify": false,
            "version": "39.2.1",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.test-module.data.template_file.loki",
            "module.test-module.data.template_file.prometheus",
            "module.test-module.data.template_file.promtail",
            "module.test-module.data.template_file.tempo",
            "module.test-module.helm_release.loki",
            "module.test-module.helm_release.promtail",
            "module.test-module.helm_release.tempo"
          ]
        }
      ]
    },
    {
      "module": "module.test-module",
      "mode": "managed",
      "type": "helm_release",
      "name": "promtail",
      "provider": "module.test-module.provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "promtail",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "promtail",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "2.6.1",
                "chart": "promtail",
                "name": "promtail",
                "namespace": "loki",
                "revision": 1,
                "values": "{\"affinity\":{},\"annotations\":{},\"config\":{\"clients\":[{\"url\":\"http://loki-gateway.loki.svc/loki/api/v1/push\"}],\"file\":\"server:\\n  log_level: {{ .Values.config.logLevel }}\\n  http_listen_port: {{ .Values.config.serverPort }}\\n  {{- tpl .Values.config.snippets.extraServerConfigs . | nindent 2 }}\\n\\nclients:\\n  {{- tpl (toYaml .Values.config.clients) . | nindent 2 }}\\n\\npositions:\\n  filename: /run/promtail/positions.yaml\\n\\nscrape_configs:\\n  {{- tpl .Values.config.snippets.scrapeConfigs . | nindent 2 }}\\n  {{- tpl .Values.config.snippets.extraScrapeConfigs . | nindent 2 }}\\n\",\"logLevel\":\"info\",\"serverPort\":3101,\"snippets\":{\"addScrapeJobLabel\":false,\"common\":[{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_node_name\"],\"target_label\":\"node_name\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_namespace\"],\"target_label\":\"namespace\"},{\"action\":\"replace\",\"replacement\":\"$1\",\"separator\":\"/\",\"source_labels\":[\"namespace\",\"app\"],\"target_label\":\"job\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_name\"],\"target_label\":\"pod\"},{\"action\":\"replace\",\"source_labels\":[\"__meta_kubernetes_pod_container_name\"],\"target_label\":\"container\"},{\"action\":\"replace\",\"replacement\":\"/var/log/pods/*$1/*.log\",\"separator\":\"/\",\"source_labels\":[\"__meta_kubernetes_pod_uid\",\"__meta_kubernetes_pod_container_name\"],\"target_label\":\"__path__\"},{\"action\":\"replace\",\"regex\":\"true/(.*)\",\"replacement\":\"/var/log/pods/*$1/*.log\",\"separator\":\"/\",\"source_labels\":[\"__meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash\",\"__meta_kubernetes_pod_annotation_kubernetes_io_config_hash\",\"__meta_kubernetes_pod_container_name\"],\"target_label\":\"__path__\"}],\"extraRelabelConfigs\":[],\"extraScrapeConfigs\":\"\",\"extraServerConfigs\":\"\",\"pipelineStages\":[{\"cri\":{}}],\"scrapeConfigs\":\"# See also https://github.com/grafana/loki/blob/master/production/ksonnet/promtail/scrape_config.libsonnet for reference\\n- job_name: kubernetes-pods\\n  pipeline_stages:\\n    {{- toYaml .Values.config.snippets.pipelineStages | nindent 4 }}\\n  kubernetes_sd_configs:\\n    - role: pod\\n  relabel_configs:\\n    - source_labels:\\n        - __meta_kubernetes_pod_controller_name\\n      regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?\\n      action: replace\\n      target_label: __tmp_controller_name\\n    - source_labels:\\n        - __meta_kubernetes_pod_label_app_kubernetes_io_name\\n        - __meta_kubernetes_pod_label_app\\n        - __tmp_controller_name\\n        - __meta_kubernetes_pod_name\\n      regex: ^;*([^;]+)(;.*)?$\\n      action: replace\\n      target_label: app\\n    - source_labels:\\n        - __meta_kubernetes_pod_label_app_kubernetes_io_instance\\n        - __meta_kubernetes_pod_label_release\\n      regex: ^;*([^;]+)(;.*)?$\\n      action: replace\\n      target_label: instance\\n    - source_labels:\\n        - __meta_kubernetes_pod_label_app_kubernetes_io_component\\n        - __meta_kubernetes_pod_label_component\\n      regex: ^;*([^;]+)(;.*)?$\\n      action: replace\\n      target_label: component\\n    {{- if .Values.config.snippets.addScrapeJobLabel }}\\n    - replacement: kubernetes-pods\\n      target_label: scrape_job\\n    {{- end }}\\n    {{- toYaml .Values.config.snippets.common | nindent 4 }}\\n    {{- with .Values.config.snippets.extraRelabelConfigs }}\\n    {{- toYaml . | nindent 4 }}\\n    {{- end }}\\n\"}},\"containerSecurityContext\":{\"allowPrivilegeEscalation\":false,\"capabilities\":{\"drop\":[\"ALL\"]},\"readOnlyRootFilesystem\":true},\"daemonset\":{\"enabled\":true},\"defaultVolumeMounts\":[{\"mountPath\":\"/run/promtail\",\"name\":\"run\"},{\"mountPath\":\"/var/lib/docker/containers\",\"name\":\"containers\",\"readOnly\":true},{\"mountPath\":\"/var/log/pods\",\"name\":\"pods\",\"readOnly\":true}],\"defaultVolumes\":[{\"hostPath\":{\"path\":\"/run/promtail\"},\"name\":\"run\"},{\"hostPath\":{\"path\":\"/var/lib/docker/containers\"},\"name\":\"containers\"},{\"hostPath\":{\"path\":\"/var/log/pods\"},\"name\":\"pods\"}],\"deployment\":{\"autoscaling\":{\"enabled\":false,\"maxReplicas\":10,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":80,\"targetMemoryUtilizationPercentage\":null},\"enabled\":false,\"replicaCount\":1},\"extraArgs\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraObjects\":[],\"extraPorts\":{},\"extraVolumeMounts\":[],\"extraVolumes\":[],\"fullnameOverride\":null,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"docker.io\",\"repository\":\"grafana/promtail\",\"tag\":null},\"imagePullSecrets\":[],\"initContainer\":{\"enabled\":false,\"fsInotifyMaxUserInstances\":128,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"docker.io\",\"repository\":\"busybox\",\"tag\":1.33}},\"livenessProbe\":{},\"nameOverride\":null,\"networkPolicy\":{\"enabled\":false,\"k8sApi\":{\"cidrs\":[],\"port\":8443},\"metrics\":{\"cidrs\":[],\"namespaceSelector\":{},\"podSelector\":{}}},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"podSecurityContext\":{\"runAsGroup\":0,\"runAsUser\":0},\"podSecurityPolicy\":{\"allowPrivilegeEscalation\":true,\"fsGroup\":{\"rule\":\"RunAsAny\"},\"hostIPC\":false,\"hostNetwork\":false,\"hostPID\":false,\"privileged\":true,\"readOnlyRootFilesystem\":true,\"requiredDropCapabilities\":[\"ALL\"],\"runAsUser\":{\"rule\":\"RunAsAny\"},\"seLinux\":{\"rule\":\"RunAsAny\"},\"supplementalGroups\":{\"rule\":\"RunAsAny\"},\"volumes\":[\"secret\",\"hostPath\",\"downwardAPI\"]},\"priorityClassName\":null,\"rbac\":{\"create\":true,\"pspEnabled\":false},\"readinessProbe\":{\"failureThreshold\":5,\"httpGet\":{\"path\":\"/ready\",\"port\":\"http-metrics\"},\"initialDelaySeconds\":10,\"periodSeconds\":10,\"successThreshold\":1,\"timeoutSeconds\":1},\"resources\":{},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"imagePullSecrets\":[],\"name\":null},\"serviceMonitor\":{\"annotations\":{},\"enabled\":false,\"interval\":null,\"labels\":{},\"metricRelabelings\":[],\"namespace\":null,\"namespaceSelector\":{},\"relabelings\":[],\"scrapeTimeout\":null},\"tolerations\":[{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/master\",\"operator\":\"Exists\"},{\"effect\":\"NoSchedule\",\"key\":\"node-role.kubernetes.io/control-plane\",\"operator\":\"Exists\"}],\"updateStrategy\":{}}",
                "version": "6.2.2"
              }
            ],
            "name": "promtail",
            "namespace": "loki",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://grafana.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 300,
            "values": [
              "# -- Overrides the chart's name\nnameOverride: null\n\n# -- Overrides the chart's computed fullname\nfullnameOverride: null\n\ndaemonset:\n  # -- Deploys Promtail as a DaemonSet\n  enabled: true\n\ndeployment:\n  # -- Deploys Promtail as a Deployment\n  enabled: false\n  replicaCount: 1\n  autoscaling:\n    # -- Creates a HorizontalPodAutoscaler for the deployment\n    enabled: false\n    minReplicas: 1\n    maxReplicas: 10\n    targetCPUUtilizationPercentage: 80\n    targetMemoryUtilizationPercentage:\n\ninitContainer:\n  # -- Specifies whether the init container for setting inotify max user instances is to be enabled\n  enabled: false\n  image:\n    # -- The Docker registry for the init container\n    registry: docker.io\n    # -- Docker image repository for the init container\n    repository: busybox\n    # -- Docker tag for the init container\n    tag: 1.33\n    # -- Docker image pull policy for the init container image\n    pullPolicy: IfNotPresent\n  # -- The inotify max user instances to configure\n  fsInotifyMaxUserInstances: 128\n\nimage:\n  # -- The Docker registry\n  registry: docker.io\n  # -- Docker image repository\n  repository: grafana/promtail\n  # -- Overrides the image tag whose default is the chart's appVersion\n  tag: null\n  # -- Docker image pull policy\n  pullPolicy: IfNotPresent\n\n# -- Image pull secrets for Docker images\nimagePullSecrets: []\n\n# -- Annotations for the DaemonSet\nannotations: {}\n\n# -- The update strategy for the DaemonSet\nupdateStrategy: {}\n\n# -- Pod labels\npodLabels: {}\n\n# -- Pod annotations\npodAnnotations: {}\n#  prometheus.io/scrape: \"true\"\n#  prometheus.io/port: \"http-metrics\"\n\n# -- The name of the PriorityClass\npriorityClassName: null\n\n# -- Liveness probe\nlivenessProbe: {}\n\n# -- Readiness probe\n# @default -- See `values.yaml`\nreadinessProbe:\n  failureThreshold: 5\n  httpGet:\n    path: /ready\n    port: http-metrics\n  initialDelaySeconds: 10\n  periodSeconds: 10\n  successThreshold: 1\n  timeoutSeconds: 1\n\n# -- Resource requests and limits\nresources: {}\n#  limits:\n#    cpu: 200m\n#    memory: 128Mi\n#  requests:\n#    cpu: 100m\n#    memory: 128Mi\n\n# -- The security context for pods\npodSecurityContext:\n  runAsUser: 0\n  runAsGroup: 0\n\n# -- The security context for containers\ncontainerSecurityContext:\n  readOnlyRootFilesystem: true\n  capabilities:\n    drop:\n      - ALL\n  allowPrivilegeEscalation: false\n\nrbac:\n  # -- Specifies whether RBAC resources are to be created\n  create: true\n  # -- Specifies whether a PodSecurityPolicy is to be created\n  pspEnabled: false\n\nserviceAccount:\n  # -- Specifies whether a ServiceAccount should be created\n  create: true\n  # -- The name of the ServiceAccount to use.\n  # If not set and `create` is true, a name is generated using the fullname template\n  name: null\n  # -- Image pull secrets for the service account\n  imagePullSecrets: []\n  # -- Annotations for the service account\n  annotations: {}\n\n# -- Node selector for pods\nnodeSelector: {}\n\n# -- Affinity configuration for pods\naffinity: {}\n\n# -- Tolerations for pods. By default, pods will be scheduled on master/control-plane nodes.\ntolerations:\n  - key: node-role.kubernetes.io/master\n    operator: Exists\n    effect: NoSchedule\n  - key: node-role.kubernetes.io/control-plane\n    operator: Exists\n    effect: NoSchedule\n\n# -- Default volumes that are mounted into pods. In most cases, these should not be changed.\n# Use `extraVolumes`/`extraVolumeMounts` for additional custom volumes.\n# @default -- See `values.yaml`\ndefaultVolumes:\n  - name: run\n    hostPath:\n      path: /run/promtail\n  - name: containers\n    hostPath:\n      path: /var/lib/docker/containers\n  - name: pods\n    hostPath:\n      path: /var/log/pods\n\n# -- Default volume mounts. Corresponds to `volumes`.\n# @default -- See `values.yaml`\ndefaultVolumeMounts:\n  - name: run\n    mountPath: /run/promtail\n  - name: containers\n    mountPath: /var/lib/docker/containers\n    readOnly: true\n  - name: pods\n    mountPath: /var/log/pods\n    readOnly: true\n\n# Extra volumes to be added in addition to those specified under `defaultVolumes`.\nextraVolumes: []\n\n# Extra volume mounts together. Corresponds to `extraVolumes`.\nextraVolumeMounts: []\n\n# Extra args for the Promtail container.\nextraArgs: []\n# -- Example:\n# -- extraArgs:\n# --   - -client.external-labels=hostname=$(HOSTNAME)\n\n# -- Extra environment variables\nextraEnv: []\n\n# -- Extra environment variables from secrets or configmaps\nextraEnvFrom: []\n\n# ServiceMonitor configuration\nserviceMonitor:\n  # -- If enabled, ServiceMonitor resources for Prometheus Operator are created\n  enabled: false\n  # -- Alternative namespace for ServiceMonitor resources\n  namespace: null\n  # -- Namespace selector for ServiceMonitor resources\n  namespaceSelector: {}\n  # -- ServiceMonitor annotations\n  annotations: {}\n  # -- Additional ServiceMonitor labels\n  labels: {}\n  # -- ServiceMonitor scrape interval\n  interval: null\n  # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)\n  scrapeTimeout: null\n  # -- ServiceMonitor relabel configs to apply to samples before scraping\n  # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n  # (defines `relabel_configs`)\n  relabelings: []\n  # -- ServiceMonitor relabel configs to apply to samples as the last\n  # step before ingestion\n  # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n  # (defines `metric_relabel_configs`)\n  metricRelabelings: []\n\n# -- Configure additional ports and services. For each configured port, a corresponding service is created.\n# See values.yaml for details\nextraPorts: {}\n#  syslog:\n#    name: tcp-syslog\n#    containerPort: 1514\n#    protocol: TCP\n#    service:\n#      type: ClusterIP\n#      clusterIP: null\n#      port: 1514\n#      externalIPs: []\n#      nodePort: null\n#      annotations: {}\n#      labels: {}\n#      loadBalancerIP: null\n#      loadBalancerSourceRanges: []\n#      externalTrafficPolicy: null\n\n# -- PodSecurityPolicy configuration.\n# @default -- See `values.yaml`\npodSecurityPolicy:\n  privileged: true\n  allowPrivilegeEscalation: true\n  volumes:\n    - \"secret\"\n    - \"hostPath\"\n    - \"downwardAPI\"\n  hostNetwork: false\n  hostIPC: false\n  hostPID: false\n  runAsUser:\n    rule: \"RunAsAny\"\n  seLinux:\n    rule: \"RunAsAny\"\n  supplementalGroups:\n    rule: \"RunAsAny\"\n  fsGroup:\n    rule: \"RunAsAny\"\n  readOnlyRootFilesystem: true\n  requiredDropCapabilities:\n    - ALL\n\n# -- Section for crafting Promtails config file. The only directly relevant value is `config.file`\n# which is a templated string that references the other values and snippets below this key.\n# @default -- See `values.yaml`\nconfig:\n  # -- The log level of the Promtail server\n  # Must be reference in `config.file` to configure `server.log_level`\n  # See default config in `values.yaml`\n  logLevel: info\n  # -- The port of the Promtail server\n  # Must be reference in `config.file` to configure `server.http_listen_port`\n  # See default config in `values.yaml`\n  serverPort: 3101\n  # -- The config of clients of the Promtail server\n  # Must be reference in `config.file` to configure `clients`\n  # @default -- See `values.yaml`\n  clients:\n    - url: http://loki-gateway.loki.svc/loki/api/v1/push\n  # -- A section of reusable snippets that can be reference in `config.file`.\n  # Custom snippets may be added in order to reduce redundancy.\n  # This is especially helpful when multiple `kubernetes_sd_configs` are use which usually have large parts in common.\n  # @default -- See `values.yaml`\n  snippets:\n    pipelineStages:\n      - cri: {}\n    common:\n      - action: replace\n        source_labels:\n          - __meta_kubernetes_pod_node_name\n        target_label: node_name\n      - action: replace\n        source_labels:\n          - __meta_kubernetes_namespace\n        target_label: namespace\n      - action: replace\n        replacement: $1\n        separator: /\n        source_labels:\n          - namespace\n          - app\n        target_label: job\n      - action: replace\n        source_labels:\n          - __meta_kubernetes_pod_name\n        target_label: pod\n      - action: replace\n        source_labels:\n          - __meta_kubernetes_pod_container_name\n        target_label: container\n      - action: replace\n        replacement: /var/log/pods/*$1/*.log\n        separator: /\n        source_labels:\n          - __meta_kubernetes_pod_uid\n          - __meta_kubernetes_pod_container_name\n        target_label: __path__\n      - action: replace\n        replacement: /var/log/pods/*$1/*.log\n        regex: true/(.*)\n        separator: /\n        source_labels:\n          - __meta_kubernetes_pod_annotationpresent_kubernetes_io_config_hash\n          - __meta_kubernetes_pod_annotation_kubernetes_io_config_hash\n          - __meta_kubernetes_pod_container_name\n        target_label: __path__\n\n    # If set to true, adds an additional label for the scrape job.\n    # This helps debug the Promtail config.\n    addScrapeJobLabel: false\n\n    # -- You can put here any keys that will be directly added to the config file's 'server' block.\n    # @default -- empty\n    extraServerConfigs: \"\"\n\n    # -- You can put here any additional scrape configs you want to add to the config file.\n    # @default -- empty\n    extraScrapeConfigs: \"\"\n\n    # -- You can put here any additional relabel_configs to \"kubernetes-pods\" job\n    extraRelabelConfigs: []\n\n    scrapeConfigs: |\n      # See also https://github.com/grafana/loki/blob/master/production/ksonnet/promtail/scrape_config.libsonnet for reference\n      - job_name: kubernetes-pods\n        pipeline_stages:\n          {{- toYaml .Values.config.snippets.pipelineStages | nindent 4 }}\n        kubernetes_sd_configs:\n          - role: pod\n        relabel_configs:\n          - source_labels:\n              - __meta_kubernetes_pod_controller_name\n            regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?\n            action: replace\n            target_label: __tmp_controller_name\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_name\n              - __meta_kubernetes_pod_label_app\n              - __tmp_controller_name\n              - __meta_kubernetes_pod_name\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: app\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_instance\n              - __meta_kubernetes_pod_label_release\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: instance\n          - source_labels:\n              - __meta_kubernetes_pod_label_app_kubernetes_io_component\n              - __meta_kubernetes_pod_label_component\n            regex: ^;*([^;]+)(;.*)?$\n            action: replace\n            target_label: component\n          {{- if .Values.config.snippets.addScrapeJobLabel }}\n          - replacement: kubernetes-pods\n            target_label: scrape_job\n          {{- end }}\n          {{- toYaml .Values.config.snippets.common | nindent 4 }}\n          {{- with .Values.config.snippets.extraRelabelConfigs }}\n          {{- toYaml . | nindent 4 }}\n          {{- end }}\n\n  # -- Config file contents for Promtail.\n  # Must be configured as string.\n  # It is templated so it can be assembled from reusable snippets in order to avoid redundancy.\n  # @default -- See `values.yaml`\n  file: |\n    server:\n      log_level: {{ .Values.config.logLevel }}\n      http_listen_port: {{ .Values.config.serverPort }}\n      {{- tpl .Values.config.snippets.extraServerConfigs . | nindent 2 }}\n\n    clients:\n      {{- tpl (toYaml .Values.config.clients) . | nindent 2 }}\n\n    positions:\n      filename: /run/promtail/positions.yaml\n\n    scrape_configs:\n      {{- tpl .Values.config.snippets.scrapeConfigs . | nindent 2 }}\n      {{- tpl .Values.config.snippets.extraScrapeConfigs . | nindent 2 }}\n\nnetworkPolicy:\n  # -- Specifies whether Network Policies should be created\n  enabled: false\n  metrics:\n    # -- Specifies the Pods which are allowed to access the metrics port.\n    # As this is cross-namespace communication, you also neeed the namespaceSelector.\n    podSelector: {}\n    # -- Specifies the namespaces which are allowed to access the metrics port\n    namespaceSelector: {}\n    # -- Specifies specific network CIDRs which are allowed to access the metrics port.\n    # In case you use namespaceSelector, you also have to specify your kubelet networks here.\n    # The metrics ports are also used for probes.\n    cidrs: []\n  k8sApi:\n    # -- Specify the k8s API endpoint port\n    port: 8443\n    # -- Specifies specific network CIDRs you want to limit access to\n    cidrs: []\n\n# -- Extra K8s manifests to deploy\nextraObjects:\n  []\n  # - apiVersion: \"kubernetes-client.io/v1\"\n  #   kind: ExternalSecret\n  #   metadata:\n  #     name: promtail-secrets\n  #   spec:\n  #     backendType: gcpSecretsManager\n  #     data:\n  #       - key: promtail-oauth2-creds\n  #         name: client_secret\n"
            ],
            "verify": false,
            "version": "6.2.2",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.test-module.data.template_file.loki",
            "module.test-module.data.template_file.promtail",
            "module.test-module.helm_release.loki"
          ]
        }
      ]
    },
    {
      "module": "module.test-module",
      "mode": "managed",
      "type": "helm_release",
      "name": "tempo",
      "provider": "module.test-module.provider[\"registry.terraform.io/hashicorp/helm\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 0,
          "attributes": {
            "atomic": false,
            "chart": "tempo-distributed",
            "cleanup_on_fail": false,
            "create_namespace": true,
            "dependency_update": false,
            "description": null,
            "devel": null,
            "disable_crd_hooks": false,
            "disable_openapi_validation": false,
            "disable_webhooks": false,
            "force_update": false,
            "id": "tempo",
            "keyring": null,
            "lint": false,
            "manifest": null,
            "max_history": 0,
            "metadata": [
              {
                "app_version": "1.4.1",
                "chart": "tempo-distributed",
                "name": "tempo",
                "namespace": "tempo",
                "revision": 1,
                "values": "{\"compactor\":{\"config\":{\"compaction\":{\"block_retention\":\"48h\"}},\"extraArgs\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{}},\"terminationGracePeriodSeconds\":30,\"tolerations\":[]},\"config\":\"multitenancy_enabled: {{ .Values.multitenancyEnabled }}\\nsearch_enabled: {{ .Values.search.enabled }}\\nmetrics_generator_enabled: {{ .Values.metricsGenerator.enabled }}\\ncompactor:\\n  compaction:\\n    block_retention: {{ .Values.compactor.config.compaction.block_retention }}\\n  ring:\\n    kvstore:\\n      store: memberlist\\n{{- if .Values.metricsGenerator.enabled }}\\nmetrics_generator:\\n  ring:\\n    kvstore:\\n      store: memberlist\\n  processor:\\n    service_graphs:\\n      max_items: {{ .Values.metricsGenerator.config.service_graphs_max_items }}\\n  storage:\\n    path: /var/tempo/wal\\n    remote_write:\\n      {{- toYaml .Values.metricsGenerator.config.storage_remote_write | nindent 6}}\\n{{- end }}\\ndistributor:\\n  ring:\\n    kvstore:\\n      store: memberlist\\n  receivers:\\n    {{- if  or (.Values.traces.jaeger.thriftCompact.enabled) (.Values.traces.jaeger.thriftBinary.enabled) (.Values.traces.jaeger.thriftHttp.enabled) (.Values.traces.jaeger.grpc.enabled) }}\\n    jaeger:\\n      protocols:\\n        {{- if .Values.traces.jaeger.thriftCompact.enabled }}\\n        thrift_compact:\\n          {{- $mergedJaegerThriftCompactConfig := mustMergeOverwrite (dict \\\"endpoint\\\" \\\"0.0.0.0:6831\\\") .Values.traces.jaeger.thriftCompact.receiverConfig }}\\n          {{- toYaml $mergedJaegerThriftCompactConfig | nindent 10 }}\\n        {{- end }}\\n        {{- if .Values.traces.jaeger.thriftBinary.enabled }}\\n        thrift_binary:\\n          {{- $mergedJaegerThriftBinaryConfig := mustMergeOverwrite (dict \\\"endpoint\\\" \\\"0.0.0.0:6832\\\") .Values.traces.jaeger.thriftBinary.receiverConfig }}\\n          {{- toYaml $mergedJaegerThriftBinaryConfig | nindent 10 }}\\n        {{- end }}\\n        {{- if .Values.traces.jaeger.thriftHttp.enabled }}\\n        thrift_http:\\n          {{- $mergedJaegerThriftHttpConfig := mustMergeOverwrite (dict \\\"endpoint\\\" \\\"0.0.0.0:14268\\\") .Values.traces.jaeger.thriftHttp.receiverConfig }}\\n          {{- toYaml $mergedJaegerThriftHttpConfig | nindent 10 }}\\n        {{- end }}\\n        {{- if .Values.traces.jaeger.grpc.enabled }}\\n        grpc:\\n          {{- $mergedJaegerGrpcConfig := mustMergeOverwrite (dict \\\"endpoint\\\" \\\"0.0.0.0:14250\\\") .Values.traces.jaeger.grpc.receiverConfig }}\\n          {{- toYaml $mergedJaegerGrpcConfig | nindent 10 }}\\n        {{- end }}\\n    {{- end }}\\n    {{- if .Values.traces.zipkin.enabled }}\\n    zipkin:\\n      {{- $mergedZipkinReceiverConfig := mustMergeOverwrite (dict \\\"endpoint\\\" \\\"0.0.0.0:9411\\\") .Values.traces.zipkin.receiverConfig }}\\n      {{- toYaml $mergedZipkinReceiverConfig | nindent 6 }}\\n    {{- end }}\\n    {{- if or (.Values.traces.otlp.http.enabled) (.Values.traces.otlp.grpc.enabled) }}\\n    otlp:\\n      protocols:\\n        {{- if .Values.traces.otlp.http.enabled }}\\n        http:\\n          {{- $mergedOtlpHttpReceiverConfig := mustMergeOverwrite (dict \\\"endpoint\\\" \\\"0.0.0.0:4318\\\") .Values.traces.otlp.http.receiverConfig }}\\n          {{- toYaml $mergedOtlpHttpReceiverConfig | nindent 10 }}\\n        {{- end }}\\n        {{- if .Values.traces.otlp.grpc.enabled }}\\n        grpc:\\n          {{- $mergedOtlpGrpcReceiverConfig := mustMergeOverwrite (dict \\\"endpoint\\\" \\\"0.0.0.0:4317\\\") .Values.traces.otlp.grpc.receiverConfig }}\\n          {{- toYaml $mergedOtlpGrpcReceiverConfig | nindent 10 }}\\n        {{- end }}\\n    {{- end }}\\n    {{- if .Values.traces.opencensus.enabled }}\\n    opencensus:\\n      {{- $mergedOpencensusReceiverConfig := mustMergeOverwrite (dict \\\"endpoint\\\" \\\"0.0.0.0:55678\\\") .Values.traces.opencensus.receiverConfig }}\\n      {{- toYaml $mergedOpencensusReceiverConfig | nindent 6 }}\\n    {{- end }}\\n    {{- if .Values.traces.kafka }}\\n    kafka:\\n      {{- toYaml .Values.traces.kafka | nindent 6 }}\\n    {{- end }}\\n  {{- if .Values.distributor.config.log_received_traces }}\\n  log_received_traces: {{ .Values.distributor.config.log_received_traces }}\\n  {{- end }}\\n  {{- if .Values.distributor.config.extend_writes }}\\n  extend_writes: {{ .Values.distributor.config.extend_writes }}\\n  {{- end }}\\n  {{- if .Values.distributor.config.search_tags_deny_list }}\\n  search_tags_deny_list:\\n    {{- with .Values.distributor.config.search_tags_deny_list }}\\n    {{- toYaml . | nindent 4 }}\\n    {{- end }}\\n  {{- end }}\\nquerier:\\n  frontend_worker:\\n    frontend_address: {{ include \\\"tempo.queryFrontendFullname\\\" . }}-discovery:9095\\n    {{- if .Values.querier.config.frontend_worker.grpc_client_config }}\\n    grpc_client_config:\\n      {{- toYaml .Values.querier.config.frontend_worker.grpc_client_config | nindent 6 }}\\n    {{- end }}\\ningester:\\n  lifecycler:\\n    ring:\\n      replication_factor: {{ .Values.ingester.config.replication_factor }}\\n      kvstore:\\n        store: memberlist\\n    tokens_file_path: /var/tempo/tokens.json\\n  {{- if .Values.ingester.config.trace_idle_period }}\\n  trace_idle_period: {{ .Values.ingester.config.trace_idle_period }}\\n  {{- end }}\\n  {{- if .Values.ingester.config.flush_check_period }}\\n  flush_check_period: {{ .Values.ingester.config.flush_check_period }}\\n  {{- end }}\\n  {{- if .Values.ingester.config.max_block_bytes }}\\n  max_block_bytes: {{ .Values.ingester.config.max_block_bytes }}\\n  {{- end }}\\n  {{- if .Values.ingester.config.max_block_duration }}\\n  max_block_duration: {{ .Values.ingester.config.max_block_duration }}\\n  {{- end }}\\n  {{- if .Values.ingester.config.complete_block_timeout }}\\n  complete_block_timeout: {{ .Values.ingester.config.complete_block_timeout }}\\n  {{- end }}\\nmemberlist:\\n  abort_if_cluster_join_fails: false\\n  join_members:\\n    - {{ include \\\"tempo.fullname\\\" . }}-gossip-ring\\noverrides:\\n  {{- toYaml .Values.global_overrides | nindent 2 }}\\nserver:\\n  http_listen_port: {{ .Values.server.httpListenPort }}\\n  log_level: {{ .Values.server.logLevel }}\\n  log_format: {{ .Values.server.logFormat }}\\n  grpc_server_max_recv_msg_size: {{ .Values.server.grpc_server_max_recv_msg_size }}\\n  grpc_server_max_send_msg_size: {{ .Values.server.grpc_server_max_send_msg_size }}\\nstorage:\\n  trace:\\n    backend: {{.Values.storage.trace.backend}}\\n    {{- if eq .Values.storage.trace.backend \\\"gcs\\\"}}\\n    gcs:\\n      {{- toYaml .Values.storage.trace.gcs | nindent 6}}\\n    {{- end}}\\n    {{- if eq .Values.storage.trace.backend \\\"s3\\\"}}\\n    s3:\\n      {{- toYaml .Values.storage.trace.s3 | nindent 6}}\\n    {{- end}}\\n    {{- if eq .Values.storage.trace.backend \\\"azure\\\"}}\\n    azure:\\n      {{- toYaml .Values.storage.trace.azure | nindent 6}}\\n    {{- end}}\\n    blocklist_poll: 5m\\n    local:\\n      path: /var/tempo/traces\\n    wal:\\n      path: /var/tempo/wal\\n    cache: memcached\\n    memcached:\\n      consistent_hash: true\\n      host: {{ include \\\"tempo.fullname\\\" . }}-memcached\\n      service: memcached-client\\n      timeout: 500ms\\n\",\"distributor\":{\"affinity\":\"podAntiAffinity:\\n  requiredDuringSchedulingIgnoredDuringExecution:\\n    - labelSelector:\\n        matchLabels:\\n          {{- include \\\"tempo.distributorSelectorLabels\\\" . | nindent 10 }}\\n      topologyKey: kubernetes.io/hostname\\n  preferredDuringSchedulingIgnoredDuringExecution:\\n    - weight: 100\\n      podAffinityTerm:\\n        labelSelector:\\n          matchLabels:\\n            {{- include \\\"tempo.distributorSelectorLabels\\\" . | nindent 12 }}\\n        topologyKey: failure-domain.beta.kubernetes.io/zone\\n\",\"autoscaling\":{\"enabled\":false,\"maxReplicas\":3,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":60,\"targetMemoryUtilizationPercentage\":null},\"config\":{\"extend_writes\":null,\"log_received_traces\":null,\"search_tags_deny_list\":[]},\"extraArgs\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{},\"loadBalancerIP\":\"\",\"loadBalancerSourceRanges\":[],\"type\":\"ClusterIP\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[]},\"gateway\":{\"affinity\":\"podAntiAffinity:\\n  requiredDuringSchedulingIgnoredDuringExecution:\\n    - labelSelector:\\n        matchLabels:\\n          {{- include \\\"tempo.gatewaySelectorLabels\\\" . | nindent 10 }}\\n      topologyKey: kubernetes.io/hostname\\n  preferredDuringSchedulingIgnoredDuringExecution:\\n    - weight: 100\\n      podAffinityTerm:\\n        labelSelector:\\n          matchLabels:\\n            {{- include \\\"tempo.gatewaySelectorLabels\\\" . | nindent 12 }}\\n        topologyKey: failure-domain.beta.kubernetes.io/zone\\n\",\"autoscaling\":{\"enabled\":false,\"maxReplicas\":3,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":60,\"targetMemoryUtilizationPercentage\":null},\"basicAuth\":{\"enabled\":false,\"existingSecret\":null,\"htpasswd\":\"{{ htpasswd (required \\\"'gateway.basicAuth.username' is required\\\" .Values.gateway.basicAuth.username) (required \\\"'gateway.basicAuth.password' is required\\\" .Values.gateway.basicAuth.password) }}\",\"password\":null,\"username\":null},\"enabled\":false,\"extraArgs\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":null,\"repository\":\"nginxinc/nginx-unprivileged\",\"tag\":\"1.19-alpine\"},\"ingress\":{\"annotations\":{},\"enabled\":false,\"hosts\":[{\"host\":\"gateway.tempo.example.com\",\"paths\":[{\"path\":\"/\"}]}],\"tls\":[{\"hosts\":[\"gateway.tempo.example.com\"],\"secretName\":\"tempo-gateway-tls\"}]},\"nginxConfig\":{\"file\":\"worker_processes  5;  ## Default: 1\\nerror_log  /dev/stderr;\\npid        /tmp/nginx.pid;\\nworker_rlimit_nofile 8192;\\n\\nevents {\\n  worker_connections  4096;  ## Default: 1024\\n}\\n\\nhttp {\\n  client_body_temp_path /tmp/client_temp;\\n  proxy_temp_path       /tmp/proxy_temp_path;\\n  fastcgi_temp_path     /tmp/fastcgi_temp;\\n  uwsgi_temp_path       /tmp/uwsgi_temp;\\n  scgi_temp_path        /tmp/scgi_temp;\\n\\n  proxy_http_version    1.1;\\n\\n  default_type application/octet-stream;\\n  log_format   {{ .Values.gateway.nginxConfig.logFormat }}\\n\\n  {{- if .Values.gateway.verboseLogging }}\\n  access_log   /dev/stderr  main;\\n  {{- else }}\\n\\n  map $status $loggable {\\n    ~^[23]  0;\\n    default 1;\\n  }\\n  access_log   /dev/stderr  main  if=$loggable;\\n  {{- end }}\\n\\n  sendfile     on;\\n  tcp_nopush   on;\\n  resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};\\n\\n  {{- with .Values.gateway.nginxConfig.httpSnippet }}\\n  {{ . | nindent 2 }}\\n  {{- end }}\\n\\n  server {\\n    listen             8080;\\n\\n    {{- if .Values.gateway.basicAuth.enabled }}\\n    auth_basic           \\\"Tempo\\\";\\n    auth_basic_user_file /etc/nginx/secrets/.htpasswd;\\n    {{- end }}\\n\\n    location = / {\\n      return 200 'OK';\\n      auth_basic off;\\n    }\\n\\n    location = /jaeger/api/traces {\\n      proxy_pass       http://{{ include \\\"tempo.distributorFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:14268/api/traces;\\n    }\\n\\n    location = /zipkin/spans {\\n      proxy_pass       http://{{ include \\\"tempo.distributorFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:9411/spans;\\n    }\\n\\n    location = /otlp/v1/traces {\\n      proxy_pass       http://{{ include \\\"tempo.distributorFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:4318/v1/traces;\\n    }\\n\\n    location ^~ /api {\\n      proxy_pass       http://{{ include \\\"tempo.queryFrontendFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location = /flush {\\n      proxy_pass       http://{{ include \\\"tempo.ingesterFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location = /shutdown {\\n      proxy_pass       http://{{ include \\\"tempo.ingesterFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location = /distributor/ring {\\n      proxy_pass       http://{{ include \\\"tempo.distributorFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location = /ingester/ring {\\n      proxy_pass       http://{{ include \\\"tempo.distributorFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    location = /compactor/ring {\\n      proxy_pass       http://{{ include \\\"tempo.compactorFullname\\\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\\n    }\\n\\n    {{- with .Values.gateway.nginxConfig.serverSnippet }}\\n    {{ . | nindent 4 }}\\n    {{- end }}\\n  }\\n}\\n\",\"httpSnippet\":\"\",\"logFormat\":\"main '$remote_addr - $remote_user [$time_local]  $status '\\n        '\\\"$request\\\" $body_bytes_sent \\\"$http_referer\\\" '\\n        '\\\"$http_user_agent\\\" \\\"$http_x_forwarded_for\\\"';\",\"serverSnippet\":\"\"},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"readinessProbe\":{\"httpGet\":{\"path\":\"/\",\"port\":\"http\"},\"initialDelaySeconds\":15,\"timeoutSeconds\":1},\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{},\"clusterIP\":null,\"labels\":{},\"loadBalancerIP\":null,\"nodePort\":null,\"port\":80,\"type\":\"ClusterIP\"},\"terminationGracePeriodSeconds\":30,\"tolerations\":[],\"verboseLogging\":true},\"global\":{\"clusterDomain\":\"cluster.local\",\"dnsNamespace\":\"kube-system\",\"dnsService\":\"kube-dns\",\"image\":{\"registry\":\"docker.io\"},\"priorityClassName\":null},\"global_overrides\":{\"per_tenant_override_config\":\"/conf/overrides.yaml\"},\"ingester\":{\"affinity\":\"podAntiAffinity:\\n  preferredDuringSchedulingIgnoredDuringExecution:\\n    - weight: 100\\n      podAffinityTerm:\\n        labelSelector:\\n          matchLabels:\\n            {{- include \\\"tempo.ingesterSelectorLabels\\\" . | nindent 12 }}\\n        topologyKey: kubernetes.io/hostname\\n    - weight: 75\\n      podAffinityTerm:\\n        labelSelector:\\n          matchLabels:\\n            {{- include \\\"tempo.ingesterSelectorLabels\\\" . | nindent 12 }}\\n        topologyKey: failure-domain.beta.kubernetes.io/zone\\n\",\"annotations\":{},\"autoscaling\":{\"enabled\":false,\"maxReplicas\":3,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":60,\"targetMemoryUtilizationPercentage\":null},\"config\":{\"complete_block_timeout\":null,\"flush_check_period\":null,\"max_block_bytes\":null,\"max_block_duration\":null,\"replication_factor\":3,\"trace_idle_period\":null},\"extraArgs\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"nodeSelector\":{},\"persistence\":{\"enabled\":false,\"size\":\"10Gi\",\"storageClass\":null},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"replicas\":3,\"resources\":{},\"service\":{\"annotations\":{}},\"terminationGracePeriodSeconds\":300,\"tolerations\":[]},\"memcached\":{\"affinity\":\"podAntiAffinity:\\n  requiredDuringSchedulingIgnoredDuringExecution:\\n    - labelSelector:\\n        matchLabels:\\n          {{- include \\\"tempo.memcachedSelectorLabels\\\" . | nindent 10 }}\\n      topologyKey: kubernetes.io/hostname\\n  preferredDuringSchedulingIgnoredDuringExecution:\\n    - weight: 100\\n      podAffinityTerm:\\n        labelSelector:\\n          matchLabels:\\n            {{- include \\\"tempo.memcachedSelectorLabels\\\" . | nindent 12 }}\\n        topologyKey: failure-domain.beta.kubernetes.io/zone\\n\",\"enabled\":true,\"extraArgs\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"host\":\"memcached\",\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":null,\"repository\":\"memcached\",\"tag\":\"1.5.17-alpine\"},\"podAnnotations\":{},\"podLabels\":{},\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{}}},\"memcachedExporter\":{\"enabled\":false,\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":null,\"repository\":\"prom/memcached-exporter\",\"tag\":\"v0.8.0\"},\"resources\":{}},\"metricsGenerator\":{\"affinity\":\"podAntiAffinity:\\n  requiredDuringSchedulingIgnoredDuringExecution:\\n    - labelSelector:\\n        matchLabels:\\n          {{- include \\\"tempo.metricsGeneratorSelectorLabels\\\" . | nindent 10 }}\\n      topologyKey: kubernetes.io/hostname\\n  preferredDuringSchedulingIgnoredDuringExecution:\\n    - weight: 100\\n      podAffinityTerm:\\n        labelSelector:\\n          matchLabels:\\n            {{- include \\\"tempo.metricsGeneratorSelectorLabels\\\" . | nindent 12 }}\\n        topologyKey: failure-domain.beta.kubernetes.io/zone\\n\",\"annotations\":{},\"config\":{\"service_graphs_max_items\":10000,\"storage_remote_write\":[]},\"enabled\":false,\"extraArgs\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"ports\":[{\"name\":\"grpc\",\"port\":9095,\"service\":true},{\"name\":\"http-memberlist\",\"port\":7946,\"service\":false},{\"name\":\"http\",\"port\":3100,\"service\":true}],\"priorityClassName\":null,\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{}},\"terminationGracePeriodSeconds\":300,\"tolerations\":[]},\"multitenancyEnabled\":false,\"overrides\":\"overrides: {}\\n\",\"prometheusRule\":{\"annotations\":{},\"enabled\":false,\"groups\":[],\"labels\":{},\"namespace\":null},\"querier\":{\"affinity\":\"podAntiAffinity:\\n  requiredDuringSchedulingIgnoredDuringExecution:\\n    - labelSelector:\\n        matchLabels:\\n          {{- include \\\"tempo.querierSelectorLabels\\\" . | nindent 10 }}\\n      topologyKey: kubernetes.io/hostname\\n  preferredDuringSchedulingIgnoredDuringExecution:\\n    - weight: 100\\n      podAffinityTerm:\\n        labelSelector:\\n          matchLabels:\\n            {{- include \\\"tempo.querierSelectorLabels\\\" . | nindent 12 }}\\n        topologyKey: failure-domain.beta.kubernetes.io/zone\\n\",\"config\":{\"frontend_worker\":{\"grpc_client_config\":{}}},\"extraArgs\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{}},\"terminationGracePeriodSeconds\":30,\"tolerations\":[]},\"queryFrontend\":{\"affinity\":\"podAntiAffinity:\\n  requiredDuringSchedulingIgnoredDuringExecution:\\n    - labelSelector:\\n        matchLabels:\\n          {{- include \\\"tempo.queryFrontendSelectorLabels\\\" . | nindent 10 }}\\n      topologyKey: kubernetes.io/hostname\\n  preferredDuringSchedulingIgnoredDuringExecution:\\n    - weight: 100\\n      podAffinityTerm:\\n        labelSelector:\\n          matchLabels:\\n            {{- include \\\"tempo.queryFrontendSelectorLabels\\\" . | nindent 12 }}\\n        topologyKey: failure-domain.beta.kubernetes.io/zone\\n\",\"autoscaling\":{\"enabled\":false,\"maxReplicas\":3,\"minReplicas\":1,\"targetCPUUtilizationPercentage\":60,\"targetMemoryUtilizationPercentage\":null},\"extraArgs\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"registry\":null,\"repository\":null,\"tag\":null},\"nodeSelector\":{},\"podAnnotations\":{},\"podLabels\":{},\"priorityClassName\":null,\"query\":{\"config\":\"backend: 127.0.0.1:3100\\n\",\"enabled\":true,\"extraArgs\":[],\"extraEnv\":[],\"extraEnvFrom\":[],\"extraVolumeMounts\":[],\"extraVolumes\":[],\"image\":{\"registry\":null,\"repository\":\"grafana/tempo-query\",\"tag\":null},\"resources\":{}},\"replicas\":1,\"resources\":{},\"service\":{\"annotations\":{},\"type\":\"ClusterIP\"},\"serviceDiscovery\":{\"annotations\":{}},\"terminationGracePeriodSeconds\":30,\"tolerations\":[]},\"rbac\":{\"create\":false,\"pspEnabled\":false},\"search\":{\"enabled\":true},\"server\":{\"grpc_server_max_recv_msg_size\":4194304,\"grpc_server_max_send_msg_size\":4194304,\"httpListenPort\":3100,\"logFormat\":\"logfmt\",\"logLevel\":\"info\"},\"serviceAccount\":{\"annotations\":{},\"create\":true,\"imagePullSecrets\":[],\"name\":null},\"serviceMonitor\":{\"annotations\":{},\"enabled\":false,\"interval\":null,\"labels\":{},\"metricRelabelings\":[],\"namespace\":null,\"namespaceSelector\":{},\"relabelings\":[],\"scheme\":\"http\",\"scrapeTimeout\":null,\"tlsConfig\":null},\"storage\":{\"trace\":{\"backend\":\"local\",\"local\":{\"path\":\"/var/tempo/traces\"},\"wal\":{\"path\":\"/var/tempo/wal\"}}},\"tempo\":{\"image\":{\"pullPolicy\":\"IfNotPresent\",\"registry\":\"docker.io\",\"repository\":\"grafana/tempo\",\"tag\":null},\"podAnnotations\":{},\"podLabels\":{},\"readinessProbe\":{\"httpGet\":{\"path\":\"/ready\",\"port\":\"http\"},\"initialDelaySeconds\":30,\"timeoutSeconds\":1},\"securityContext\":{},\"structuredConfig\":{}},\"traces\":{\"jaeger\":{\"grpc\":{\"enabled\":true,\"receiverConfig\":{}},\"thriftBinary\":{\"enabled\":true,\"receiverConfig\":{}},\"thriftCompact\":{\"enabled\":true,\"receiverConfig\":{}},\"thriftHttp\":{\"enabled\":true,\"receiverConfig\":{}}},\"kafka\":{},\"opencensus\":{\"enabled\":false,\"receiverConfig\":{}},\"otlp\":{\"grpc\":{\"enabled\":true,\"receiverConfig\":{}},\"http\":{\"enabled\":true,\"receiverConfig\":{}}},\"zipkin\":{\"enabled\":false,\"receiverConfig\":{}}}}",
                "version": "0.21.8"
              }
            ],
            "name": "tempo",
            "namespace": "tempo",
            "postrender": [],
            "recreate_pods": false,
            "render_subchart_notes": true,
            "replace": false,
            "repository": "https://grafana.github.io/helm-charts",
            "repository_ca_file": null,
            "repository_cert_file": null,
            "repository_key_file": null,
            "repository_password": null,
            "repository_username": null,
            "reset_values": false,
            "reuse_values": false,
            "set": [],
            "set_sensitive": [],
            "skip_crds": false,
            "status": "deployed",
            "timeout": 600,
            "values": [
              "global:\n  image:\n    # -- Overrides the Docker registry globally for all images\n    registry: docker.io\n  # -- Overrides the priorityClassName for all pods\n  priorityClassName: null\n  # -- configures cluster domain (\"cluster.local\" by default)\n  clusterDomain: \"cluster.local\"\n  # -- configures DNS service name\n  dnsService: \"kube-dns\"\n  # -- configures DNS service namespace\n  dnsNamespace: \"kube-system\"\n# -- Overrides the chart's computed fullname\n# fullnameOverride: tempo\ntempo:\n  image:\n    # -- The Docker registry\n    registry: docker.io\n    # -- Docker image repository\n    repository: grafana/tempo\n    # -- Overrides the image tag whose default is the chart's appVersion\n    tag: null\n    pullPolicy: IfNotPresent\n  readinessProbe:\n    httpGet:\n      path: /ready\n      port: http\n    initialDelaySeconds: 30\n    timeoutSeconds: 1\n  # -- Global labels for all tempo pods\n  podLabels: {}\n  # -- Common annotations for all pods\n  podAnnotations: {}\n  # -- SecurityContext holds pod-level security attributes and common container settings\n  securityContext: {}\n  #  capabilities:\n  #    drop:\n  #    - ALL\n  #  readOnlyRootFilesystem: true\n  #  runAsNonRoot: true\n  #  runAsUser: 1000\n  # -- Structured tempo configuration\n  structuredConfig: {}\n\nserviceAccount:\n  # -- Specifies whether a ServiceAccount should be created\n  create: true\n  # -- The name of the ServiceAccount to use.\n  # If not set and create is true, a name is generated using the fullname template\n  name: null\n  # -- Image pull secrets for the service account\n  imagePullSecrets: []\n  # -- Annotations for the service account\n  annotations: {}\n\nrbac:\n  # -- Specifies whether RBAC manifests should be created\n  create: false\n  # -- Specifies whether a PodSecurityPolicy should be created\n  pspEnabled: false\n\n# Configuration for the ingester\ningester:\n  # -- Annotations for the ingester StatefulSet\n  annotations: {}\n  # -- Number of replicas for the ingester\n  replicas: 3\n  autoscaling:\n    # -- Enable autoscaling for the ingester\n    enabled: false\n    # -- Minimum autoscaling replicas for the ingester\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the ingester\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the ingester\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the ingester\n    targetMemoryUtilizationPercentage:\n  image:\n    # -- The Docker registry for the ingester image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the ingester image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the ingester image. Overrides `tempo.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for ingester pods\n  priorityClassName: null\n  # -- Labels for ingester pods\n  podLabels: {}\n  # -- Annotations for ingester pods\n  podAnnotations: {}\n  # -- Additional CLI args for the ingester\n  extraArgs: []\n  # -- Environment variables to add to the ingester pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the ingester pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the ingester\n  resources: {}\n  # -- Grace period to allow the ingester to shutdown before it is killed. Especially for the ingestor,\n  # this must be increased. It must be long enough so ingesters can be gracefully shutdown flushing/transferring\n  # all data and to successfully leave the member ring on shutdown.\n  terminationGracePeriodSeconds: 300\n  # -- Affinity for ingester pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Soft node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.ingesterSelectorLabels\" . | nindent 12 }}\n            topologyKey: kubernetes.io/hostname\n        - weight: 75\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.ingesterSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for ingester pods\n  nodeSelector: {}\n  # -- Tolerations for ingester pods\n  tolerations: []\n  # -- Extra volumes for ingester pods\n  extraVolumeMounts: []\n  # -- Extra volumes for ingester deployment\n  extraVolumes: []\n  persistence:\n    # -- Enable creating PVCs which is required when using boltdb-shipper\n    enabled: false\n    # -- Size of persistent disk\n    size: 10Gi\n    # -- Storage class to be used.\n    # If defined, storageClassName: \u003cstorageClass\u003e.\n    # If set to \"-\", storageClassName: \"\", which disables dynamic provisioning.\n    # If empty or set to null, no storageClassName spec is\n    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).\n    storageClass: null\n  config:\n    # -- Number of copies of spans to store in the ingester ring\n    replication_factor: 3\n    # -- Amount of time a trace must be idle before flushing it to the wal.\n    trace_idle_period: null\n    # -- How often to sweep all tenants and move traces from live -\u003e wal -\u003e completed blocks.\n    flush_check_period: null\n    # -- Maximum size of a block before cutting it\n    max_block_bytes: null\n    # -- Maximum length of time before cutting a block\n    max_block_duration: null\n    # -- Duration to keep blocks in the ingester after they have been flushed\n    complete_block_timeout: null\n  service:\n    # -- Annotations for ingester service\n    annotations: {}\n\n# Configuration for the metrics-generator\nmetricsGenerator:\n  # -- Specifies whether a metrics-generator should be deployed\n  enabled: false\n  # -- Annotations for the metrics-generator StatefulSet\n  annotations: {}\n  # -- Number of replicas for the metrics-generator\n  replicas: 1\n  image:\n    # -- The Docker registry for the metrics-generator image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the metrics-generator image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the metrics-generator image. Overrides `tempo.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for metrics-generator pods\n  priorityClassName: null\n  # -- Labels for metrics-generator pods\n  podLabels: {}\n  # -- Annotations for metrics-generator pods\n  podAnnotations: {}\n  # -- Additional CLI args for the metrics-generator\n  extraArgs: []\n  # -- Environment variables to add to the metrics-generator pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the metrics-generator pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the metrics-generator\n  resources: {}\n  # -- Grace period to allow the metrics-generator to shutdown before it is killed. Especially for the ingestor,\n  # this must be increased. It must be long enough so metrics-generators can be gracefully shutdown flushing/transferring\n  # all data and to successfully leave the member ring on shutdown.\n  terminationGracePeriodSeconds: 300\n  # -- Affinity for metrics-generator pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.metricsGeneratorSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.metricsGeneratorSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for metrics-generator pods\n  nodeSelector: {}\n  # -- Tolerations for metrics-generator pods\n  tolerations: []\n  # -- Extra volumes for metrics-generator pods\n  extraVolumeMounts: []\n  # -- Extra volumes for metrics-generator deployment\n  extraVolumes: []\n  # -- Default ports\n  ports:\n    - name: grpc\n      port: 9095\n      service: true\n    - name: http-memberlist\n      port: 7946\n      service: false\n    - name: http\n      port: 3100\n      service: true\n  config:\n    #  MaxItems is the amount of edges that will be stored in the store.\n    service_graphs_max_items: 10000\n    storage_remote_write: []\n    # - url: http://cortex/api/v1/push\n    #   send_exemplars: true\n    #   headers:\n    #     x-scope-orgid: operations\n  service:\n    # -- Annotations for Metrics Generator service\n    annotations: {}\n\n# Configuration for the distributor\ndistributor:\n  # -- Number of replicas for the distributor\n  replicas: 1\n  autoscaling:\n    # -- Enable autoscaling for the distributor\n    enabled: false\n    # -- Minimum autoscaling replicas for the distributor\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the distributor\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the distributor\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the distributor\n    targetMemoryUtilizationPercentage:\n  image:\n    # -- The Docker registry for the ingester image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the ingester image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the ingester image. Overrides `tempo.image.tag`\n    tag: null\n  service:\n    # -- Annotations for distributor service\n    annotations: {}\n    # -- Type of service for the distributor\n    type: ClusterIP\n    # -- If type is LoadBalancer you can assign the IP to the LoadBalancer\n    loadBalancerIP: \"\"\n    # -- If type is LoadBalancer limit incoming traffic from IPs.\n    loadBalancerSourceRanges: []\n  # -- The name of the PriorityClass for distributor pods\n  priorityClassName: null\n  # -- Labels for distributor pods\n  podLabels: {}\n  # -- Annotations for distributor pods\n  podAnnotations: {}\n  # -- Additional CLI args for the distributor\n  extraArgs: []\n  # -- Environment variables to add to the distributor pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the distributor pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the distributor\n  resources: {}\n  # -- Grace period to allow the distributor to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for distributor pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.distributorSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.distributorSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for distributor pods\n  nodeSelector: {}\n  # -- Tolerations for distributor pods\n  tolerations: []\n  # -- Extra volumes for distributor pods\n  extraVolumeMounts: []\n  # -- Extra volumes for distributor deployment\n  extraVolumes: []\n  config:\n    # -- Enable to log every received trace id to help debug ingestion\n    log_received_traces: null\n    # -- Disables write extension with inactive ingesters\n    extend_writes: null\n    # -- List of tags that will not be extracted from trace data for search lookups\n    search_tags_deny_list: []\n\ncompactor:\n  # -- Number of replicas for the compactor\n  replicas: 1\n  image:\n    # -- The Docker registry for the compactor image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the compactor image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the compactor image. Overrides `tempo.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for compactor pods\n  priorityClassName: null\n  # -- Labels for compactor pods\n  podLabels: {}\n  # -- Annotations for compactor pods\n  podAnnotations: {}\n  # -- Additional CLI args for the compactor\n  extraArgs: []\n  # -- Environment variables to add to the compactor pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the compactor pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the compactor\n  resources: {}\n  # -- Grace period to allow the compactor to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Node selector for compactor pods\n  nodeSelector: {}\n  # -- Tolerations for compactor pods\n  tolerations: []\n  # -- Extra volumes for compactor pods\n  extraVolumeMounts: []\n  # -- Extra volumes for compactor deployment\n  extraVolumes: []\n  config:\n    compaction:\n      # -- Duration to keep blocks\n      block_retention: 48h\n  service:\n    # -- Annotations for compactor service\n    annotations: {}\n\n# Configuration for the querier\nquerier:\n  # -- Number of replicas for the querier\n  replicas: 1\n  image:\n    # -- The Docker registry for the querier image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the querier image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the querier image. Overrides `tempo.image.tag`\n    tag: null\n  # -- The name of the PriorityClass for querier pods\n  priorityClassName: null\n  # -- Labels for querier pods\n  podLabels: {}\n  # -- Annotations for querier pods\n  podAnnotations: {}\n  # -- Additional CLI args for the querier\n  extraArgs: []\n  # -- Environment variables to add to the querier pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the querier pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the querier\n  resources: {}\n  # -- Grace period to allow the querier to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for querier pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.querierSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.querierSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for querier pods\n  nodeSelector: {}\n  # -- Tolerations for querier pods\n  tolerations: []\n  # -- Extra volumes for querier pods\n  extraVolumeMounts: []\n  # -- Extra volumes for querier deployment\n  extraVolumes: []\n  config:\n    frontend_worker:\n      # -- grpc client configuration\n      grpc_client_config: {}\n  service:\n    # -- Annotations for querier service\n    annotations: {}\n\n# Configuration for the query-frontend\nqueryFrontend:\n  query:\n    # -- Required for grafana version \u003c7.5 for compatibility with jaeger-ui. Doesn't work on ARM arch\n    enabled: true\n    image:\n      # -- The Docker registry for the query-frontend image. Overrides `tempo.image.registry`\n      registry: null\n      # -- Docker image repository for the query-frontend image. Overrides `tempo.image.repository`\n      repository: grafana/tempo-query\n      # -- Docker image tag for the query-frontend image. Overrides `tempo.image.tag`\n      tag: null\n    # -- Resource requests and limits for the query\n    resources: {}\n    # -- Additional CLI args for tempo-query pods\n    extraArgs: []\n    # -- Environment variables to add to the tempo-query pods\n    extraEnv: []\n    # -- Environment variables from secrets or configmaps to add to the tempo-query pods\n    extraEnvFrom: []\n    # -- Extra volumes for tempo-query pods\n    extraVolumeMounts: []\n    # -- Extra volumes for tempo-query deployment\n    extraVolumes: []\n    config: |\n      backend: 127.0.0.1:3100\n  # -- Number of replicas for the query-frontend\n  replicas: 1\n  autoscaling:\n    # -- Enable autoscaling for the query-frontend\n    enabled: false\n    # -- Minimum autoscaling replicas for the query-frontend\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the query-frontend\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the query-frontend\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the query-frontend\n    targetMemoryUtilizationPercentage:\n  image:\n    # -- The Docker registry for the query-frontend image. Overrides `tempo.image.registry`\n    registry: null\n    # -- Docker image repository for the query-frontend image. Overrides `tempo.image.repository`\n    repository: null\n    # -- Docker image tag for the query-frontend image. Overrides `tempo.image.tag`\n    tag: null\n  service:\n    # -- Annotations for queryFrontend service\n    annotations: {}\n    # -- Type of service for the queryFrontend\n    type: ClusterIP\n  serviceDiscovery:\n    # -- Annotations for queryFrontendDiscovery service\n    annotations: {}\n  # -- The name of the PriorityClass for query-frontend pods\n  priorityClassName: null\n  # -- Labels for queryFrontend pods\n  podLabels: {}\n  # -- Annotations for query-frontend pods\n  podAnnotations: {}\n  # -- Additional CLI args for the query-frontend\n  extraArgs: []\n  # -- Environment variables to add to the query-frontend pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the query-frontend pods\n  extraEnvFrom: []\n  # -- Resource requests and limits for the query-frontend\n  resources: {}\n  # -- Grace period to allow the query-frontend to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for query-frontend pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.queryFrontendSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.queryFrontendSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for query-frontend pods\n  nodeSelector: {}\n  # -- Tolerations for query-frontend pods\n  tolerations: []\n  # -- Extra volumes for query-frontend pods\n  extraVolumeMounts: []\n  # -- Extra volumes for query-frontend deployment\n  extraVolumes: []\n\nsearch:\n  # -- Enable Tempo search\n  enabled: true\n\nmultitenancyEnabled: false\n\ntraces:\n  jaeger:\n    grpc:\n      # -- Enable Tempo to ingest Jaeger GRPC traces\n      enabled: true\n      # -- Jaeger GRPC receiver config\n      receiverConfig: {}\n    thriftBinary:\n      # -- Enable Tempo to ingest Jaeger Thrift Binary traces\n      enabled: true\n      # -- Jaeger Thrift Binary receiver config\n      receiverConfig: {}\n    thriftCompact:\n      # -- Enable Tempo to ingest Jaeger Thrift Compact traces\n      enabled: true\n      # -- Jaeger Thrift Compact receiver config\n      receiverConfig: {}\n    thriftHttp:\n      # -- Enable Tempo to ingest Jaeger Thrift HTTP traces\n      enabled: true\n      # -- Jaeger Thrift HTTP receiver config\n      receiverConfig: {}\n  zipkin:\n    # -- Enable Tempo to ingest Zipkin traces\n    enabled: false\n    # -- Zipkin receiver config\n    receiverConfig: {}\n  otlp:\n    http:\n      # -- Enable Tempo to ingest Open Telemetry HTTP traces\n      enabled: true\n      # -- HTTP receiver advanced config\n      receiverConfig: {}\n    grpc:\n      # -- Enable Tempo to ingest Open Telemetry GRPC traces\n      enabled: true\n      # -- GRPC receiver advanced config\n      receiverConfig: {}\n  opencensus:\n    # -- Enable Tempo to ingest Open Census traces\n    enabled: false\n    # -- Open Census receiver config\n    receiverConfig: {}\n  # -- Enable Tempo to ingest traces from Kafka. Reference: https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/kafkareceiver\n  kafka: {}\n\nconfig: |\n  multitenancy_enabled: {{ .Values.multitenancyEnabled }}\n  search_enabled: {{ .Values.search.enabled }}\n  metrics_generator_enabled: {{ .Values.metricsGenerator.enabled }}\n  compactor:\n    compaction:\n      block_retention: {{ .Values.compactor.config.compaction.block_retention }}\n    ring:\n      kvstore:\n        store: memberlist\n  {{- if .Values.metricsGenerator.enabled }}\n  metrics_generator:\n    ring:\n      kvstore:\n        store: memberlist\n    processor:\n      service_graphs:\n        max_items: {{ .Values.metricsGenerator.config.service_graphs_max_items }}\n    storage:\n      path: /var/tempo/wal\n      remote_write:\n        {{- toYaml .Values.metricsGenerator.config.storage_remote_write | nindent 6}}\n  {{- end }}\n  distributor:\n    ring:\n      kvstore:\n        store: memberlist\n    receivers:\n      {{- if  or (.Values.traces.jaeger.thriftCompact.enabled) (.Values.traces.jaeger.thriftBinary.enabled) (.Values.traces.jaeger.thriftHttp.enabled) (.Values.traces.jaeger.grpc.enabled) }}\n      jaeger:\n        protocols:\n          {{- if .Values.traces.jaeger.thriftCompact.enabled }}\n          thrift_compact:\n            {{- $mergedJaegerThriftCompactConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:6831\") .Values.traces.jaeger.thriftCompact.receiverConfig }}\n            {{- toYaml $mergedJaegerThriftCompactConfig | nindent 10 }}\n          {{- end }}\n          {{- if .Values.traces.jaeger.thriftBinary.enabled }}\n          thrift_binary:\n            {{- $mergedJaegerThriftBinaryConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:6832\") .Values.traces.jaeger.thriftBinary.receiverConfig }}\n            {{- toYaml $mergedJaegerThriftBinaryConfig | nindent 10 }}\n          {{- end }}\n          {{- if .Values.traces.jaeger.thriftHttp.enabled }}\n          thrift_http:\n            {{- $mergedJaegerThriftHttpConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:14268\") .Values.traces.jaeger.thriftHttp.receiverConfig }}\n            {{- toYaml $mergedJaegerThriftHttpConfig | nindent 10 }}\n          {{- end }}\n          {{- if .Values.traces.jaeger.grpc.enabled }}\n          grpc:\n            {{- $mergedJaegerGrpcConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:14250\") .Values.traces.jaeger.grpc.receiverConfig }}\n            {{- toYaml $mergedJaegerGrpcConfig | nindent 10 }}\n          {{- end }}\n      {{- end }}\n      {{- if .Values.traces.zipkin.enabled }}\n      zipkin:\n        {{- $mergedZipkinReceiverConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:9411\") .Values.traces.zipkin.receiverConfig }}\n        {{- toYaml $mergedZipkinReceiverConfig | nindent 6 }}\n      {{- end }}\n      {{- if or (.Values.traces.otlp.http.enabled) (.Values.traces.otlp.grpc.enabled) }}\n      otlp:\n        protocols:\n          {{- if .Values.traces.otlp.http.enabled }}\n          http:\n            {{- $mergedOtlpHttpReceiverConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:4318\") .Values.traces.otlp.http.receiverConfig }}\n            {{- toYaml $mergedOtlpHttpReceiverConfig | nindent 10 }}\n          {{- end }}\n          {{- if .Values.traces.otlp.grpc.enabled }}\n          grpc:\n            {{- $mergedOtlpGrpcReceiverConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:4317\") .Values.traces.otlp.grpc.receiverConfig }}\n            {{- toYaml $mergedOtlpGrpcReceiverConfig | nindent 10 }}\n          {{- end }}\n      {{- end }}\n      {{- if .Values.traces.opencensus.enabled }}\n      opencensus:\n        {{- $mergedOpencensusReceiverConfig := mustMergeOverwrite (dict \"endpoint\" \"0.0.0.0:55678\") .Values.traces.opencensus.receiverConfig }}\n        {{- toYaml $mergedOpencensusReceiverConfig | nindent 6 }}\n      {{- end }}\n      {{- if .Values.traces.kafka }}\n      kafka:\n        {{- toYaml .Values.traces.kafka | nindent 6 }}\n      {{- end }}\n    {{- if .Values.distributor.config.log_received_traces }}\n    log_received_traces: {{ .Values.distributor.config.log_received_traces }}\n    {{- end }}\n    {{- if .Values.distributor.config.extend_writes }}\n    extend_writes: {{ .Values.distributor.config.extend_writes }}\n    {{- end }}\n    {{- if .Values.distributor.config.search_tags_deny_list }}\n    search_tags_deny_list:\n      {{- with .Values.distributor.config.search_tags_deny_list }}\n      {{- toYaml . | nindent 4 }}\n      {{- end }}\n    {{- end }}\n  querier:\n    frontend_worker:\n      frontend_address: {{ include \"tempo.queryFrontendFullname\" . }}-discovery:9095\n      {{- if .Values.querier.config.frontend_worker.grpc_client_config }}\n      grpc_client_config:\n        {{- toYaml .Values.querier.config.frontend_worker.grpc_client_config | nindent 6 }}\n      {{- end }}\n  ingester:\n    lifecycler:\n      ring:\n        replication_factor: {{ .Values.ingester.config.replication_factor }}\n        kvstore:\n          store: memberlist\n      tokens_file_path: /var/tempo/tokens.json\n    {{- if .Values.ingester.config.trace_idle_period }}\n    trace_idle_period: {{ .Values.ingester.config.trace_idle_period }}\n    {{- end }}\n    {{- if .Values.ingester.config.flush_check_period }}\n    flush_check_period: {{ .Values.ingester.config.flush_check_period }}\n    {{- end }}\n    {{- if .Values.ingester.config.max_block_bytes }}\n    max_block_bytes: {{ .Values.ingester.config.max_block_bytes }}\n    {{- end }}\n    {{- if .Values.ingester.config.max_block_duration }}\n    max_block_duration: {{ .Values.ingester.config.max_block_duration }}\n    {{- end }}\n    {{- if .Values.ingester.config.complete_block_timeout }}\n    complete_block_timeout: {{ .Values.ingester.config.complete_block_timeout }}\n    {{- end }}\n  memberlist:\n    abort_if_cluster_join_fails: false\n    join_members:\n      - {{ include \"tempo.fullname\" . }}-gossip-ring\n  overrides:\n    {{- toYaml .Values.global_overrides | nindent 2 }}\n  server:\n    http_listen_port: {{ .Values.server.httpListenPort }}\n    log_level: {{ .Values.server.logLevel }}\n    log_format: {{ .Values.server.logFormat }}\n    grpc_server_max_recv_msg_size: {{ .Values.server.grpc_server_max_recv_msg_size }}\n    grpc_server_max_send_msg_size: {{ .Values.server.grpc_server_max_send_msg_size }}\n  storage:\n    trace:\n      backend: {{.Values.storage.trace.backend}}\n      {{- if eq .Values.storage.trace.backend \"gcs\"}}\n      gcs:\n        {{- toYaml .Values.storage.trace.gcs | nindent 6}}\n      {{- end}}\n      {{- if eq .Values.storage.trace.backend \"s3\"}}\n      s3:\n        {{- toYaml .Values.storage.trace.s3 | nindent 6}}\n      {{- end}}\n      {{- if eq .Values.storage.trace.backend \"azure\"}}\n      azure:\n        {{- toYaml .Values.storage.trace.azure | nindent 6}}\n      {{- end}}\n      blocklist_poll: 5m\n      local:\n        path: /var/tempo/traces\n      wal:\n        path: /var/tempo/wal\n      cache: memcached\n      memcached:\n        consistent_hash: true\n        host: {{ include \"tempo.fullname\" . }}-memcached\n        service: memcached-client\n        timeout: 500ms\n\n# Set Tempo server configuration\n# Refers to https://grafana.com/docs/tempo/latest/configuration/#server\nserver:\n  # --  HTTP server listen host\n  httpListenPort: 3100\n  # -- Log level. Can be set to trace, debug, info (default), warn error, fatal, panic\n  logLevel: info\n  # -- Log format. Can be set to logfmt (default) or json.\n  logFormat: logfmt\n  # -- Max gRPC message size that can be received\n  grpc_server_max_recv_msg_size: 4194304\n  # -- Max gRPC message size that can be sent\n  grpc_server_max_send_msg_size: 4194304\n# To configure a different storage backend instead of local storage:\n# storage:\n#   trace:\n#     backend: azure\n#     azure:\n#       container-name:\n#       storage-account-name:\n#       storage-account-key:\nstorage:\n  trace:\n    # -- The supported storage backends are gcs, s3 and azure, as specified in https://grafana.com/docs/tempo/latest/configuration/#storage\n    backend: local\n    local:\n      path: /var/tempo/traces\n    wal:\n      path: /var/tempo/wal\n\n# Global overrides\nglobal_overrides:\n  per_tenant_override_config: /conf/overrides.yaml\n  # metrics_generator_processors:\n  #   - service-graphs\n  #   - span-metrics\n\n# Per tenants overrides\noverrides: |\n  overrides: {}\n\n# memcached is for all of the Tempo pieces to coordinate with each other.\n# you can use your self memcacherd by set enable: false and host + service\nmemcached:\n  # -- Specified whether the memcached cachce should be enabled\n  enabled: true\n  image:\n    # -- The Docker registry for the Memcached image. Overrides `global.image.registry`\n    registry: null\n    # -- Memcached Docker image repository\n    repository: memcached\n    # -- Memcached Docker image tag\n    tag: 1.5.17-alpine\n    # -- Memcached Docker image pull policy\n    pullPolicy: IfNotPresent\n  host: memcached\n  # Number of replicas for memchached\n  replicas: 1\n  # -- Additional CLI args for memcached\n  extraArgs: []\n  # -- Environment variables to add to memcached pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to memcached pods\n  extraEnvFrom: []\n  # -- Labels for memcached pods\n  podLabels: {}\n  # -- Annotations for memcached pods\n  podAnnotations: {}\n  # -- Resource requests and limits for memcached\n  resources: {}\n  # -- Affinity for memcached pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.memcachedSelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.memcachedSelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  service:\n    # -- Annotations for memcached service\n    annotations: {}\n\nmemcachedExporter:\n  # -- Specifies whether the Memcached Exporter should be enabled\n  enabled: false\n  image:\n    # -- The Docker registry for the Memcached Exporter image. Overrides `global.image.registry`\n    registry: null\n    # -- Memcached Exporter Docker image repository\n    repository: prom/memcached-exporter\n    # -- Memcached Exporter Docker image tag\n    tag: v0.8.0\n    # -- Memcached Exporter Docker image pull policy\n    pullPolicy: IfNotPresent\n    # -- Memcached Exporter resource requests and limits\n  resources: {}\n\n# ServiceMonitor configuration\nserviceMonitor:\n  # -- If enabled, ServiceMonitor resources for Prometheus Operator are created\n  enabled: false\n  # -- Alternative namespace for ServiceMonitor resources\n  namespace: null\n  # -- Namespace selector for ServiceMonitor resources\n  namespaceSelector: {}\n  # -- ServiceMonitor annotations\n  annotations: {}\n  # -- Additional ServiceMonitor labels\n  labels: {}\n  # -- ServiceMonitor scrape interval\n  interval: null\n  # -- ServiceMonitor scrape timeout in Go duration format (e.g. 15s)\n  scrapeTimeout: null\n  # -- ServiceMonitor relabel configs to apply to samples before scraping\n  # https://github.com/prometheus-operator/prometheus-operator/blob/master/Documentation/api.md#relabelconfig\n  relabelings: []\n  # -- ServiceMonitor metric relabel configs to apply to samples before ingestion\n  # https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#endpoint\n  metricRelabelings: []\n  # -- ServiceMonitor will use http by default, but you can pick https as well\n  scheme: http\n  # -- ServiceMonitor will use these tlsConfig settings to make the health check requests\n  tlsConfig: null\n\n# Rules for the Prometheus Operator\nprometheusRule:\n  # -- If enabled, a PrometheusRule resource for Prometheus Operator is created\n  enabled: false\n  # -- Alternative namespace for the PrometheusRule resource\n  namespace: null\n  # -- PrometheusRule annotations\n  annotations: {}\n  # -- Additional PrometheusRule labels\n  labels: {}\n  # -- Contents of Prometheus rules file\n  groups: []\n  # - name: loki-rules\n  #   rules:\n  #     - record: job:loki_request_duration_seconds_bucket:sum_rate\n  #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job)\n  #     - record: job_route:loki_request_duration_seconds_bucket:sum_rate\n  #       expr: sum(rate(loki_request_duration_seconds_bucket[1m])) by (le, job, route)\n  #     - record: node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate\n  #       expr: sum(rate(container_cpu_usage_seconds_total[1m])) by (node, namespace, pod, container)\n\n# Configuration for the gateway\ngateway:\n  # -- Specifies whether the gateway should be enabled\n  enabled: false\n  # -- Number of replicas for the gateway\n  replicas: 1\n  autoscaling:\n    # -- Enable autoscaling for the gateway\n    enabled: false\n    # -- Minimum autoscaling replicas for the gateway\n    minReplicas: 1\n    # -- Maximum autoscaling replicas for the gateway\n    maxReplicas: 3\n    # -- Target CPU utilisation percentage for the gateway\n    targetCPUUtilizationPercentage: 60\n    # -- Target memory utilisation percentage for the gateway\n    targetMemoryUtilizationPercentage:\n  # -- Enable logging of 2xx and 3xx HTTP requests\n  verboseLogging: true\n  image:\n    # -- The Docker registry for the gateway image. Overrides `global.image.registry`\n    registry: null\n    # -- The gateway image repository\n    repository: nginxinc/nginx-unprivileged\n    # -- The gateway image tag\n    tag: 1.19-alpine\n    # -- The gateway image pull policy\n    pullPolicy: IfNotPresent\n  # -- The name of the PriorityClass for gateway pods\n  priorityClassName: null\n  # -- Labels for gateway pods\n  podLabels: {}\n  # -- Annotations for gateway pods\n  podAnnotations: {}\n  # -- Additional CLI args for the gateway\n  extraArgs: []\n  # -- Environment variables to add to the gateway pods\n  extraEnv: []\n  # -- Environment variables from secrets or configmaps to add to the gateway pods\n  extraEnvFrom: []\n  # -- Volumes to add to the gateway pods\n  extraVolumes: []\n  # -- Volume mounts to add to the gateway pods\n  extraVolumeMounts: []\n  # -- Resource requests and limits for the gateway\n  resources: {}\n  # -- Grace period to allow the gateway to shutdown before it is killed\n  terminationGracePeriodSeconds: 30\n  # -- Affinity for gateway pods. Passed through `tpl` and, thus, to be configured as string\n  # @default -- Hard node and soft zone anti-affinity\n  affinity: |\n    podAntiAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        - labelSelector:\n            matchLabels:\n              {{- include \"tempo.gatewaySelectorLabels\" . | nindent 10 }}\n          topologyKey: kubernetes.io/hostname\n      preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 100\n          podAffinityTerm:\n            labelSelector:\n              matchLabels:\n                {{- include \"tempo.gatewaySelectorLabels\" . | nindent 12 }}\n            topologyKey: failure-domain.beta.kubernetes.io/zone\n  # -- Node selector for gateway pods\n  nodeSelector: {}\n  # -- Tolerations for gateway pods\n  tolerations: []\n  # Gateway service configuration\n  service:\n    # -- Port of the gateway service\n    port: 80\n    # -- Type of the gateway service\n    type: ClusterIP\n    # -- ClusterIP of the gateway service\n    clusterIP: null\n    # -- Node port if service type is NodePort\n    nodePort: null\n    # -- Load balancer IPO address if service type is LoadBalancer\n    loadBalancerIP: null\n    # -- Annotations for the gateway service\n    annotations: {}\n    # -- Labels for gateway service\n    labels: {}\n  # Gateway ingress configuration\n  ingress:\n    # -- Specifies whether an ingress for the gateway should be created\n    enabled: false\n    # -- Ingress Class Name. MAY be required for Kubernetes versions \u003e= 1.18\n    # ingressClassName: nginx\n    # -- Annotations for the gateway ingress\n    annotations: {}\n    # -- Hosts configuration for the gateway ingress\n    hosts:\n      - host: gateway.tempo.example.com\n        paths:\n          - path: /\n            # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers\n            # pathType: Prefix\n    # -- TLS configuration for the gateway ingress\n    tls:\n      - secretName: tempo-gateway-tls\n        hosts:\n          - gateway.tempo.example.com\n  # Basic auth configuration\n  basicAuth:\n    # -- Enables basic authentication for the gateway\n    enabled: false\n    # -- The basic auth username for the gateway\n    username: null\n    # -- The basic auth password for the gateway\n    password: null\n    # -- Uses the specified username and password to compute a htpasswd using Sprig's `htpasswd` function.\n    # The value is templated using `tpl`. Override this to use a custom htpasswd, e.g. in case the default causes\n    # high CPU load.\n    htpasswd: \u003e-\n      {{ htpasswd (required \"'gateway.basicAuth.username' is required\" .Values.gateway.basicAuth.username) (required \"'gateway.basicAuth.password' is required\" .Values.gateway.basicAuth.password) }}\n    # -- Existing basic auth secret to use. Must contain '.htpasswd'\n    existingSecret: null\n  # Configures the readiness probe for the gateway\n  readinessProbe:\n    httpGet:\n      path: /\n      port: http\n    initialDelaySeconds: 15\n    timeoutSeconds: 1\n  nginxConfig:\n    # -- NGINX log format\n    logFormat: |-\n      main '$remote_addr - $remote_user [$time_local]  $status '\n              '\"$request\" $body_bytes_sent \"$http_referer\" '\n              '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n    # -- Allows appending custom configuration to the server block\n    serverSnippet: \"\"\n    # -- Allows appending custom configuration to the http block\n    httpSnippet: \"\"\n    # -- Config file contents for Nginx. Passed through the `tpl` function to allow templating\n    # @default -- See values.yaml\n    file: |\n      worker_processes  5;  ## Default: 1\n      error_log  /dev/stderr;\n      pid        /tmp/nginx.pid;\n      worker_rlimit_nofile 8192;\n\n      events {\n        worker_connections  4096;  ## Default: 1024\n      }\n\n      http {\n        client_body_temp_path /tmp/client_temp;\n        proxy_temp_path       /tmp/proxy_temp_path;\n        fastcgi_temp_path     /tmp/fastcgi_temp;\n        uwsgi_temp_path       /tmp/uwsgi_temp;\n        scgi_temp_path        /tmp/scgi_temp;\n\n        proxy_http_version    1.1;\n\n        default_type application/octet-stream;\n        log_format   {{ .Values.gateway.nginxConfig.logFormat }}\n\n        {{- if .Values.gateway.verboseLogging }}\n        access_log   /dev/stderr  main;\n        {{- else }}\n\n        map $status $loggable {\n          ~^[23]  0;\n          default 1;\n        }\n        access_log   /dev/stderr  main  if=$loggable;\n        {{- end }}\n\n        sendfile     on;\n        tcp_nopush   on;\n        resolver {{ .Values.global.dnsService }}.{{ .Values.global.dnsNamespace }}.svc.{{ .Values.global.clusterDomain }};\n\n        {{- with .Values.gateway.nginxConfig.httpSnippet }}\n        {{ . | nindent 2 }}\n        {{- end }}\n\n        server {\n          listen             8080;\n\n          {{- if .Values.gateway.basicAuth.enabled }}\n          auth_basic           \"Tempo\";\n          auth_basic_user_file /etc/nginx/secrets/.htpasswd;\n          {{- end }}\n\n          location = / {\n            return 200 'OK';\n            auth_basic off;\n          }\n\n          location = /jaeger/api/traces {\n            proxy_pass       http://{{ include \"tempo.distributorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:14268/api/traces;\n          }\n\n          location = /zipkin/spans {\n            proxy_pass       http://{{ include \"tempo.distributorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:9411/spans;\n          }\n\n          location = /otlp/v1/traces {\n            proxy_pass       http://{{ include \"tempo.distributorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:4318/v1/traces;\n          }\n\n          location ^~ /api {\n            proxy_pass       http://{{ include \"tempo.queryFrontendFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /flush {\n            proxy_pass       http://{{ include \"tempo.ingesterFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /shutdown {\n            proxy_pass       http://{{ include \"tempo.ingesterFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /distributor/ring {\n            proxy_pass       http://{{ include \"tempo.distributorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /ingester/ring {\n            proxy_pass       http://{{ include \"tempo.distributorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          location = /compactor/ring {\n            proxy_pass       http://{{ include \"tempo.compactorFullname\" . }}.{{ .Release.Namespace }}.svc.{{ .Values.global.clusterDomain }}:3100$request_uri;\n          }\n\n          {{- with .Values.gateway.nginxConfig.serverSnippet }}\n          {{ . | nindent 4 }}\n          {{- end }}\n        }\n      }\n"
            ],
            "verify": false,
            "version": "0.21.8",
            "wait": true,
            "wait_for_jobs": false
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.test-module.data.template_file.loki",
            "module.test-module.data.template_file.promtail",
            "module.test-module.data.template_file.tempo",
            "module.test-module.helm_release.loki",
            "module.test-module.helm_release.promtail"
          ]
        }
      ]
    },
    {
      "module": "module.test-module",
      "mode": "managed",
      "type": "kubectl_manifest",
      "name": "syntetic",
      "provider": "provider[\"registry.terraform.io/gavinbunney/kubectl\"]",
      "instances": [
        {
          "index_key": 0,
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/hotrod",
            "ignore_fields": null,
            "kind": "Namespace",
            "live_manifest_incluster": "eee57f256d84d3de71fa6671fff10e2587b943fe9ec82188d10faaa9611b8c7e",
            "live_uid": "23aafafe-c18b-4eb4-a54f-df774e325ea7",
            "name": "hotrod",
            "namespace": null,
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "23aafafe-c18b-4eb4-a54f-df774e325ea7",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": false,
            "yaml_body": "---\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: hotrod\n  labels:\n    name: hotrod",
            "yaml_body_parsed": "apiVersion: v1\nkind: Namespace\nmetadata:\n  labels:\n    name: hotrod\n  name: hotrod\n",
            "yaml_incluster": "eee57f256d84d3de71fa6671fff10e2587b943fe9ec82188d10faaa9611b8c7e"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "module.test-module.data.kubectl_path_documents.syntetic",
            "module.test-module.data.template_file.loki",
            "module.test-module.data.template_file.prometheus",
            "module.test-module.data.template_file.promtail",
            "module.test-module.data.template_file.tempo",
            "module.test-module.helm_release.loki",
            "module.test-module.helm_release.myapp",
            "module.test-module.helm_release.prometheus",
            "module.test-module.helm_release.promtail",
            "module.test-module.helm_release.tempo"
          ]
        },
        {
          "index_key": 1,
          "schema_version": 1,
          "attributes": {
            "api_version": "apps/v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/apis/apps/v1/namespaces/default/deployments/hotrod",
            "ignore_fields": null,
            "kind": "Deployment",
            "live_manifest_incluster": "ecb42c4234d22406d0ad6146900e7118c851ad8043787454abe29799fe3144b6",
            "live_uid": "a84a1703-7ff1-494b-9bb8-bec93e4a0969",
            "name": "hotrod",
            "namespace": null,
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "a84a1703-7ff1-494b-9bb8-bec93e4a0969",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": false,
            "yaml_body": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hotrod\n  name: hotrod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hotrod\n  strategy: {}\n  template:\n    metadata:\n      labels:\n        app: hotrod\n    spec:\n      containers:\n      - image: jaegertracing/example-hotrod:latest\n        name: hotrod\n        args: [\"all\"]\n        env:\n          - name: JAEGER_AGENT_HOST\n            value: tempo-tempo-distributed-distributor.tempo.svc\n          - name: JAEGER_AGENT_PORT\n            value: \"6831\"\n        ports:\n          - containerPort: 8080\n            name: frontend\n          - containerPort: 8081\n            name: customer\n          - containerPort: 8083\n            name: route\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100M\n          requests:\n            cpu: 100m\n            memory: 100M",
            "yaml_body_parsed": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hotrod\n  name: hotrod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hotrod\n  strategy: {}\n  template:\n    metadata:\n      labels:\n        app: hotrod\n    spec:\n      containers:\n      - args:\n        - all\n        env:\n        - name: JAEGER_AGENT_HOST\n          value: tempo-tempo-distributed-distributor.tempo.svc\n        - name: JAEGER_AGENT_PORT\n          value: \"6831\"\n        image: jaegertracing/example-hotrod:latest\n        name: hotrod\n        ports:\n        - containerPort: 8080\n          name: frontend\n        - containerPort: 8081\n          name: customer\n        - containerPort: 8083\n          name: route\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100M\n          requests:\n            cpu: 100m\n            memory: 100M\n",
            "yaml_incluster": "ecb42c4234d22406d0ad6146900e7118c851ad8043787454abe29799fe3144b6"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "module.test-module.data.kubectl_path_documents.syntetic",
            "module.test-module.data.template_file.loki",
            "module.test-module.data.template_file.prometheus",
            "module.test-module.data.template_file.promtail",
            "module.test-module.data.template_file.tempo",
            "module.test-module.helm_release.loki",
            "module.test-module.helm_release.myapp",
            "module.test-module.helm_release.prometheus",
            "module.test-module.helm_release.promtail",
            "module.test-module.helm_release.tempo"
          ]
        },
        {
          "index_key": 2,
          "schema_version": 1,
          "attributes": {
            "api_version": "v1",
            "apply_only": false,
            "force_conflicts": false,
            "force_new": false,
            "id": "/api/v1/namespaces/default/services/hotrod",
            "ignore_fields": null,
            "kind": "Service",
            "live_manifest_incluster": "bbe59462b4fb78e5f5b814c4753a9b02abafe3977701db9f1f35f9d1dbac8968",
            "live_uid": "7dcf08dd-b19e-4821-ba56-8e9b0c21e569",
            "name": "hotrod",
            "namespace": null,
            "override_namespace": null,
            "sensitive_fields": null,
            "server_side_apply": false,
            "timeouts": null,
            "uid": "7dcf08dd-b19e-4821-ba56-8e9b0c21e569",
            "validate_schema": true,
            "wait": null,
            "wait_for_rollout": false,
            "yaml_body": "apiVersion: v1\nkind: Service\nmetadata:\n  name: hotrod\nspec:\n  selector:\n    app: hotrod\n  ports:\n    - name: frontend\n      protocol: TCP\n      port: 8080\n      targetPort: frontend",
            "yaml_body_parsed": "apiVersion: v1\nkind: Service\nmetadata:\n  name: hotrod\nspec:\n  ports:\n  - name: frontend\n    port: 8080\n    protocol: TCP\n    targetPort: frontend\n  selector:\n    app: hotrod\n",
            "yaml_incluster": "bbe59462b4fb78e5f5b814c4753a9b02abafe3977701db9f1f35f9d1dbac8968"
          },
          "sensitive_attributes": [],
          "private": "eyJlMmJmYjczMC1lY2FhLTExZTYtOGY4OC0zNDM2M2JjN2M0YzAiOnsiY3JlYXRlIjo2MDAwMDAwMDAwMDB9LCJzY2hlbWFfdmVyc2lvbiI6IjEifQ==",
          "dependencies": [
            "module.test-module.data.kubectl_path_documents.syntetic",
            "module.test-module.data.template_file.loki",
            "module.test-module.data.template_file.prometheus",
            "module.test-module.data.template_file.promtail",
            "module.test-module.data.template_file.tempo",
            "module.test-module.helm_release.loki",
            "module.test-module.helm_release.myapp",
            "module.test-module.helm_release.prometheus",
            "module.test-module.helm_release.promtail",
            "module.test-module.helm_release.tempo"
          ]
        }
      ]
    },
    {
      "module": "module.test-module",
      "mode": "managed",
      "type": "null_resource",
      "name": "exec-cm-grafana",
      "provider": "provider[\"registry.terraform.io/hashicorp/null\"]",
      "instances": [
        {
          "status": "tainted",
          "schema_version": 0,
          "attributes": {
            "id": "895337899940038058",
            "triggers": null
          },
          "sensitive_attributes": [],
          "private": "bnVsbA==",
          "dependencies": [
            "module.test-module.data.template_file.loki",
            "module.test-module.data.template_file.prometheus",
            "module.test-module.data.template_file.promtail",
            "module.test-module.data.template_file.tempo",
            "module.test-module.helm_release.loki",
            "module.test-module.helm_release.prometheus",
            "module.test-module.helm_release.promtail",
            "module.test-module.helm_release.tempo"
          ]
        }
      ]
    }
  ],
  "check_results": null
}
